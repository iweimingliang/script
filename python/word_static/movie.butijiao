
Overview
Quick Start Guide
Installing Rancher
Rancher Server
Rancher Server Requirements
Rancher Server Tags
Launching Rancher Server
Setting up HA
HA Requirements
Using an External DB
Bind Mounted MySQL Volume
Using an ELB in AWS
Using LDAP/AD with TLS
Behind an HTTP Proxy
SELinux Requirements for RHEL/CentOS
Basic SSL Configuration
No Internet Access
Upgrading Rancher
Rancher Configuration
Environments
Hosts
Registries
Certificates
Infrastructure Services
Cattle
Kubernetes
Mesos
Swarm - Experimental
Windows - Experimental
Using Native Docker CLI
Rancher Catalog
Rancher CLI
API Documentation
Contributing to Rancher
FAQs
Docs Archive
Installing Rancher Server
Rancher is deployed as a set of Docker containers. Running Rancher is as simple as launching two containers. One container as the management server and another container on a node as an agent.

Rancher Server - Single Container (non-HA)
Rancher Server - Single Container (non-HA) - External database
Rancher Server - Single Container (non-HA)- Bind mounted MySQL volume
Rancher Server - Full Active/Active HA
Rancher Server - Using ELB in AWS
Rancher Server - AD/OpenLDAP using TLS
Rancher Server - HTTP Proxy
NOTE:
You can get all help options for the Rancher server container by running docker run rancher/server --help.

REQUIREMENTS
Any modern Linux distribution with a supported version of Docker. RancherOS, Ubuntu, RHEL/CentOS 7 are more heavily tested.
For RHEL/CentOS, the default storage driver, i.e. devicemapper using loopback, is not recommended by Docker. Please refer to the Docker documentation on how to change it.
For RHEL/CentOS, if you want to enable SELinux, you will need to install an additional SELinux module.
1GB RAM
MySQL server should have a max_connections setting > 150
MYSQL Configuration Requirements
Option 1: Run with Antelope with default of COMPACT
Option 2: Run MySQL 5.7 with Barracuda where the default ROW_FORMAT is Dynamic
NOTE:
Currently, Docker for Mac is not supported in Rancher.

RANCHER SERVER TAGS
Rancher server has 2 different tags. For each major release tag, we will provide documentation for the specific version.

rancher/server:latest tag will be our latest development builds. These builds will have been validated through our CI automation framework. These releases are not meant for deployment in production.
rancher/server:stable tag will be our latest stable release builds. This tag is the version that we recommend for production.
Please do not use any release with a rc{n} suffix. These rc builds are meant for the Rancher team to test out builds.


LAUNCHING RANCHER SERVER - SINGLE CONTAINER (NON-HA)
On the Linux machine with Docker installed, the command to start a single instance of Rancher is simple.

$ sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server
RANCHER UI
The UI and API will be available on the exposed port 8080. After the docker image is downloaded, it will take a minute or two before Rancher has successfully started and is available to view.

Navigate to the following URL: http://<SERVER_IP>:8080. The <SERVER_IP> is the public IP address of the host that is running Rancher server.

Once the UI is up and running, you can start by adding hosts or select a container orchestration from the Infrastructure catalog. By default, if a different container orchestration type is not selected, the environment will be using cattle. After the hosts are added into Rancher, you can start adding services or launch templates from the Rancher catalog.


LAUNCHING RANCHER SERVER - SINGLE CONTAINER - EXTERNAL DATABASE
Instead of using the internal database that comes with Rancher server, you can start Rancher server pointing to an external database. The command would be the same, but appending in additional arguments to direct how to connect to your external database.

NOTE:
Your database, name and user of the database will already need to be created, but no schemas will need to be created. Rancher will automatically create all the schemas related to Rancher.

Here is an example of a SQL command to create a database and users.

> CREATE DATABASE IF NOT EXISTS cattle COLLATE = 'utf8_general_ci' CHARACTER SET = 'utf8';
> GRANT ALL ON cattle.* TO 'cattle'@'%' IDENTIFIED BY 'cattle';
> GRANT ALL ON cattle.* TO 'cattle'@'localhost' IDENTIFIED BY 'cattle';
To start Rancher connecting to an external database, you pass in additional arguments as part of the command for the container.

$ sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server \
    --db-host myhost.example.com --db-port 3306 --db-user username --db-pass password --db-name cattle
Most of the options to pass in also have default values and are not required. Only the location of the MySQL server is required.

--db-host               IP or hostname of MySQL server
--db-port               port of MySQL server (default: 3306)
--db-user               username for MySQL login (default: cattle)
--db-pass               password for MySQL login (default: cattle)
--db-name               MySQL database name to use (default: cattle)


NOTE:
In previous versions of Rancher server, we had connected to an external database using environment variables, those environment variables will continue to work, but Rancher recommends using the arguments instead.


LAUNCHING RANCHER SERVER - SINGLE CONTAINER - BIND MOUNT MYSQL VOLUME
If you would like to persist the database inside your container to a volume on your host, launch Rancher server by bind mounting the MySQL volume.

$ sudo docker run -d -v <host_vol>:/var/lib/mysql --restart=unless-stopped -p 8080:8080 rancher/server
With this command, the database will persist on the host. If you have an existing Rancher container and would like to bind mount the MySQL volume, the instructions are located in our upgrading documentation.


LAUNCHING RANCHER SERVER - FULL ACTIVE/ACTIVE HA
Running Rancher server in High Availability (HA) is as easy as running Rancher server using an external database, exposing an additional port, and adding in an additional argument to the command for the external load balancer.

REQUIREMENTS FOR HA
HA Nodes:
Any modern Linux distribution with a supported version of Docker. RancherOS, Ubuntu, RHEL/CentOS 7 are more heavily tested.
For RHEL/CentOS, the default storage driver, i.e. devicemapper using loopback, is not recommended by Docker. Please refer to the Docker documentation on how to change it.
For RHEL/CentOS, if you want to enable SELinux, you will need to install an additional SELinux module.
Ports that needs to be opened between nodes: 9345, 8080
1GB RAM
MySQL database
At least 1 GB RAM
50 connections per Rancher server node (e.g. A 3 node setup will need to support at least 150 connections)
MYSQL Configuration Requirements
Option 1: Run with Antelope with default of COMPACT
Option 2: Run MySQL 5.7 with Barracuda where the default ROW_FORMAT is Dynamic
External Load Balancer
Port that needs to be opened between nodes and external load balancer: 8080
NOTE:
Currently, Docker for Mac is not supported in Rancher.

RECOMMENDATIONS FOR LARGER DEPLOYMENTS
Each Rancher server node should have a 4 GB or 8 GB heap size, which requires having at least 8 GB or 16 GB of RAM
MySQL database should have fast disks
For true HA, a replicated MySQL database with proper backups is recommended. Using Galera and forcing writes to a single node, due to transaction locks, would be an alternative.
On each of your nodes that you want to add into the HA setup, run the following command:

# Launch on each node in your HA cluster
$ docker run -d --restart=unless-stopped -p 8080:8080 -p 9345:9345 rancher/server \
     --db-host myhost.example.com --db-port 3306 --db-user username --db-pass password --db-name cattle \
     --advertise-address <IP_of_the_Node>
For each node, the <IP_of_the_Node> will be unique to each node, as it will be the IP of each specific node that is being added into the HA setup.

If you change -p 8080:8080 to expose the HTTP port to a different port on the host, you will need to add --advertise-http-port <host_port> to the command.

NOTE:
You can get the help for the commands by running docker run rancher/server --help

Configure an external load balancer that will balance traffic on ports 80 and 443 across a pool of nodes that will be running Rancher server and target the nodes on port 8080. Your load balancer must support websockets and forwarded-for headers, in order for Rancher to function properly. See SSL settings page for example configuration settings.

NOTES ON THE RANCHER SERVER NODES IN HA
If the IP of your Rancher server node changes, your node will no longer be part of the Rancher HA cluster. You must stop the old Rancher server container using the incorrect IP for --advertise-address and start a new Rancher server with the correct IP for --advertise-address.


RUNNING RANCHER SERVER BEHIND AN ELASTIC/CLASSIC LOAD BALANCER (ELB) IN AWS
We recommend using an ELB in AWS in front of your Rancher servers. In order for ELB to work correctly with Rancher’s websockets, you will need to enable proxy protocol mode and ensure HTTP support is disabled. By default, ELB is enabled in HTTP/HTTPS mode, which does not support websockets. Special attention must be paid to listener configuration.

If you have issues with ELB setup, we recommend trying the terraform version as this reduces the opportunity to miss a setting.

NOTE:
If you are using a self signed certificate, please read more about how to configure your ELB in AWS under our SSL section.

LISTENER CONFIGURATION - PLAINTEXT
For simple, unencrypted load balancing purposes, the following listener configuration is required:

Configuration Type	Load Balancer Protocol	Load Balancer Port	Instance Protocol	Instance Port
Plaintext	TCP	80	TCP	8080 (or the port used with --advertise-http-port when launching Rancher server)
ENABLING PROXY PROTOCOL
In order for websockets to function properly, the ELB proxy protocol policy must be applied.

Enable proxy protocol mode
$ aws elb create-load-balancer-policy --load-balancer-name <LB_NAME> --policy-name <POLICY_NAME> --policy-type-name ProxyProtocolPolicyType --policy-attributes AttributeName=ProxyProtocol,AttributeValue=true
$ aws elb set-load-balancer-policies-for-backend-server --load-balancer-name <LB_NAME> --instance-port 443 --policy-names <POLICY_NAME>
$ aws elb set-load-balancer-policies-for-backend-server --load-balancer-name <LB_NAME> --instance-port 8080 --policy-names <POLICY_NAME>
Health check can be configured to use HTTP:8080 using /ping as your path.
CONFIGURING USING TERRAFORM
The following can be used as an example for configuring with Terraform:

resource "aws_elb" "lb" {
  name               = "<LB_NAME>"
  availability_zones = ["us-west-2a","us-west-2b","us-west-2c"]
  security_groups = ["<SG_ID>"]

  listener {
    instance_port     = 8080
    instance_protocol = "tcp"
    lb_port           = 443
    lb_protocol       = "ssl"
    ssl_certificate_id = "<IAM_PATH_TO_CERT>"
  }

}

resource "aws_proxy_protocol_policy" "websockets" {
  load_balancer  = "${aws_elb.lb.name}"
  instance_ports = ["8080"]
}

RUNNING RANCHER SERVER BEHIND AN APPLICATION LOAD BALANCER (ALB) IN AWS
We no longer recommend Application Load Balancer (ALB) in AWS over using the Elastic/Classic Load Balancer (ELB). If you still choose to use an ALB, you will need to direct the traffic to the HTTP port on the nodes, which is 8080 by default.


ENABLING ACTIVE DIRECTORY OR OPENLDAP FOR TLS
In order to enable Active Directory or OpenLDAP for Rancher server with TLS, the Rancher server container will need to be started with the LDAP certificate, provided by your LDAP setup. On the Linux machine that you want to launch Rancher server on, save the certificate.

Start Rancher by bind mounting the volume that has the certificate. The certificate must be called ca.crt inside the container.

$ sudo docker run -d --restart=unless-stopped -p 8080:8080 \
  -v /some/dir/cert.crt:/var/lib/rancher/etc/ssl/ca.crt rancher/server
You can check that the ca.crt was passed to Rancher server container successfully by checking the logs of the rancher server container.

$ docker logs <SERVER_CONTAINER_ID>
In the beginning of the logs, there will be confirmation that the certificate was added correctly.

Adding ca.crt to Certs.
Updating certificates in /etc/ssl/certs... 1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....done.
Certificate was added to keystore

LAUNCHING RANCHER SERVER BEHIND AN HTTP PROXY
In order to set up an HTTP proxy, the Docker daemon will need to be modified to point to the proxy. Before starting Rancher server, edit the /etc/default/docker file to point to your proxy and restart Docker.

$ sudo vi /etc/default/docker
In the file, edit the #export http_proxy="http://127.0.0.1:3128/" to have it point to your proxy. Save your changes and then restart docker. Restarting Docker is different on every OS.

NOTE:
If you are running Docker with systemd, please follow Docker’s instructions on how to configure the HTTP proxy.

In order for the Rancher catalog to load, the proxy will need to be configured and Rancher server will need to be launched with environment variables to pass in the proxy information.

$ sudo docker run -d \
    -e http_proxy=<proxyURL> \
    -e https_proxy=<proxyURL> \
    -e no_proxy="localhost,127.0.0.1" \
    -e NO_PROXY="localhost,127.0.0.1" \
    --restart=unless-stopped -p 8080:8080 rancher/server
If the Rancher catalog will not be used, run the Rancher server command as you normally would.

When adding hosts to Rancher, there is no additional requirements behind an HTTP proxy.

Copyright © 2017 Rancher Labs. All Rights Reserved.

Run this command in the host that is running Rancher server.

When you click Close on the Rancher UI, you will be directed back to the Infrastructure -> Hosts view. In a couple of minutes, the host will automatically appear.

INFRASTRUCTURE SERVICES
When you first log in to Rancher, you are automatically in a Default environment. The default cattle environment template has been selected for this environment to launch infrastructure services. These infrastructure services are required to be launched to take advantage of Rancher’s benefits like dns, metadata, networking, and health checks. These infrastructure stacks can be found in Stacks -> Infrastructure. These stacks will be in an unhealthy state until a host is added into Rancher. After adding a host, it is recommended to wait until all the infrastructure stacks are active before adding services.

On the host, the containers from the infrastructure services will be hidden unless you click on the Show System checkbox.

CREATE A CONTAINER THROUGH UI
Navigate to the Stacks page, if you see the welcome screen, you can click on the Define a Service button in the welcome screen. If there are already services in your Rancher set up, you can click on Add Service in any existing stack or create a new stack to add services in. A stack is just a convenient way to group services together. If you need to create a new stack, click on Add Stack, provide a name and description and click Create. Then, click on Add Service in the new stack.

Provide the service with a name like “first-service”. You can just use our default settings and click Create. Rancher will start launching the container on the host. Regardless what IP address your host has, the first-container will have an IP address in the 10.42.*.* range as Rancher has created a managed overlay network with the ipsec infrastructure service. This managed overlay network is how containers can communicate with each other across different hosts.

If you click on the dropdown of the first-container, you will be able to perform management actions like stopping the container, viewing the logs, or accessing the container console.

CREATE A CONTAINER THROUGH NATIVE DOCKER CLI
Rancher will display any containers on the host even if the container is created outside of the UI. Create a container in the host’s shell terminal.

$ docker run -d -it --name=second-container ubuntu:14.04.2
In the UI, you will see second-container pop up on your host!

Rancher reacts to events that happen on the Docker daemon and does the right thing to reconcile its view of the world with reality. You can read more about using Rancher with the native docker CLI.

If you look at the IP address of the second-container, you will notice that it is not in the 10.42.*.* range. It instead has the usual IP address assigned by the Docker daemon. This is the expected behavior of creating a Docker container through the CLI.

What if we want to create a Docker container through CLI and still give it an IP address from Rancher’s overlay network? All we need to do is add a label (i.e. io.rancher.container.network=true) in the command to let Rancher know that you want this container to be part of the managed network.

$ docker run -d -it --label io.rancher.container.network=true ubuntu:14.04.2
CREATE A MULTI-CONTAINER APPLICATION
We have shown you how to create individual containers and explained how they would be connected in our cross-host network. Most real-world applications, however, are made out of multiple services, with each service made up of multiple containers. A LetsChat application, for example, could consist of the following services:

A load balancer. The load balancer redirects Internet traffic to the “LetsChat” application.
A web service consisting of two “LetsChat” containers.
A database service consisting of one “Mongo” container.
The load balancer targets the web service (i.e. LetsChat), and the web service will link to the database service (i.e. Mongo).

In this section, we will walk through how to create and deploy the LetsChat application in Rancher.

Navigate to the Stacks page, if you see the welcome screen, you can click on the Define a Service button in the welcome screen. If there are already services in your Rancher set up, you can click on Add Stack to create a new stack. Provide a name and description and click Create. Then, click on Add Service in the new stack.

First, we’ll create a database service called database and use the mongo image. Click Create. You will be immediately brought to a stack page, which will contain the newly created database service.

Next, click on Add Service again to add another service. We’ll add a LetsChat service and link to the database service. Let’s use the name, web, and use the sdelements/lets-chat image. In the UI, we’ll move the slider to have the scale of the service to be 2 containers. In the Service Links, add the database service and provide the name mongo. Just like in Docker, Rancher will link the necessary environment variables in the letschat image from the linked database when you input the “as name” as mongo. Click Create.

Finally, we’ll create our load balancer. Click on the dropdown menu icon next to the Add Service button. Select Add Load Balancer. Provide a name like letschatapplb. Input the source port (i.e. 80), select the target service (i.e. web), and select a target port (i.e. 8080). The web service is listening on port 8080. Click Create.

Our LetsChat application is now complete! On the Stacks page, you’ll be able to find the exposed port of the load balancer as a link. Click on that link and a new browser will open, which will display the LetsChat application.

CREATE A MULTI-CONTAINER APPLICATION USING RANCHER CLI
In this section, we will show you how to create and deploy the same LetsChat application we created in the previous section using our command-line tool called Rancher CLI.

When bringing services up in Rancher, the Rancher CLI tool works similarly to the popular Docker Compose tool. It takes in the same docker-compose.yml file and deploys the application on Rancher. You can specify additional attributes in a rancher-compose.yml file which extends and overwrites the docker-compose.yml file.

In the previous section, we created a LetsChat application with a load balancer. If you had created it in Rancher, you can download the files directly from our UI by selecting Export Config from the stack’s dropdown menu. The docker-compose.yml and rancher-compose.yml files would look like this:

EXAMPLE DOCKER-COMPOSE.YML
version: '2'
services:
  letschatapplb:
    #If you only have 1 host and also created the host in the UI,
    # you may have to change the port exposed on the host.
    ports:
    - 80:80/tcp
    labels:
      io.rancher.container.create_agent: 'true'
      io.rancher.container.agent.role: environmentAdmin
    image: rancher/lb-service-haproxy:v0.4.2
  web:
    labels:
      io.rancher.container.pull_image: always
    tty: true
    image: sdelements/lets-chat
    links:
    - database:mongo
    stdin_open: true
  database:
    labels:
      io.rancher.container.pull_image: always
    tty: true
    image: mongo
    stdin_open: true
EXAMPLE RANCHER-COMPOSE.YML
version: '2'
services:
  letschatapplb:
    scale: 1
    lb_config:
      certs: []
      port_rules:
      - hostname: ''
        path: ''
        priority: 1
        protocol: http
        service: quickstartguide/web
        source_port: 80
        target_port: 8080
    health_check:
      port: 42
      interval: 2000
      unhealthy_threshold: 3
      healthy_threshold: 2
      response_timeout: 2000
  web:
    scale: 2
  database:
    scale: 1

Download the Rancher CLI binary from the Rancher UI by clicking on Download CLI, which is located on the right side of the footer. We provide the ability to download binaries for Windows, Mac, and Linux.

In order for services to be launched in Rancher using Rancher CLI, you will need to set some environment variables. You will need to create an account API Key in the Rancher UI. Click on API and click on Add Account API Key. Save the username (access key) and password (secret key). Set up the environment variables needed for Rancher CLI: RANCHER_URL, RANCHER_ACCESS_KEY, and RANCHER_SECRET_KEY.

# Set the url that Rancher is on
$ export RANCHER_URL=http://server_ip:8080/
# Set the access key, i.e. username
$ export RANCHER_ACCESS_KEY=<username_of_key>
# Set the secret key, i.e. password
$ export RANCHER_SECRET_KEY=<password_of_key>

Now, navigate to the directory where you saved docker-compose.yml and rancher-compose.yml and run the command.

$ rancher up -d -s NewLetsChatApp

In Rancher, a new stack will be created called NewLetsChatApp with all of the services launched in Rancher.
RabbitMQPivotal

Search RabbitMQ

FeaturesInstallationDocsTutorialsSupportCommunityPivotal is HiringBlog
RabbitMQ Configuration

Overview

RabbitMQ comes with default built-in settings which will quite likely be sufficient for running your RabbitMQ server effectively. If it runs fine, then you possibly don't need any configuration at all.

RabbitMQ provides three general ways to customise the server:

environment variables
define ports, file locations and names (taken from the shell, or set in the environment configuration file, rabbitmq-env.conf/rabbitmq-env-conf.bat)
a configuration file
defines server component settings for permissions, limits and clusters, and also plugin settings.
runtime parameters and policies
defines cluster-wide settings which can change at run time
Most settings are configured using the first two methods. This guide, therefore, focuses on them.
Config File Locations
Default config file locations vary between operating systems and package types. This topic is covered in more details in the rest of this guide.

To verify effective RabbitMQ config file location, see the section below.

Verify Configuration (How to Find Config File Location)
The active configuration file can be verified by inspecting RabbitMQ log file. It will show up in the log file at the top together with other broker boot log entries, for example:

node           : rabbit@example
home dir       : /var/lib/rabbitmq
config file(s) : /etc/rabbitmq/rabbitmq.config
In case log file cannot be found or read by RabbitMQ, log entry will say so:
node           : rabbit@example
home dir       : /var/lib/rabbitmq
config file(s) : /var/lib/rabbitmq/hare.config (not found)
Customise RabbitMQ Environment

Certain server parameters can be configured using environment variables: node name, RabbitMQ configuration file location, AMQP 0-9-1 and inter-node communication ports, and so on.

Unix (general)
On Unix-based systems (including Linux, MacOSX) you can create/edit rabbitmq-env.conf to define environment variables. Its location is configurable using the RABBITMQ_CONF_ENV_FILE environment variable.

Use the standard environment variable names (but drop the RABBITMQ_ prefix) e.g.

#example rabbitmq-env.conf file entries
#Rename the node
NODENAME=bunny@myhost
#Config file location and new filename bunnies.config
CONFIG_FILE=/etc/rabbitmq/testdir/bunnies
More info on using rabbitmq-env.conf
Windows
If you need to customise names, ports, locations, it is easiest to configure environment variables in the Windows dialogue: Start > Settings > Control Panel > System > Advanced > Environment Variables. Then create or edit the system variable name and value.

Alternatively, you can create/edit rabbitmq-env-conf.bat to define environment variables. Its location is configurable using the RABBITMQ_CONF_ENV_FILE environment variable.

For environment changes to take effect on Windows, the service must be re-installed. It is not sufficient to restart the service. This can be done using the installer or on the command line with administrator permissions:

Start an admin command prompt
cd into the sbin folder under RabbitMQ server installation directory (e.g. C:\Program Files (x86)\RabbitMQ Server\rabbitmq_server-3.6.6\sbin)
Run rabbitmq-service.bat remove
Set environment variables via command line, i.e. run commands like the following: set RABBITMQ_BASE=c:\Data\RabbitMQ
Run rabbitmq-service.bat install
Alternative, if new configuration needs to take effect after next broker restart, one step can be skipped:
Start an admin command prompt
cd into the sbin folder under RabbitMQ server installation directory
Set environment variables via command line
Run rabbitmq-service.bat install, which will only update service parameters
RabbitMQ Environment Variables

RabbitMQ environment variable names have the prefix RABBITMQ_. A typical variable called RABBITMQ_var_name is set as follows:

a shell environment variable called RABBITMQ_var_name is used if this is defined;
otherwise, a variable called var_name is used if this is set in the rabbitmq-env.conf file;
otherwise, a system-specified default value is used.
In this way, variables set in the shell environment take priority over variables set in rabbitmq-env.conf, which in turn over-ride RabbitMQ built-in defaults.

It is unlikely you will need to set any of these environment variables. If you have non-standard requirements, then RabbitMQ environment variables include, but are not limited to:

Name	Default	Description
RABBITMQ_NODE_IP_ADDRESS	the empty string - meaning bind to all network interfaces.	Change this if you only want to bind to one network interface. To bind to two or more interfaces, use the tcp_listeners key in rabbitmq.config.
RABBITMQ_NODE_PORT	5672	
RABBITMQ_DIST_PORT	RABBITMQ_NODE_PORT + 20000	Port used for inter-node and CLI tool communition. Ignored if your config file sets kernel.inet_dist_listen_min or kernel.inet_dist_listen_max keys. See Networking for details.
RABBITMQ_NODENAME	
Unix*: rabbit@$HOSTNAME
Windows: rabbit@%COMPUTERNAME%
The node name should be unique per erlang-node-and-machine combination. To run multiple nodes, see the clustering guide.
RABBITMQ_CONF_ENV_FILE	
Generic UNIX - $RABBITMQ_HOME/etc/rabbitmq/rabbitmq-env.conf
Debian - /etc/rabbitmq/rabbitmq-env.conf
RPM - /etc/rabbitmq/rabbitmq-env.conf
Mac OS X (Homebrew) - ${install_prefix}/etc/rabbitmq/rabbitmq-env.conf, the Homebrew prefix is usually /usr/local
Windows - %APPDATA%\RabbitMQ\rabbitmq-env-conf.bat
Location of the file that contains environment variable definitions (without the RABBITMQ_ prefix). Note that the file name on Windows is different from other operating systems.
RABBITMQ_USE_LONGNAME		When set to true this will cause RabbitMQ to use fully qualified names to identify nodes. This may prove useful on EC2. Note that it is not possible to switch between using short and long names without resetting the node.
RABBITMQ_SERVICENAME	Windows Service: RabbitMQ	The name of the installed service. This will appear in services.msc.
RABBITMQ_CONSOLE_LOG	Windows Service:	Set this variable to new or reuse to redirect console output from the server to a file named %RABBITMQ_SERVICENAME%.debug in the default RABBITMQ_BASE directory.
If not set, console output from the server will be discarded (default).
new A new file will be created each time the service starts.
reuse The file will be overwritten each time the service starts.
RABBITMQ_CTL_ERL_ARGS	None	Parameters for the erl command used when invoking rabbitmqctl. This should be overridden for debugging purposes only.
RABBITMQ_SERVER_ERL_ARGS	
Unix*: "+K true +A30 +P 1048576 -kernel inet_default_connect_options [{nodelay,true}]"
Windows: None
Standard parameters for the erl command used when invoking the RabbitMQ Server. This should be overridden for debugging purposes only. Overriding this variable replaces the default value.
RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS	
Unix*: None
Windows: None
Additional parameters for the erl command used when invoking the RabbitMQ Server. The value of this variable is appended the default list of arguments (RABBITMQ_SERVER_ERL_ARGS).
RABBITMQ_SERVER_START_ARGS	None	Extra parameters for the erl command used when invoking the RabbitMQ Server. This will not override RABBITMQ_SERVER_ERL_ARGS.
* Unix, Linux, MacOSX
In addition, there are several environment variables which tell RabbitMQ where to locate its database, log files, plugins, configuration etc.

Other variables upon which RabbitMQ depends are:

Name	Default	Description
HOSTNAME	
Unix, Linux: `env hostname`
MacOSX: `env hostname -s`
The name of the current machine
COMPUTERNAME	Windows: localhost	The name of the current machine
ERLANG_SERVICE_MANAGER_PATH	Windows Service: %ERLANG_HOME%\erts-x.x.x\bin	This path is the location of erlsrv.exe, the Erlang service wrapper script.
Configuration File

The rabbitmq.config File
The configuration file rabbitmq.config allows the RabbitMQ core application, Erlang services and RabbitMQ plugins to be configured. It is a standard Erlang configuration file, documented on the Erlang Config Man Page.

An minimalistic example configuration file follows:

  [
    {rabbit, [{tcp_listeners, [5673]}]}
  ].
This example will alter the port RabbitMQ listens on for AMQP 0-9-1 client connections from 5672 to 5673.

To override main RabbitMQ config file location, use the RABBITMQ_CONFIG_FILE environment variable.

Note that this configuration file is not the same as the environment configuration file, rabbitmq-env.conf, which can be used to set environment variables on non-Windows systems.

Location of rabbitmq.config and rabbitmq-env.conf
The location of these files is distribution-specific. By default, they are not created, but expect to be located in the following places on each platform:

Generic UNIX - $RABBITMQ_HOME/etc/rabbitmq/
Debian - /etc/rabbitmq/
RPM - /etc/rabbitmq/
Mac OS X (Homebrew) - ${install_prefix}/etc/rabbitmq/, the Homebrew prefix is usually /usr/local
Windows - %APPDATA%\RabbitMQ\
If rabbitmq-env.conf doesn't exist, it can be created manually in the location, specified by the RABBITMQ_CONF_ENV_FILE variable. On Windows systems, it is named rabbitmq-env.bat.

If rabbitmq.config doesn't exist, it can be created manually. Set the RABBITMQ_CONFIG_FILE environment variable if you change the location. The Erlang runtime automatically appends the .config extension to the value of this variable.

Restart the server after changes. Windows service users will need to re-install the service after adding or removing a configuration file.

Example rabbitmq.config File
RabbitMQ server source repository contains an example configuration file named rabbitmq.config.example. This example file contains an example of most of the configuration items you might want to set (with some very obscure ones omitted) along with documentation for those settings. All configuration items are commented out in the example, so you can uncomment what you need. Note that the example file is meant to be used as, well, example, and should not be treated as a general recommendation.

In most distributions we place this example file in the same location as the real file should be placed (see above). However, for the Debian and RPM distributions policy forbids doing so; instead you can find it in /usr/share/doc/rabbitmq-server/ or /usr/share/doc/rabbitmq-server-3.6.6/ respectively.

Variables Configurable in rabbitmq.config
Many users of RabbitMQ never need to change any of these values, and some are fairly obscure. However, for completeness they are all listed here.

Key	Documentation
tcp_listeners	List of ports on which to listen for AMQP connections (without SSL). Can contain integers (meaning "listen on all interfaces") or tuples such as {"127.0.0.1", 5672} to listen on one or more interfaces.
Default: [5672]

num_tcp_acceptors	Number of Erlang processes that will accept connections for the TCP listeners.
Default: 10

handshake_timeout	Maximum time for AMQP 0-8/0-9/0-9-1 handshake (after socket connection and SSL handshake), in milliseconds.
Default: 10000

ssl_listeners	As above, for SSL connections.
Default: []

num_ssl_acceptors	Number of Erlang processes that will accept connections for the SSL listeners.
Default: 1

ssl_options	SSL configuration. See the SSL documentation.
Default: []

ssl_handshake_timeout	SSL handshake timeout, in milliseconds.
Default: 5000

vm_memory_high_watermark	Memory threshold at which the flow control is triggered. See the memory-based flow control documentation.
Default: 0.4

vm_memory_high_watermark_paging_ratio	Fraction of the high watermark limit at which queues start to page messages out to disc to free up memory. See the memory-based flow control documentation.
Default: 0.5

disk_free_limit	Disk free space limit of the partition on which RabbitMQ is storing data. When available disk space falls below this limit, flow control is triggered. The value may be set relative to the total amount of RAM (e.g. {mem_relative, 1.0}). The value may also be set to an integer number of bytes. Or, alternatively, in information units (e.g "50MB"). By default free disk space must exceed 50MB. See the Disk Alarms documentation.
Default: 50000000

log_levels	Controls the granularity of logging. The value is a list of log event category and log level pairs.
The level can be one of 'none' (no events are logged), 'error' (only errors are logged), 'warning' (only errors and warning are logged), 'info' (errors, warnings and informational messages are logged), or 'debug' (errors, warnings, informational messages and debugging messages are logged).

At present there are four categories defined. Other, currently uncategorised, events are always logged.

The categories are:

channel - for all events relating to AMQP channels
connection - for all events relating to network connections
federation - for all events relating to federation
mirroring - for all events relating to mirrored queues
Default: [{connection, info}]

frame_max	Maximum permissible size of a frame (in bytes) to negotiate with clients. Setting to 0 means "unlimited" but will trigger a bug in some QPid clients. Setting a larger value may improve throughput; setting a smaller value may improve latency.
Default: 131072

channel_max	Maximum permissible number of channels to negotiate with clients. Setting to 0 means "unlimited". Using more channels increases memory footprint of the broker.
Default: 0

channel_operation_timeout	Channel operation timeout in milliseconds (used internally, not directly exposed to clients due to messaging protocol differences and limitations).
Default: 15000

heartbeat	Value representing the heartbeat delay, in seconds, that the server sends in the connection.tune frame. If set to 0, heartbeats are disabled. Clients might not follow the server suggestion, see the AMQP reference for more details. Disabling heartbeats might improve performance in situations with a great number of connections, but might lead to connections dropping in the presence of network devices that close inactive connections.
Default: 60 (580 prior to release 3.5.5)

default_vhost	Virtual host to create when RabbitMQ creates a new database from scratch. The exchange amq.rabbitmq.log will exist in this virtual host.
Default: <<"/">>

default_user	User name to create when RabbitMQ creates a new database from scratch.
Default: <<"guest">>

default_pass	Password for the default user.
Default: <<"guest">>

default_user_tags	Tags for the default user.
Default: [administrator]

default_permissions	Permissions to assign to the default user when creating it.
Default: [<<".*">>, <<".*">>, <<".*">>]

loopback_users	List of users which are only permitted to connect to the broker via a loopback interface (i.e. localhost).
If you wish to allow the default guest user to connect remotely, you need to change this to [].

Default: [<<"guest">>]

cluster_nodes	Set this to cause clustering to happen automatically when a node starts for the very first time. The first element of the tuple is the nodes that the node will try to cluster to. The second element is either disc or ram and determines the node type.
Default: {[], disc}

server_properties	List of key-value pairs to announce to clients on connection.
Default: []

collect_statistics	Statistics collection mode. Primarily relevant for the management plugin. Options are:
none (do not emit statistics events)
coarse (emit per-queue / per-channel / per-connection statistics)
fine (also emit per-message statistics)
You probably don't want to change this yourself.
Default: none

collect_statistics_interval	Statistics collection interval in milliseconds. Primarily relevant for the management plugin.
Default: 5000

auth_mechanisms	SASL authentication mechanisms to offer to clients.
Default: ['PLAIN', 'AMQPLAIN']

auth_backends	
List of authentication / authorisation backends to use. This list can contain names of modules (in which case the same module is used for both authentication and authorisation), or 2-tuples like {ModN, ModZ} in which case ModN is used for authentication and ModZ is used for authorisation.

In the 2-tuple case, ModZ can be replaced by a list, all the elements of which must confirm each authorisation query, e.g. {ModN, [ModZ1, ModZ2]}. This allows authorisation plugins to mix-in and provide additional security constraints.

Other databases than rabbit_auth_backend_internal are available through plugins.

Default: [rabbit_auth_backend_internal]

reverse_dns_lookups	Set to true to have RabbitMQ perform a reverse DNS lookup on client connections, and present that information through rabbitmqctl and the management plugin.
Default: false

delegate_count	Number of delegate processes to use for intra-cluster communication. On a machine which has a very large number of cores and is also part of a cluster, you may wish to increase this value.
Default: 16

trace_vhosts	Used internally by the tracer. You shouldn't change this.
Default: []

tcp_listen_options	Default socket options. You probably don't want to change this.
Default:

[{backlog,       128},
         {nodelay,       true},
         {linger,        {true,0}},
         {exit_on_close, false}]
hipe_compile	Set to true to precompile parts of RabbitMQ with HiPE, a just-in-time compiler for Erlang. This will increase server throughput at the cost of increased startup time.
You might see 20-50% better performance at the cost of a few minutes delay at startup. These figures are highly workload- and hardware-dependent.

HiPE support may not be compiled into your Erlang installation. If it is not, enabling this option will just cause a warning message to be displayed and startup will proceed as normal. For example, Debian / Ubuntu users will need to install the erlang-base-hipe package.

HiPE is not available at all on some platforms, notably including Windows.

HiPE has known issues in Erlang/OTP versions prior to 17.5. Using a recent Erlang/OTP version is highly recommended for HiPE.

Default: false

cluster_partition_handling	How to handle network partitions. Available modes are:
ignore
pause_minority
{pause_if_all_down, [nodes], ignore | autoheal} where [nodes] is a list of node names
(ex: ['rabbit@node1', 'rabbit@node2'])
autoheal
See the documentation on partitions for more information.
Default: ignore

cluster_keepalive_interval	How frequently nodes should send keepalive messages to other nodes (in milliseconds). Note that this is not the same thing as net_ticktime; missed keepalive messages will not cause nodes to be considered down.
Default: 10000

queue_index_embed_msgs_below	Size in bytes of message below which messages will be embedded directly in the queue index. You are advised to read the persister tuning documentation before changing this.
Default: 4096

msg_store_index_module	Implementation module for queue indexing. You are advised to read the persister tuning documentation before changing this.
Default: rabbit_msg_store_ets_index

backing_queue_module	Implementation module for queue contents. You probably don't want to change this.
Default: rabbit_variable_queue

msg_store_file_size_limit	Tunable value for the persister. You almost certainly should not change this.
Default: 16777216

mnesia_table_loading_timeout	Timeout used when waiting for Mnesia tables in a cluster to become available.
Default: 30000

queue_index_max_ journal_entries	Tunable value for the persister. You almost certainly should not change this.
Default: 65536

queue_master_locator	Queue master location strategy. Available strategies are:
<<"min-masters">>
<<"client-local">>
<<"random">>
See the documentation on queue master location for more information.
Default: <<"client-local">>

lazy_queue_explicit_ gc_run_operation_threshold	Tunable value only for lazy queues when under memory pressure. This is the threshold at which the garbage collector is triggered. A low value could reduce performance, and a high one can improve performance, but cause higher memory consumption. You almost certainly should not change this.
Default: 250

In addition, many plugins can have sections in the configuration file, with names of the form rabbitmq_plugin. Our maintained plugins are documented in the following locations:

rabbitmq_management
rabbitmq_management_agent
rabbitmq_web_dispatch
rabbitmq_stomp
rabbitmq_shovel
rabbitmq_auth_backend_ldap
Configuration entry encryption
Sensitive configuration entries (e.g. password, URL containing credentials) can be encrypted in the RabbitMQ configuration file. The broker decrypts encrypted entries on start.

Note that encrypted configuration entries don't make the system meaningfully more secure. Nevertheless, they allow deployments of RabbitMQ to conform to regulations in various countries requiring that no sensitive data should appear in plain text in configuration files.

Encrypted values must be inside an Erlang encrypted tuple: {encrypted, ...}. Here is an example of a configuration file with an encrypted password for the default user:

[
  {rabbit, [
      {default_user, <<"guest">>},
      {default_pass,
        {encrypted,
         <<"cPAymwqmMnbPXXRVqVzpxJdrS8mHEKuo2V+3vt1u/fymexD9oztQ2G/oJ4PAaSb2c5N/hRJ2aqP/X0VAfx8xOQ==">>
        }
      },
      {config_entry_decoder, [
             {passphrase, <<"mypassphrase">>}
         ]}
    ]}
].
            
Note the config_entry_decoder key with the passphrase that RabbitMQ will use to decrypt encrypted values.
The passphrase doesn't have to be hardcoded in the configuration file, it can be in a separate file:

[
  {rabbit, [
      ...
      {config_entry_decoder, [
             {passphrase, {file, "/path/to/passphrase/file"}}
         ]}
    ]}
].
            
RabbitMQ can also request an operator to enter the passphrase when it starts by using {passphrase, prompt}.
Use rabbitmqctl and the encode command to encrypt values:

$ rabbitmqctl encode '<<"guest">>' mypassphrase
{encrypted,<<"... long encrypted value...">>}
$ rabbitmqctl encode '"amqp://fred:secret@host1.domain/my_vhost"' mypassphrase
{encrypted,<<"... long encrypted value...">>}
            
Add the --decode option if you want to decrypt values:
$ rabbitmqctl encode --decode '{encrypted, <<"...">>}' mypassphrase
<<"guest">>
$ rabbitmqctl encode --decode '{encrypted, <<"...">>}' mypassphrase
"amqp://fred:secret@host1.domain/my_vhost"
            
The encryption mechanism uses PBKDF2 to produce a derived key from the passphrase. The default hash function is SHA512 and the default number of iterations is 1000. The default cipher is AES 256 CBC.

You can change these defaults in the configuration file:

[
  {rabbit, [
      ...
      {config_entry_decoder, [
             {passphrase, "mypassphrase"},
             {cipher, blowfish_cfb64},
             {hash, sha256},
             {iterations, 10000}
         ]}
    ]}
].          
On the command line:
$ rabbitmqctl encode --cipher blowfish_cfb64 --hash sha256 --iterations 10000 \
                     '<<"guest">>' mypassphrase
            
In This Section
Server Documentation
Configuration
File Locations
Persistence
Networking
Parameters
TLS/SSL Support
Distributed RabbitMQ
Reliable Delivery
Clustering
High Availability
High Availability (pacemaker)
Access Control (Authorisation)
Production Checklist
SASL Authentication
Alarms
Networking
Memory Use
Lazy Queues
Firehose / Tracing
Manual Pages
Windows Quirks
Client Documentation
Plugins
News
Protocol
Our Extensions
Building
Previous Releases
License
In This Page
Overview
Customise RabbitMQ Environment
RabbitMQ Environment Variables
Configuration File
Sitemap | Contact | This Site is Open Source | Pivotal is Hiring

Copyright ? 2007-Present Pivotal Software, Inc. All rights reserved. Terms of Use, Privacy and Trademark Guidelines


# Save the DB on disk:
#
#   save <seconds> <changes>
#
#   Will save the DB if both the given number of seconds and the given
#   number of write operations against the DB occurred.
#
#   In the example below the behaviour will be to save:
#   after 900 sec (15 min) if at least 1 key changed
#   after 300 sec (5 min) if at least 10 keys changed
#   after 60 sec if at least 10000 keys changed
#
#   Note: you can disable saving at all commenting all the "save" lines.
#
#   It is also possible to remove all the previously configured save
#   points by adding a save directive with a single empty string argument
#   like in the following example:
#
#   save ""
Browse Current Documentation - (Module Index)
What's new in Python 2.7
Tutorial
Library Reference
Language Reference
Extending and Embedding
Python/C API
Using Python
Python HOWTOs
Search the online docs
Download Current Documentation (multiple formats are available, including typeset versions for printing.)
VMware ESXi 5.5 Update 3a Release Notes

VMware ESXi? 5.5 Update 3a | 6 OCT 2015 | Build 3116895
Last updated: 6 OCT 2015
Check for additions and updates to these release notes.
What's in the Release Notes

The release notes cover the following topics:
What's New
Earlier Releases of ESXi 5.5
Internationalization
Compatibility and Installation
Upgrades for This Release
Open Source Components for VMware vSphere
Product Support Notices
Patches Contained in this Release
Resolved Issues
Known Issues
What's New

This release of VMware ESXi contains the following enhancements:
Log Rotation Enablement – Log rotation for vmx files allows you to reduce the log file sizes by specifying the size of each log and the number of previous logs to keep.


Certification of PVSCSI Adapter – PVSCSI adapter is certified for use with MSCS, core clustering and applications including SQL and Exchange. This creates performance gains when moving from LSI Logic SAS to PVSCSI.


Support for Next Generation Processors – In this release, we will continue our support for next generation processors from Intel and AMD. Please see the VMware Compatibility Guide for more info.


ESXi Authentication for Active Directory – ESXi is modified to only support AES256-CTS/AES128-CTS/RC4-HMAC encryption for Kerberos communication between ESXi and Active Directory.


Resolved Issues – This release delivers a number of bug fixes that have been documented in the Resolved Issues section.
Earlier Releases of ESXi 5.5

Features and known issues of ESXi 5.5 are described in the release notes for each release. Release notes for earlier releases of ESXi 5.5, are:
VMware ESXi 5.5 Update 2 Release Notes
VMware ESXi 5.5 Update 1 Release Notes
VMware vSphere 5.5 Release Notes
Internationalization

VMware vSphere 5.5 Update 3a is available in the following languages:
English
French
German
Japanese
Korean
Simplified Chinese
Traditional Chinese
Compatibility and Installation

ESXi, vCenter Server, and vSphere Web Client Version Compatibility

The VMware Product Interoperability Matrix provides details about the compatibility of current and earlier versions of VMware vSphere components, including ESXi, VMware vCenter Server, the vSphere Web Client, and optional VMware products. Check the VMware Product Interoperability Matrix also for information about supported management and backup agents before you install ESXi or vCenter Server.
The vSphere Client and the vSphere Web Client are packaged on the vCenter Server ISO. You can install one or both clients by using the VMware vCenter? Installer wizard.
ESXi, vCenter Server, and VDDK Compatibility

Virtual Disk Development Kit (VDDK) 5.5.3 adds support for ESXi 5.5 Update 3 and vCenter Server 5.5 Update 3 releases.
For more information about VDDK, see http://www.vmware.com/support/developer/vddk/.
ESXi and Virtual SAN Compatibility

Virtual SAN does not support clusters that are configured with ESXi hosts earlier than 5.5 Update 1. Make sure all hosts in the Virtual SAN cluster are upgraded to ESXi 5.5 Update 1 or later, before enabling Virtual SAN. vCenter Server should also be upgraded to 5.5 Update 1 or later.
Hardware Compatibility for ESXi

To view a list of processors, storage devices, SAN arrays, and I/O devices that are compatible with vSphere 5.5 Update 3, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Device Compatibility for ESXi

To determine which devices are compatible with ESXi 5.5 Update 3a, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Some devices are deprecated and no longer supported on ESXi 5.5 and later. During the upgrade process, the device driver is installed on the ESXi 5.5.x host. It might still function on ESXi 5.5.x, but the device is not supported on ESXi 5.5.x. For a list of devices that have been deprecated and are no longer supported on ESXi 5.5.x, see the VMware Knowledge Base article Deprecated devices and warnings during ESXi 5.5 upgrade process.
Guest Operating System Compatibility for ESXi

To determine which guest operating systems are compatible with vSphere 5.5 Update 3a, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Virtual Machine Compatibility for ESXi

Virtual machines that are compatible with ESX 3.x and later (hardware version 4) are supported with ESXi 5.5 Update 3. Virtual machines that are compatible with ESX 2.x and later (hardware version 3) are not supported. To use such virtual machines on ESXi 5.5 Update 3, upgrade the virtual machine compatibility. See thevSphere Upgrade documentation.
vSphere Client Connections to Linked Mode Environments with vCenter Server 5.x

vCenter Server 5.5 can exist in Linked Mode only with other instances of vCenter Server 5.5.
Installation Notes for This Release

Read the vSphere Installation and Setup documentation for guidance about installing and configuring ESXi and vCenter Server.
Although the installations are straightforward, several subsequent configuration steps are essential. Read the following documentation:
Licensing in the vCenter Server and Host Management documentation
Networking in the vSphere Networking documentation
Security in the vSphere Security documentation for information on firewall ports
Migrating Third-Party Solutions

You cannot directly migrate third-party solutions installed on an ESX or ESXi host as part of a host upgrade. Architectural changes between ESXi 5.1 and ESXi 5.5 result in the loss of third-party components and possible system instability. To accomplish such migrations, you can create a custom ISO file with Image Builder. For information about upgrading your host with third-party customizations, see the vSphere Upgrade documentation. For information about using Image Builder to make a custom ISO, see the vSphere Installation and Setup documentation.
Upgrades and Installations Disallowed for Unsupported CPUs

vSphere 5.5.x supports only CPUs with LAHF and SAHF CPU instruction sets. During an installation or upgrade, the installer checks the compatibility of the host CPU with vSphere 5.5.x. If your host hardware is not compatible, a purple screen appears with a message about incompatibility. You cannot install or upgrade to vSphere 5.5.x.
Upgrades for This Release

For instructions about upgrading vCenter Server and ESX/ESXi hosts, see the vSphere Upgrade documentation.
Supported Upgrade Paths for Upgrade to ESXi 5.5 Update 3a:
Upgrade Deliverables
Supported Upgrade Tools
Supported Upgrade Paths to ESXi 5.5 Update 3a
ESX/ESXi 4.0:
Includes
ESX/ESXi 4.0 Update 1
ESX/ESXi 4.0 Update 2
ESX/ESXi 4.0 Update 3
ESX/ESXi 4.0 Update 4
ESX/ESXi 4.1:
Includes
ESX/ESXi 4.1 Update 1
ESX/ESXi 4.1 Update 2
ESX/ESXi 4.1 Update 3
ESXi 5.0:
Includes
ESXi 5.0 Update 1 
ESXi 5.0 Update 2 
ESXi 5.0 Update 3 
ESXi 5.1
Includes
ESXi 5.1 Update 1
ESXi 5.1 Update 2 
ESXi 5.5
Includes
ESXi 5.5 Update 1
ESXi 5.5 Update 2

VMware-VMvisor-Installer-5.5.0.update03-3116895.x86_64.iso

VMware vCenter Update Manager
CD Upgrade
Scripted Upgrade
Yes
Yes
Yes
Yes
Yes
ESXi550-201510001.zip    
VMware vCenter Update Manager
ESXCLI
VMware vSphere CLI
No
No
Yes*
Yes*
Yes
Using patch definitions downloaded from VMware portal (online)    VMware vCenter Update Manager with patch baseline    
No
No
No
No
Yes

*Note: Upgrade from ESXi 5.0.x, or ESXi 5.1.x, to ESXi 5.5 Update 3a using -    ESXi550-201510001.zip is supported only with ESXCLI. You need to run the esxcli software profile update --depot=<depot_location> --profile=<profile_name> command to perform the upgrade. For more information, see the ESXi 5.5.x Upgrade Options topic in the vSphere Upgrade guide.
Open Source Components for VMware vSphere 5.5 Update 3

The copyright statements and licenses applicable to the open source software components distributed in vSphere 5.5 Update 3 are available athttp://www.vmware.com/download/vsphere/open_source.html, on the Open Source tab. You can also download the source files for any GPL, LGPL, or other similar licenses that require the source code or modifications to source code to be made available for the most recent available release of vSphere.
Product Support Notices

vSphere Web Client. Because Linux platforms are no longer supported by Adobe Flash, vSphere Web Client is not supported on the Linux OS. Third party browsers that add support for Adobe Flash on the Linux desktop OS might continue to function.
VMware vCenter Server Appliance. In vSphere 5.5, the VMware vCenter Server Appliance meets high-governance compliance standards through the enforcement of the DISA Security Technical Information Guidelines (STIG). Before you deploy VMware vCenter Server Appliance, see the VMware Hardened Virtual Appliance Operations Guide for information about the new security deployment standards and to ensure successful operations.
vCenter Server database. vSphere 5.5 removes support for IBM DB2 as the vCenter Server database.
VMware Tools. Beginning with vSphere 5.5, all information about how to install and configure VMware Tools in vSphere is merged with the other vSphere documentation. For information about using VMware Tools in vSphere, see the vSphere documentation. Installing and Configuring VMware Tools is not relevant to vSphere 5.5 and later.
VMware Tools. Beginning with vSphere 5.5, VMware Tools do not provide ThinPrint features.
vSphere Data Protection. vSphere Data Protection 5.1 is not compatible with vSphere 5.5 because of a change in the way vSphere Web Client operates. vSphere Data Protection 5.1 users who upgrade to vSphere 5.5 must also update vSphere Data Protection to continue using vSphere Data Protection.
Patches Contained in this Release

This release contains all bulletins for ESXi that were released earlier to the release date of this product. See the VMware Download Patches page for more information about the individual bulletins.
Patch Release ESXi550-Update03 contains the following individual bulletins:
ESXi550-201510401-BG: Updates ESXi 5.5 esx-base vib
Patch Release ESXi550-Update03 contains the following image profiles:
ESXi-5.5.0-20151004001-standard
ESXi-5.5.0-20151004001-no-tools
For information on patch and update classification, see KB 2014447.
Resolved Issues

This section describes resolved issues in this release:
Backup
CIM and API Issues
Miscellaneous Issues
Networking Issues
Security Issues
Server Configuration Issues
Supported Hardware Issues
Storage Issues
Upgrade and Installation Issues
Virtual SAN Issues
vCenter Server and vSphere Web Client Issues
Virtual Machine Management Issues
VMware HA and Fault Tolerance
vMotion and Storage vMotion Issues
VMware Tools Issues
Backup Issues

 Attempts to restore a virtual machine might fail with an error
Attempts to restore a virtual machine on an ESXi host using vSphere Data Protection might fail and display an error message similar to the following:

Unexpected exception received during reconfigure

This issue is resolved in this release.
CIM and API Issues

 Spew in syslog when system event log is full and indication subscriptions exist
Spew in syslog when System Event Log (SEL) is full and indication subscriptions exist. The following logs are logged in rapidly: 

sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default
sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default
sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default

This issue is resolved in this release.
 CIM indications might fail when you use Auto Deploy to reboot the ESXi hosts
If the sfcbd service stops running, the CIM indications in host profile cannot be applied successfully.

This issue is resolved in this release by ensuring that the CIM indications do not rely on the status of the sfcbd service while applying the host profile.
 Status of some disks might be displayed as UNCONFIGURED GOOD instead of ONLINE
Status of some disks on an ESXi 5.5 host might be displayed as UNCONFIGURED GOOD instead of ONLINE. This issue occurs for LSI controller using the LSI CIM provider.

This issue is resolved in this release.
 Load kernel module might fail through CIM interface
The LoadModule command might fail when using the CIM interface client to load the kernel module. An error message similar to the following is displayed:

Access denied by VMkernel access control policy.

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to an openwsmand error. An error message similar to the following might be reported in thesyslog.log file:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release.
 Querying hardware status on the vSphere Client might fail with an error
Attempts to query the hardware status on the vSphere Client might fail. An error message similar to the following is displayed in the /var/log/syslog.log file in the ESXi host:

TIMEOUT DOING SHARED SOCKET RECV RESULT (1138472) Timeout (or other socket error) waiting for response from provider Header Id (16040) Request to provider 111 in process 4 failed. Error:Timeout (or other socket error) waiting for response from provider Dropped response operation details -- nameSpace: root/cimv2, className: OMC_RawIpmiSensor, Type: 0

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to an openwsmand error. An error message similar to the following might be reported in thesyslog.log file:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release.
 The sfcbd service might stop responding with an error message
The sfcbd service might stop responding and you might find the following error message in the syslog file:
 Commands Clients Documentation Community Download Support License
Redis cluster tutorial
This document is a gentle introduction to Redis Cluster, that does not use complex to understand distributed systems concepts. It provides instructions about how to setup a cluster, test, and operate it, without going into the details that are covered in the Redis Cluster specification but just describing how the system behaves from the point of view of the user.
However this tutorial tries to provide information about the availability and consistency characteristics of Redis Cluster from the point of view of the final user, stated in a simple to understand way.
Note this tutorial requires Redis version 3.0 or higher.
If you plan to run a serious Redis Cluster deployment, the more formal specification is a suggested reading, even if not strictly required. However it is a good idea to start from this document, play with Redis Cluster some time, and only later read the specification.
Redis Cluster 101
Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes.
Redis Cluster also provides some degree of availability during partitions, that is in practical terms the ability to continue the operations when some nodes fail or are not able to communicate. However the cluster stops to operate in the event of larger failures (for example when the majority of masters are unavailable).
So in practical terms, what you get with Redis Cluster?
    - The ability to automatically split your dataset among multiple nodes.
    - The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.
Redis Cluster TCP ports
Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve clients, for example 6379, plus the port obtained by adding 10000 to the data port, so 16379 in the example.
This second high port is used for the Cluster bus, that is a node-to-node communication channel using a binary protocol. The Cluster bus is used by nodes for failure detection, configuration update, failover authorization and so forth. Clients should never try to communicate with the cluster bus port, but always with the normal Redis command port, however make sure you open both ports in your firewall, otherwise Redis cluster nodes will be not able to communicate.
The command port and cluster bus port offset is fixed and is always 10000.
Note that for a Redis Cluster to work properly you need, for each node:
    1. The normal client communication port (usually 6379) used to communicate with clients to be open to all the clients that need to reach the cluster, plus all the other cluster nodes (that use the client port for keys migrations).
    2. The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes.
If you don't open both TCP ports, your cluster will not work as expected.
The cluster bus uses a different, binary protocol, for node to node data exchange, which is more suited to exchange information between nodes using little bandwidth and processing time.
Redis Cluster and Docker
Currently Redis Cluster does not support NATted environments and in general environments where IP addresses or TCP ports are remapped.
Docker uses a technique called port mapping: programs running inside Docker containers may be exposed with a different port compared to the one the program believes to be using. This is useful in order to run multiple containers using the same ports, at the same time, in the same server.
In order to make Docker compatible with Redis Cluster you need to use the host networking mode of Docker. Please check the --net=host option in the Docker documentation for more information.
Redis Cluster data sharding
Redis Cluster does not use consistent hashing, but a different form of sharding where every key is conceptually part of what we call an hash slot.
There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.
Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where:
    - Node A contains hash slots from 0 to 5500.
    - Node B contains hash slots from 5501 to 11000.
    - Node C contains hash slots from 11001 to 16383.
This allows to add and remove nodes in the cluster easily. For example if I want to add a new node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove node A from the cluster I can just move the hash slots served by A to B and C. When the node A will be empty I can remove it from the cluster completely.
Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime.
Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot. The user can force multiple keys to be part of the same hash slot by using a concept called hash tags.
Hash tags are documented in the Redis Cluster specification, but the gist is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments.
Redis Cluster master-slave model
In order to remain available when a subset of master nodes are failing or are not able to communicate with the majority of nodes, Redis Cluster uses a master-slave model where every hash slot has from 1 (the master itself) to N replicas (N-1 additional slaves nodes).
In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000.
However when the cluster is created (or at a latter time) we add a slave node to every master, so that the final cluster is composed of A, B, C that are masters nodes, and A1, B1, C1 that are slaves nodes, the system is able to continue if node B fails.
Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly.
However note that if nodes B and B1 fail at the same time Redis Cluster is not able to continue to operate.
Redis Cluster consistency guarantees
Redis Cluster is not able to guarantee strong consistency. In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.
The first reason why Redis Cluster can lose writes is because it uses asynchronous replication. This means that during writes the following happens:
    - Your client writes to the master B.
    - The master B replies OK to your client.
    - The master B propagates the write to its slaves B1, B2 and B3.
As you can see B does not wait for an acknowledge from B1, B2, B3 before replying to the client, since this would be a prohibitive latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever.
This is very similar to what happens with most databases that are configured to flush data to disk every second, so it is a scenario you are already able to reason about because of past experiences with traditional database systems not involving distributed systems. Similarly you can improve consistency by forcing the database to flush data on disk before replying to the client, but this usually results into prohibitively low performance. That would be the equivalent of synchronous replication in the case of Redis Cluster.
Basically there is a trade-off to take between performance and consistency.
Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command, this makes losing writes a lot less likely, however note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible under more complex failure scenarios that a slave that was not able to receive the write is elected as master.
There is another notable scenario where Redis Cluster will lose writes, that happens during a network partition where a client is isolated with a minority of instances including at least a master.
Take as an example our 6 nodes cluster composed of A, B, C, A1, B1, C1, with 3 masters and 3 slaves. There is also a client, that we will call Z1.
After a partition occurs, it is possible that in one side of the partition we have A, C, A1, B1, C1, and in the other side we have B and Z1.
Z1 is still able to write to B, that will accept its writes. If the partition heals in a very short time, the cluster will continue normally. However if the partition lasts enough time for B1 to be promoted to master in the majority side of the partition, the writes that Z1 is sending to B will be lost.
Note that there is a maximum window to the amount of writes Z1 will be able to send to B: if enough time has elapsed for the majority side of the partition to elect a slave as master, every master node in the minority side stops accepting writes.
This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout.
After node timeout has elapsed, a master node is considered to be failing, and can be replaced by one of its replicas. Similarly after node timeout has elapsed without a master node to be able to sense the majority of the other master nodes, it enters an error state and stops accepting writes.
Redis Cluster configuration parameters
We are about to create an example cluster deployment. Before to continue let's introduce the configuration parameters that Redis Cluster introduces in the redis.conf file. Some will be obvious, others will be more clear as you continue reading.
    - cluster-enabled <yes/no>: If yes enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usually.
    - cluster-config-file <filename>: Note that despite the name of this option, this is not an user editable configuration file, but the file where a Redis Cluster node automatically persists the cluster configuration (the state, basically) every time there is a change, in order to be able to re-read it at startup. The file lists things like the other nodes in the cluster, their state, persistent variables, and so forth. Often this file is rewritten and flushed on disk as a result of some message reception.
    - cluster-node-timeout <milliseconds>: The maximum amount of time a Redis Cluster node can be unavailable, without it being considered as failing. If a master node is not reachable for more than the specified amount of time, it will be failed over by its slaves. This parameter controls other important things in Redis Cluster. Notably, every node that can't reach the majority of master nodes for the specified amount of time, will stop accepting queries.
    - cluster-slave-validity-factor <factor>: If set to zero, a slave will always try to failover a master, regardless of the amount of time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as the node timeout value multiplied by the factor provided with this option, and if the node is a slave, it will not try to start a failover if the master link was disconnected for more than the specified amount of time. For example if the node timeout is set to 5 seconds, and the validity factor is set to 10, a slave disconnected from the master for more than 50 seconds will not try to failover its master. Note that any value different than zero may result in Redis Cluster to be unavailable after a master failure if there is no slave able to failover it. In that case the cluster will return back available only when the original master rejoins the cluster.
    - cluster-migration-barrier <count>: Minimum number of slaves a master will remain connected with, for another slave to migrate to a master which is no longer covered by any slave. See the appropriate section about replica migration in this tutorial for more information.
    - cluster-require-full-coverage <yes/no>: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node. If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.
Creating and using a Redis Cluster
Note: to deploy a Redis Cluster manually is very important to learn certain operation aspects of it. However if you want to get a cluster up and running ASAP skip this section and the next one and go directly to Creating a Redis Cluster using the create-cluster script.
To create a cluster, the first thing we need is to have a few empty Redis instances running in cluster mode. This basically means that clusters are not created using normal Redis instances, but a special mode needs to be configured so that the Redis instance will enable the Cluster specific features and commands.
The following is a minimal Redis cluster configuration file:
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
As you can see what enables the cluster mode is simply the cluster-enabled directive. Every instance also contains the path of a file where the configuration for this node is stored, that by default is nodes.conf. This file is never touched by humans, it is simply generated at startup by the Redis Cluster instances, and updated every time it is needed.
Note that the minimal cluster that works as expected requires to contain at least three master nodes. For your first tests it is strongly suggested to start a six nodes cluster with three masters and three slaves.
To do so, enter a new directory, and create the following directories named after the port number of the instance we'll run inside any given directory.
Something like:
mkdir cluster-test
cd cluster-test
mkdir 7000 7001 7002 7003 7004 7005
Create a redis.conf file inside each of the directories, from 7000 to 7005. As a template for your configuration file just use the small example above, but make sure to replace the port number 7000 with the right port number according to the directory name.
Now copy your redis-server executable, compiled from the latest sources in the unstable branch at GitHub, into the cluster-test directory, and finally open 6 terminal tabs in your favorite terminal application.
Start every instance like that, one every tab:
cd 7000
../redis-server ./redis.conf
As you can see from the logs of every instance, since no nodes.conf file existed, every node assigns itself a new ID.
[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1
This ID will be used forever by this specific instance in order for the instance to have a unique name in the context of the cluster. Every node remembers every other node using this IDs, and not by IP or port. IP addresses and ports may change, but the unique node identifier will never change for all the life of the node. We call this identifier simply Node ID.
Creating the cluster
Now that we have a number of instances running, we need to create our cluster by writing some meaningful configuration to the nodes.
This is very easy to accomplish as we are helped by the Redis Cluster command line utility called redis-trib, a Ruby program executing special commands on instances in order to create new clusters, check or reshard an existing cluster, and so forth.
The redis-trib utility is in the src directory of the Redis source code distribution. You need to install redis gem to be able to run redis-trib.
gem install redis
To create your cluster simply type:
./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \
127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005
The command used here is create, since we want to create a new cluster. The option --replicas 1 means that we want a slave for every master created. The other arguments are the list of addresses of the instances I want to use to create the new cluster.
Obviously the only setup with our requirements is to create a cluster with 3 masters and 3 slaves.
Redis-trib will propose you a configuration. Accept typing yes. The cluster will be configured and joined, that means, instances will be bootstrapped into talking with each other. Finally if everything went ok you'll see a message like that:
[OK] All 16384 slots covered
This means that there is at least a master instance serving each of the 16384 slots available.
Creating a Redis Cluster using the create-cluster script
If you don't want to create a Redis Cluster by configuring and executing individual instances manually as explained above, there is a much simpler system (but you'll not learn the same amount of operational details).
Just check utils/create-cluster directory in the Redis distribution. There is a script called create-cluster inside (same name as the directory it is contained into), it's a simple bash script. In order to start a 6 nodes cluster with 3 masters and 3 slaves just type the following commands:
    1. create-cluster start
    2. create-cluster create
Reply to yes in step 2 when the redis-trib utility wants you to accept the cluster layout.
You can now interact with the cluster, the first node will start at port 30001 by default. When you are done, stop the cluster with:
    1. create-cluster stop.
Please read the README inside this directory for more information on how to run the script.
Playing with the cluster
At this stage one of the problems with Redis Cluster is the lack of client libraries implementations.
I'm aware of the following implementations:
    - redis-rb-cluster is a Ruby implementation written by me (@antirez) as a reference for other languages. It is a simple wrapper around the original redis-rb, implementing the minimal semantics to talk with the cluster efficiently.
    - redis-py-cluster A port of redis-rb-cluster to Python. Supports majority of redis-py functionality. Is in active development.
    - The popular Predis has support for Redis Cluster, the support was recently updated and is in active development.
    - The most used Java client, Jedis recently added support for Redis Cluster, see the Jedis Cluster section in the project README.
    - StackExchange.Redis offers support for C# (and should work fine with most .NET languages; VB, F#, etc)
    - thunk-redis offers support for Node.js and io.js, it is a thunk/promise-based redis client with pipelining and cluster.
    - redis-go-cluster is an implementation of Redis Cluster for the Go language using the Redigo library client as the base client. Implements MGET/MSET via result aggregation.
    - The redis-cli utility in the unstable branch of the Redis repository at GitHub implements a very basic cluster support when started with the -c switch.
An easy way to test Redis Cluster is either to try any of the above clients or simply the redis-cli command line utility. The following is an example of interaction using the latter:
$ redis-cli -c -p 7000
redis 127.0.0.1:7000> set foo bar
-> Redirected to slot [12182] located at 127.0.0.1:7002
OK
redis 127.0.0.1:7002> set hello world
-> Redirected to slot [866] located at 127.0.0.1:7000
OK
redis 127.0.0.1:7000> get foo
-> Redirected to slot [12182] located at 127.0.0.1:7002
"bar"
redis 127.0.0.1:7000> get hello
-> Redirected to slot [866] located at 127.0.0.1:7000
"world"
Note: if you created the cluster using the script your nodes may listen to different ports, starting from 30001 by default.
The redis-cli cluster support is very basic so it always uses the fact that Redis Cluster nodes are able to redirect a client to the right node. A serious client is able to do better than that, and cache the map between hash slots and nodes addresses, to directly use the right connection to the right node. The map is refreshed only when something changed in the cluster configuration, for example after a failover or after the system administrator changed the cluster layout by adding or removing nodes.
Writing an example app with redis-rb-cluster
Before going forward showing how to operate the Redis Cluster, doing things like a failover, or a resharding, we need to create some example application or at least to be able to understand the semantics of a simple Redis Cluster client interaction.
In this way we can run an example and at the same time try to make nodes failing, or start a resharding, to see how Redis Cluster behaves under real world conditions. It is not very helpful to see what happens while nobody is writing to the cluster.
This section explains some basic usage of redis-rb-cluster showing two examples. The first is the following, and is theexample.rb file inside the redis-rb-cluster distribution:
   1  require './cluster'
   2
   3  if ARGV.length != 2
   4      startup_nodes = [
   5          {:host => "127.0.0.1", :port => 7000},
   6          {:host => "127.0.0.1", :port => 7001}
   7      ]
   8  else
   9      startup_nodes = [
  10          {:host => ARGV[0], :port => ARGV[1].to_i}
  11      ]
  12  end
  13
  14  rc = RedisCluster.new(startup_nodes,32,:timeout => 0.1)
  15
  16  last = false
  17
  18  while not last
  19      begin
  20          last = rc.get("__last__")
  21          last = 0 if !last
  22      rescue => e
  23          puts "error #{e.to_s}"
  24          sleep 1
  25      end
  26  end
  27
  28  ((last.to_i+1)..1000000000).each{|x|
  29      begin
  30          rc.set("foo#{x}",x)
  31          puts rc.get("foo#{x}")
  32          rc.set("__last__",x)
  33      rescue => e
  34          puts "error #{e.to_s}"
  35      end
  36      sleep 0.1
  37  }
The application does a very simple thing, it sets keys in the form foo<number> to number, one after the other. So if you run the program the result is the following stream of commands:
    - SET foo0 0
    - SET foo1 1
    - SET foo2 2
    - And so forth...
The program looks more complex than it should usually as it is designed to show errors on the screen instead of exiting with an exception, so every operation performed with the cluster is wrapped by begin rescue blocks.
The line 14 is the first interesting line in the program. It creates the Redis Cluster object, using as argument a list ofstartup nodes, the maximum number of connections this object is allowed to take against different nodes, and finally the timeout after a given operation is considered to be failed.
The startup nodes don't need to be all the nodes of the cluster. The important thing is that at least one node is reachable. Also note that redis-rb-cluster updates this list of startup nodes as soon as it is able to connect with the first node. You should expect such a behavior with any other serious client.
Now that we have the Redis Cluster object instance stored in the rc variable we are ready to use the object like if it was a normal Redis object instance.
This is exactly what happens in line 18 to 26: when we restart the example we don't want to start again with foo0, so we store the counter inside Redis itself. The code above is designed to read this counter, or if the counter does not exist, to assign it the value of zero.
However note how it is a while loop, as we want to try again and again even if the cluster is down and is returning errors. Normal applications don't need to be so careful.
Lines between 28 and 37 start the main loop where the keys are set or an error is displayed.
Note the sleep call at the end of the loop. In your tests you can remove the sleep if you want to write to the cluster as fast as possible (relatively to the fact that this is a busy loop without real parallelism of course, so you'll get the usually 10k ops/second in the best of the conditions).
Normally writes are slowed down in order for the example application to be easier to follow by humans.
Starting the application produces the following output:
ruby ./example.rb
1
2
3
4
5
6
7
8
9
^C (I stopped the program here)
This is not a very interesting program and we'll use a better one in a moment but we can already see what happens during a resharding when the program is running.
Resharding the cluster
Now we are ready to try a cluster resharding. To do this please keep the example.rb program running, so that you can see if there is some impact on the program running. Also you may want to comment the sleep call in order to have some more serious write load during resharding.
Resharding basically means to move hash slots from a set of nodes to another set of nodes, and like cluster creation it is accomplished using the redis-trib utility.
To start a resharding just type:
./redis-trib.rb reshard 127.0.0.1:7000
You only need to specify a single node, redis-trib will find the other nodes automatically.
Currently redis-trib is only able to reshard with the administrator support, you can't just say move 5% of slots from this node to the other one (but this is pretty trivial to implement). So it starts with questions. The first is how much a big resharding do you want to do:
How many slots do you want to move (from 1 to 16384)?
We can try to reshard 1000 hash slots, that should already contain a non trivial amount of keys if the example is still running without the sleep call.
Then redis-trib needs to know what is the target of the resharding, that is, the node that will receive the hash slots. I'll use the first master node, that is, 127.0.0.1:7000, but I need to specify the Node ID of the instance. This was already printed in a list by redis-trib, but I can always find the ID of a node with the following command if I need:
$ redis-cli -p 7000 cluster nodes | grep myself
97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5460
Ok so my target node is 97a3a64667477371c4479320d683e4c8db5858b1.
Now you'll get asked from what nodes you want to take those keys. I'll just type all in order to take a bit of hash slots from all the other master nodes.
After the final confirmation you'll see a message for every slot that redis-trib is going to move from a node to another, and a dot will be printed for every actual key moved from one side to the other.
While the resharding is in progress you should be able to see your example program running unaffected. You can stop and restart it multiple times during the resharding if you want.
At the end of the resharding, you can test the health of the cluster with the following command:
./redis-trib.rb check 127.0.0.1:7000
All the slots will be covered as usually, but this time the master at 127.0.0.1:7000 will have more hash slots, something around 6461.
Scripting a resharding operation
Reshardings can be performed automatically without the need to manually enter the parameters in an interactive way. This is possible using a command line like the following:
./redis-trib.rb reshard --from <node-id> --to <node-id> --slots <number of slots> --yes <host>:<port>
This allows to build some automatism if you are likely to reshard often, however currently there is no way for redis-trib to automatically rebalance the cluster checking the distribution of keys across the cluster nodes and intelligently moving slots as needed. This feature will be added in the future.
A more interesting example application
The example application we wrote early is not very good. It writes to the cluster in a simple way without even checking if what was written is the right thing.
From our point of view the cluster receiving the writes could just always write the key foo to 42 to every operation, and we would not notice at all.
So in the redis-rb-cluster repository, there is a more interesting application that is called consistency-test.rb. It uses a set of counters, by default 1000, and sends INCR commands in order to increment the counters.
However instead of just writing, the application does two additional things:
    - When a counter is updated using INCR, the application remembers the write.
    - It also reads a random counter before every write, and check if the value is what we expected it to be, comparing it with the value it has in memory.
What this means is that this application is a simple consistency checker, and is able to tell you if the cluster lost some write, or if it accepted a write that we did not receive acknowledgment for. In the first case we'll see a counter having a value that is smaller than the one we remember, while in the second case the value will be greater.
Running the consistency-test application produces a line of output every second:
$ ruby consistency-test.rb
925 R (0 err) | 925 W (0 err) |
5030 R (0 err) | 5030 W (0 err) |
9261 R (0 err) | 9261 W (0 err) |
13517 R (0 err) | 13517 W (0 err) |
17780 R (0 err) | 17780 W (0 err) |
22025 R (0 err) | 22025 W (0 err) |
25818 R (0 err) | 25818 W (0 err) |
The line shows the number of Reads and Writes performed, and the number of errors (query not accepted because of errors since the system was not available).
If some inconsistency is found, new lines are added to the output. This is what happens, for example, if I reset a counter manually while the program is running:
$ redis-cli -h 127.0.0.1 -p 7000 set key_217 0
OK

(in the other tab I see...)

94774 R (0 err) | 94774 W (0 err) |
98821 R (0 err) | 98821 W (0 err) |
102886 R (0 err) | 102886 W (0 err) | 114 lost |
107046 R (0 err) | 107046 W (0 err) | 114 lost |
When I set the counter to 0 the real value was 114, so the program reports 114 lost writes (INCR commands that are not remembered by the cluster).
This program is much more interesting as a test case, so we'll use it to test the Redis Cluster failover.
Testing the failover
Note: during this test, you should take a tab open with the consistency test application running.
In order to trigger the failover, the simplest thing we can do (that is also the semantically simplest failure that can occur in a distributed system) is to crash a single process, in our case a single master.
We can identify a cluster and crash it with the following command:
$ redis-cli -p 7000 cluster nodes | grep master
3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385482984082 0 connected 5960-10921
2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 master - 0 1385482983582 0 connected 11423-16383
97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422
Ok, so 7000, 7001, and 7002 are masters. Let's crash node 7002 with the DEBUG SEGFAULT command:
$ redis-cli -p 7002 debug segfault
Error: Server closed the connection
Now we can look at the output of the consistency test to see what it reported.
18849 R (0 err) | 18849 W (0 err) |
23151 R (0 err) | 23151 W (0 err) |
27302 R (0 err) | 27302 W (0 err) |

... many error warnings here ...

29659 R (578 err) | 29660 W (577 err) |
33749 R (578 err) | 33750 W (577 err) |
37918 R (578 err) | 37919 W (577 err) |
42077 R (578 err) | 42078 W (577 err) |
As you can see during the failover the system was not able to accept 578 reads and 577 writes, however no inconsistency was created in the database. This may sound unexpected as in the first part of this tutorial we stated that Redis Cluster can lose writes during the failover because it uses asynchronous replication. What we did not say is that this is not very likely to happen because Redis sends the reply to the client, and the commands to replicate to the slaves, about at the same time, so there is a very small window to lose data. However the fact that it is hard to trigger does not mean that it is impossible, so this does not change the consistency guarantees provided by Redis cluster.
We can now check what is the cluster setup after the failover (note that in the meantime I restarted the crashed instance so that it rejoins the cluster as a slave):
$ redis-cli -p 7000 cluster nodes
3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385503418521 0 connected
a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385503419023 0 connected
97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422
3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385503419023 3 connected 11423-16383
3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385503417005 0 connected 5960-10921
2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385503418016 3 connected
Now the masters are running on ports 7000, 7001 and 7005. What was previously a master, that is the Redis instance running on port 7002, is now a slave of 7005.
The output of the CLUSTER NODES command may look intimidating, but it is actually pretty simple, and is composed of the following tokens:
    - Node ID
    - ip:port
    - flags: master, slave, myself, fail, ...
    - if it is a slave, the Node ID of the master
    - Time of the last pending PING still waiting for a reply.
    - Time of the last PONG received.
    - Configuration epoch for this node (see the Cluster specification).
    - Status of the link to this node.
    - Slots served...
Manual failover
Sometimes it is useful to force a failover without actually causing any problem on a master. For example in order to upgrade the Redis process of one of the master nodes it is a good idea to failover it in order to turn it into a slave with minimal impact on availability.
Manual failovers are supported by Redis Cluster using the CLUSTER FAILOVER command, that must be executed in one of the slaves of the master you want to failover.
Manual failovers are special and are safer compared to failovers resulting from actual master failures, since they occur in a way that avoid data loss in the process, by switching clients from the original master to the new master only when the system is sure that the new master processed all the replication stream from the old one.
This is what you see in the slave log when you perform a manual failover:
# Manual failover user request accepted.
# Received replication offset for paused master manual failover: 347540
# All master replication stream processed, manual failover can start.
# Start of election delayed for 0 milliseconds (rank #0, offset 347540).
# Starting a failover election for epoch 7545.
# Failover election won: I'm the new master.
Basically clients connected to the master we are failing over are stopped. At the same time the master sends its replication offset to the slave, that waits to reach the offset on its side. When the replication offset is reached, the failover starts, and the old master is informed about the configuration switch. When the clients are unblocked on the old master, they are redirected to the new master.
Adding a new node
Adding a new node is basically the process of adding an empty node and then moving some data into it, in case it is a new master, or telling it to setup as a replica of a known node, in case it is a slave.
We'll show both, starting with the addition of a new master instance.
In both cases the first step to perform is adding an empty node.
This is as simple as to start a new node in port 7006 (we already used from 7000 to 7005 for our existing 6 nodes) with the same configuration used for the other nodes, except for the port number, so what you should do in order to conform with the setup we used for the previous nodes:
    - Create a new tab in your terminal application.
    - Enter the cluster-test directory.
    - Create a directory named 7006.
    - Create a redis.conf file inside, similar to the one used for the other nodes but using 7006 as port number.
    - Finally start the server with ../redis-server ./redis.conf
At this point the server should be running.
Now we can use redis-trib as usually in order to add the node to the existing cluster.
./redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000
As you can see I used the add-node command specifying the address of the new node as first argument, and the address of a random existing node in the cluster as second argument.
In practical terms redis-trib here did very little to help us, it just sent a CLUSTER MEET message to the node, something that is also possible to accomplish manually. However redis-trib also checks the state of the cluster before to operate, so it is a good idea to perform cluster operations always via redis-trib even when you know how the internals work.
Now we can connect to the new node to see if it really joined the cluster:
redis 127.0.0.1:7006> cluster nodes
3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385543178575 0 connected 5960-10921
3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385543179583 0 connected
f093c80dde814da99c5cf72a7dd01590792b783b :0 myself,master - 0 0 0 connected
2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543178072 3 connected
a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385543178575 0 connected
97a3a64667477371c4479320d683e4c8db5858b1 127.0.0.1:7000 master - 0 1385543179080 0 connected 0-5959 10922-11422
3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385543177568 3 connected 11423-16383
Note that since this node is already connected to the cluster it is already able to redirect client queries correctly and is generally speaking part of the cluster. However it has two peculiarities compared to the other masters:
    - It holds no data as it has no assigned hash slots.
    - Because it is a master without assigned slots, it does not participate in the election process when a slave wants to become a master.
Now it is possible to assign hash slots to this node using the resharding feature of redis-trib. It is basically useless to show this as we already did in a previous section, there is no difference, it is just a resharding having as a target the empty node.
Adding a new node as a replica
Adding a new Replica can be performed in two ways. The obvious one is to use redis-trib again, but with the --slave option, like this:
./redis-trib.rb add-node --slave 127.0.0.1:7006 127.0.0.1:7000
Note that the command line here is exactly like the one we used to add a new master, so we are not specifying to which master we want to add the replica. In this case what happens is that redis-trib will add the new node as replica of a random master among the masters with less replicas.
However you can specify exactly what master you want to target with your new replica with the following command line:
./redis-trib.rb add-node --slave --master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7006 127.0.0.1:7000
This way we assign the new replica to a specific master.
A more manual way to add a replica to a specific master is to add the new node as an empty master, and then turn it into a replica using the CLUSTER REPLICATE command. This also works if the node was added as a slave but you want to move it as a replica of a different master.
For example in order to add a replica for the node 127.0.0.1:7005 that is currently serving hash slots in the range 11423-16383, that has a Node ID 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e, all I need to do is to connect with the new node (already added as empty master) and send the command:
redis 127.0.0.1:7006> cluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e
That's it. Now we have a new replica for this set of hash slots, and all the other nodes in the cluster already know (after a few seconds needed to update their config). We can verify with the following command:
$ redis-cli -p 7000 cluster nodes | grep slave | grep 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e
f093c80dde814da99c5cf72a7dd01590792b783b 127.0.0.1:7006 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617702 3 connected
2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617198 3 connected
The node 3c3a0c... now has two slaves, running on ports 7002 (the existing one) and 7006 (the new one).
Removing a node
To remove a slave node just use the del-node command of redis-trib:
./redis-trib del-node 127.0.0.1:7000 `<node-id>`
The first argument is just a random node in the cluster, the second argument is the ID of the node you want to remove.
You can remove a master node in the same way as well, however in order to remove a master node it must be empty. If the master is not empty you need to reshard data away from it to all the other master nodes before.
An alternative to remove a master node is to perform a manual failover of it over one of its slaves and remove the node after it turned into a slave of the new master. Obviously this does not help when you want to reduce the actual number of masters in your cluster, in that case, a resharding is needed.
Replicas migration
In Redis Cluster it is possible to reconfigure a slave to replicate with a different master at any time just using the following command:
CLUSTER REPLICATE <master-node-id>
However there is a special scenario where you want replicas to move from one master to another one automatically, without the help of the system administrator. The automatic reconfiguration of replicas is called replicas migrationand is able to improve the reliability of a Redis Cluster.
Note: you can read the details of replicas migration in the Redis Cluster Specification, here we'll only provide some information about the general idea and what you should do in order to benefit from it.
The reason why you may want to let your cluster replicas to move from one master to another under certain condition, is that usually the Redis Cluster is as resistant to failures as the number of replicas attached to a given master.
For example a cluster where every master has a single replica can't continue operations if the master and its replica fail at the same time, simply because there is no other instance to have a copy of the hash slots the master was serving. However while netsplits are likely to isolate a number of nodes at the same time, many other kind of failures, like hardware or software failures local to a single node, are a very notable class of failures that are unlikely to happen at the same time, so it is possible that in your cluster where every master has a slave, the slave is killed at 4am, and the master is killed at 6am. This still will result in a cluster that can no longer operate.
To improve reliability of the system we have the option to add additional replicas to every master, but this is expensive. Replica migration allows to add more slaves to just a few masters. So you have 10 masters with 1 slave each, for a total of 20 instances. However you add, for example, 3 instances more as slaves of some of your masters, so certain masters will have more than a single slave.
With replicas migration what happens is that if a master is left without slaves, a replica from a master that has multiple slaves will migrate to the orphaned master. So after your slave goes down at 4am as in the example we made above, another slave will take its place, and when the master will fail as well at 5am, there is still a slave that can be elected so that the cluster can continue to operate.
So what you should know about replicas migration in short?
    - The cluster will try to migrate a replica from the master that has the greatest number of replicas in a given moment.
    - To benefit from replica migration you have just to add a few more replicas to a single master in your cluster, it does not matter what master.
    - There is a configuration parameter that controls the replica migration feature that is called cluster-migration-barrier: you can read more about it in the example redis.conf file provided with Redis Cluster.
Upgrading nodes in a Redis Cluster
Upgrading slave nodes is easy since you just need to stop the node and restart it with an updated version of Redis. If there are clients scaling reads using slave nodes, they should be able to reconnect to a different slave if a given one is not available.
Upgrading masters is a bit more complex, and the suggested procedure is:
    1. Use CLUSTER FAILOVER to trigger a manual failover of the master to one of its slaves (see the "Manual failover" section of this documentation).
    2. Wait for the master to turn into a slave.
    3. Finally upgrade the node as you do for slaves.
    4. If you want the master to be the node you just upgraded, trigger a new manual failover in order to turn back the upgraded node into a master.
Following this procedure you should upgrade one node after the other until all the nodes are upgraded.
Migrating to Redis Cluster
Users willing to migrate to Redis Cluster may have just a single master, or may already using a preexisting sharding setup, where keys are split among N nodes, using some in-house algorithm or a sharding algorithm implemented by their client library or Redis proxy.
In both cases it is possible to migrate to Redis Cluster easily, however what is the most important detail is if multiple-keys operations are used by the application, and how. There are three different cases:
    1. Multiple keys operations, or transactions, or Lua scripts involving multiple keys, are not used. Keys are accessed independently (even if accessed via transactions or Lua scripts grouping multiple commands, about the same key, together).
    2. Multiple keys operations, transactions, or Lua scripts involving multiple keys are used but only with keys having the same hash tag, which means that the keys used together all have a {...} sub-string that happens to be identical. For example the following multiple keys operation is defined in the context of the same hash tag:SUNION {user:1000}.foo {user:1000}.bar.
    3. Multiple keys operations, transactions, or Lua scripts involving multiple keys are used with key names not having an explicit, or the same, hash tag.
The third case is not handled by Redis Cluster: the application requires to be modified in order to don't use multi keys operations or only use them in the context of the same hash tag.
Case 1 and 2 are covered, so we'll focus on those two cases, that are handled in the same way, so no distinction will be made in the documentation.
Assuming you have your preexisting data set split into N masters, where N=1 if you have no preexisting sharding, the following steps are needed in order to migrate your data set to Redis Cluster:
    1. Stop your clients. No automatic live-migration to Redis Cluster is currently possible. You may be able to do it orchestrating a live migration in the context of your application / environment.
    2. Generate an append only file for all of your N masters using the BGREWRITEAOF command, and waiting for the AOF file to be completely generated.
    3. Save your AOF files from aof-1 to aof-N somewhere. At this point you can stop your old instances if you wish (this is useful since in non-virtualized deployments you often need to reuse the same computers).
    4. Create a Redis Cluster composed of N masters and zero slaves. You'll add slaves later. Make sure all your nodes are using the append only file for persistence.
    5. Stop all the cluster nodes, substitute their append only file with your pre-existing append only files, aof-1 for the first node, aof-2 for the second node, up to aof-N.
    6. Restart your Redis Cluster nodes with the new AOF files. They'll complain that there are keys that should not be there according to their configuration.
    7. Use redis-trib fix command in order to fix the cluster so that keys will be migrated according to the hash slots each node is authoritative or not.
    8. Use redis-trib check at the end to make sure your cluster is ok.
    9. Restart your clients modified to use a Redis Cluster aware client library.
There is an alternative way to import data from external instances to a Redis Cluster, which is to use the redis-trib import command.
The command moves all the keys of a running instance (deleting the keys from the source instance) to the specified pre-existing Redis Cluster. However note that if you use a Redis 2.8 instance as source instance the operation may be slow since 2.8 does not implement migrate connection caching, so you may want to restart your source instance with a Redis 3.x version before to perform such operation.
This website is open source software. See all credits.
Sponsored byRedis Labs
spSendReq/spSendMsg failed to send on 7 (-1)
Error getting provider context from provider manager: 11
This document is a gentle introduction to Redis Cluster, that does not use complex to understand distributed systems concepts. It provides instructions about how to setup a cluster, test, and operate it, without going into the details that are covered in the Redis Cluster specification but just describing how the system behaves from the point of view of the user.
However this tutorial tries to provide information about the availability and consistency characteristics of Redis Cluster from the point of view of the final user, stated in a simple to understand way.
Note this tutorial requires Redis version 3.0 or higher.
If you plan to run a serious Redis Cluster deployment, the more formal specification is a suggested reading, even if not strictly required. However it is a good idea to start from this document, play with Redis Cluster some time, and only later read the specification.
Redis Cluster 101

This issue occurs when there is a contention for semaphore between the CIM server and the providers.

This issue is resolved in this release.
 False alarms appear in the Hardware Status tab of the vSphere Client
After you upgrade Integrated Lights Out (iLO) firmware on HP DL980 G7, false alarms appear in the Hardware Status tab of the vSphere Client. Error messages similar to the following might be logged in the /var/log/syslog.log file:

sfcb-vmware_raw[nnnnn]: IpmiIfruInfoAreaLength: Reading FRU for 0x0 at 0x8 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfcFruChassis: Reading FRU Chassis Info Area length for 0x0 FAILED
sfcb-vmware_raw[nnnnn]: IpmiIfcFruBoard: Reading FRU Board Info details for 0x0 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfruInfoAreaLength: Reading FRU for 0x0 at 0x70 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfcFruProduct: Reading FRU product Info Area length for 0x0 FAILED
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: data length mismatch req=19,resp=3
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0001,resp=0002
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0002,resp=0003
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0003,resp=0004
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0004,resp=0005
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0005,resp=0006
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0006,resp=0007

This issue is resolved in this release.
 ESXi might send duplicate events to management software
ESXi might send duplicate events to the management software when an Intelligent Platform Management Interface (IPMI) sensor event is triggered on the ESXi Host.

This issue is resolved in this release.
 Unable to monitor Hardware Status after removing CIM indication subscription
If the CIM client sends two requests of Delete Instance to the same CIM indication subscription, the sfcb-vmware_int might stop responding due to memory contention. You might not be able to monitor the Hardware Status with the vCenter Server and ESXi.

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail respond due to openwsmand error. An error message similar to the following might be reported:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release. 
 CIM client might display an error due to multiple enumeration
When you execute multiple enumerate queries on VMware Ethernet port class using the CBEnumInstances method, servers running on an ESXi 5.5 might notice an error message similar to the following:

CIM error: enumInstances Class not found

This issue occurs when the management software fails to retrieve information provided by VMware_EthernetPort()class. When the issue occurs, query on memstats might display the following error message:

MemStatsTraverseGroups: VSI_GetInstanceListAlloc failure: Not found.

This issue is resolve in this release.
 Unable to monitor Hardware Status on an ESXi host
An ESXi host might report an error in the Hardware Status tab due to the unresponsive hardware monitoring service (sfcbd). An error similar to the following is written to syslog.log:

sfcb-hhrc[5149608]: spGetMsg receiving from 65 5149608-11 Resource temporarily unavailable
sfcb-hhrc[5149608]: rcvMsg receiving from 65 5149608-11 Resource temporarily unavailable
sfcb-hhrc[5149608]: Timeout or other socket error
sfcb-LSIESG_SMIS13_HHR[6064161]: spGetMsg receiving from 51 6064161-11 Resource temporarily unavailable
sfcb-LSIESG_SMIS13_HHR[6064161]: rcvMsg receiving from 51 6064161-11 Resource temporarily unavailable
sfcb-LSIESG_SMIS13_HHR[6064161]: Timeout or other socket error
sfcb-kmoduleprovider[6064189]: spGetMsg receiving from 57 6064189-11 Resource temporarily unavailable
sfcb-kmoduleprovider[6064189]: rcvMsg receiving from 57 6064189-11 Resource temporarily unavailable
sfcb-kmoduleprovider[6064189]: Timeout or other socket error

The syslog below in debug level indicates that an invalid data of 0x3c is sent by IPMI when the expected data is 0x01.

sfcb-vmware_raw[35704]: IpmiIfcRhFruInv: fru.header.version: 0x3c

This issue occurs when sfcb-vmware_raw provider receives invalid data from the Intelligent Platform Management Interface (IPMI) tool while reading the Field Replaceable Unit (FRU) inventory data.

This issue is resolved in this release.
Miscellaneous Issues

 Cloning CBT-enabled virtual machine templates from ESXi hosts might fail 
Attempt to clone CBT-enabled virtual machines templates simultaneously from two different ESXi 5.5 hosts might fail. An error message similar to the following is displayed:

Failed to open VM_template.vmdk': Could not open/create change tracking file (2108).

This issue is resolved in this release.
 Unable to log into ESXi host with Active Directory credentials 
Attempts to login to an ESXi host might fail after the host successfully joins an Active Directory. This occurs when a user from one domain attempts to join another trusted domain, which is not present in the ESXi client site. An error similar to the following is written to sys.log/netlogon.log file:

netlogond[17229]: [LWNetDnsQueryWithBuffer() /build/mts/release/bora-1028347/likewise/esxi-esxi/src/linux/netlogon/utils/lwnet-dns.c:1185] DNS lookup for '_ldap._tcp.<domain details>' failed with errno 0, h_errno = 1

This issue is resolved in this release.
 Update to the cURL library
cURL fails to resolve localhost when IPv6 is disabled. An error message similar to the following is displayed:

error: enumInstances Failed initialization

This issue is resolved in this release.
Networking Issues

 ESXi hosts with the virtual machines having e1000 or e1000e vNIC driver might fail with a purple screen 
ESXi hosts with the virtual machines having e1000 or e1000e vNIC driver might fail with a purple screen when you enable TCP segmentation Offload (TSO). Error messages similar to the following might be written to the log files:

cpu7:nnnnnn)Code start: 0xnnnnnnnnnnnn VMK uptime: 9:21:12:17.991
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000TxTSOSend@vmkernel#nover+0x65b stack: 0xnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000PollTxRing@vmkernel#nover+0x18ab stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000DevAsyncTx@vmkernel#nover+0xa2 stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetWorldletPerVMCB@vmkernel#nover+0xae stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]WorldletProcessQueue@vmkernel#nover+0x488 stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]WorldletBHHandler@vmkernel#nover+0x60 stack: 0xnnnnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]BH_Check@vmkernel#nover+0x185 stack: 0xnnnnnnnnnnnn

This issue is resolved in this release.
 ESXi responds to unnecessary Internet Control Message Protocol request types
The ESXi host might respond to unnecessary Internet Control Message Protocol (ICMP) request types.

This issue is resolved in this release.
 ESXi hostd might fail when performing storage device rescan operations
When you perform storage device rescan operations, the hostd might fail as multiple threads attempt to modify the same object. You might see error messages similar to the following in the vmkwarning.log file: 

cpu43:nnnnnnn)ALERT: hostd detected to be non-responsive
cpu20:nnnnnnn)ALERT: hostd detected to be non-responsive

This issue is resolved in this release.
 Log spew is observed when ESXi host is added in vCenter server
When you add an ESXi host in vCenter server and create a VMkernel interface for vMotion, you will see the following message displayed in quick succession (log spew) in the hostd.log file:

Failed to find vds Id for portset vSwitch0

This issue is resolved in this release.
 Microsoft Windows Deployment Services (WDS) might fail to PXE boot virtual machines that use the VMXNET3 network adapter
Attempts to PXE boot virtual machines that use the VMXNET3 network adapter by using the Microsoft Windows Deployment Services (WDS) might fail with messages similar to the following:

Windows failed to start. A recent hardware or software change might be the cause. To fix the problem:
1. Insert your Windows installation disc and restart your computer.
2. Choose your language setting, and then click Next.
3. Click Repair your computer.
If you do not have the disc, contact your system administrator or computer manufacturer for assistance.

Status: 0xc0000001

Info: The boot selection failed because a required device is inaccessible.

This issue is resolved in this release.
 Enable the configuration of Rx Ring#2 to solve the Rx Ring#2 out of memory and packet drops on the receiver side issues
A Linux virtual machine enabled with Large Receive Offload (LRO) functionality on VMXNET3 device might experience packet drops on the receiver side when the Rx Ring #2 runs out of memory, since the size of Rx Ring#2 is unable to be configured originally.

This issue is resolved in this release.
 Purple diagnostic screen might be displayed when using DvFilter with a NetQueue supported uplink
An ESXi server might experience a purple diagnostic screen when using DvFilter with a NetQueue supported uplink connected to a vSwitch or a vSphere Distributed Switch (VDS). The ESXi host might report a backtrace similar to the following:

pcpu:22 world:4118 name:"idle22" (IS)
pcpu:23 world:2592367 name:"vmm1:S10274-AAG" (V)
@BlueScreen: Spin count exceeded (^P) - possible deadlock
Code start: 0xnnnnnnnnnnnn VMK uptime: 57:09:18:15.770
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]Panic@vmkernel#nover+0xnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]SP_WaitLock@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetSchedFIFOInput@vmkernel#nover+0xnnn stack: 0x0
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetSchedInput@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]IOChain_Resume@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]PortOutput@vmkernel#nover+0xnn stack: 0xnn

This issue is resolved in this release.
 ESXi host fails with purple diagnostic screen when the Netflow feature is deactivated
An ESXi host might fail with a PF exception 14 purple diagnostic screen when the Netflow feature of vSphere Distributed Switch gets deactivated. The issue occurs due to a timer synchronization problem.

This issue is resolved in this release.
 Changing the network scheduler to SFQ during heavy I/O might result in an unrecoverable transmission
When heavy I/O load is in progress, the SFQ Network scheduler might reset the physical NIC when switching the network schedulers. This might cause an unrecoverable transmission where no packets are transmitted to the driver.

This issue is resolved in this release.
 vmkping command with Jumbo Frames might fail
The vmkping command with Jumbo Frames might fail after one vmknic MTU is changed amongst many in the same switch. An error message similar to the following is displayed:

sendto() failed (Message too long)

This issue is resolved in this release.
 ESXi firewall might reject the services that use port 0-65535 as service port
The Virtual Serial Port Concentrator (vSPC) or NFS client service might not function on the ESXi platform. This happens when there is a different ruleset order, which allows port 0-65535, as a result of enabling sequence. This results in the vSPC or NFS Client related packets to be dropped unexpectedly even if the allowed IP on corresponding ruleset is specified.

This issue is resolved in this release.
 IPv6 RA does not function as expected when tagging 802.1q with VMXNET3 adapters
IPv6 Router Advertisements (RA) does not function as expected when tagging 802.1q with VMXNET3 adapters in an Linux virtual machine as the IPv6 RA address intended for the VLAN interface is delivered to the base interface.

This issue is resolved in this release.
 ESXi host might lose network connectivity
An ESXi host might lose network connectivity and experience stability issues when multiple error messages similar to the following are logged in:

WARNING: Heartbeat: 785: PCPU 63 didn't have a heartbeat for 7 seconds; *may* be locked up.

This issue is resolved in this release.
 Network connectivity lost when applying host profile during Auto Deploy
When applying host profile during Auto Deploy, you might lose network connectivity because the VXLAN Tunnel Endpoint (VTEP) NIC gets tagged as management vmknic.

This issue is resolved in this release.

Security Issues

 Update to the libxml2 library
The ESXi userworld libxml2 library is updated to verion 2.9.2.
 Update to the ESXi userworld OpenSSL library
The ESXi userworld OpenSSL library is updated to version 1.0.1m.
 Update to the libPNG library
The libPNG library is updated to libpng-1.6.16.
Server Configuration Issues

 Serial over LAN Console Redirection might not function properly
A PCIe serial port redirection card might not function properly when connected to an Industry Standard Architecture (ISA) Interrupt Request (IRQ) (0-15 decimals) on an Advanced Programmable Interrupt Controller (APIC) as it is unable have its interrupts received by the CPU. To allow these and other PCI devices connected to ISA IRQs to function, VMkernel will now allow level-triggered interrupts on ISA IRQs.

This issue is resolved in this release. 
 Esxtop might incorrectly display the CPU utilization at 100%
The PCPU UTIL/CORE UTIL in esxtop utility incorrectly displays CPU utilization at 100% if you have the PcpuMigrateIdlePcpus set at 0.

This issue is resolved in this release. 
 Unknown(1) status reported when querying Fiber Channel Host Bus Adapters
After you upgrade the ESXi host from ESXi 5.1 to 5.5 and import the latest MIB module, the third-party monitoring software returns "unknown(1)" status when querying Fiber Channel (FC) Host Bus Adapters (HBA).

This issue is resolved in this release. 
 Host gateway deleted and compliance failures might occur when existing ESXi host profile re-applied to stateful ESXi host
When an existing ESXi host profile is applied to a newly installed ESXi 5.5 host, the profile compliance status might show as noncompliant. This happens when the host profile is created from hosts with VXLAN interface configured, the test for compliance on hosts with the previously created host profile might fail. An error message similar to the following is displayed:

IP route configuration doesn't match the specification

This issue is resolved in this release. 
 Purple diagnostic screen with Page Fault exception displayed in a nested ESXi environment 
In a nested ESXi environment, implementation of CpuSchedAfterSwitch() results in a race condition in the scheduler code and a purple diagnostic screen with Page Fault exception is displayed.

This issue is resolved in this release. 
 iSCSI initiator name allowed when enabling software iSCSI using esxcli 
You can now specify an iSCSI initiator name to the esxcli iscsi software set command.
 Virtual machine might not display a warning message when the CPU is not fully reserved
When you create a virtual machine with sched.cpu.latencySensitivity set to high and power it on, the exclusive affinity for the vCPUs might not get enabled if the VM does not have a full CPU reservation.

In earlier releases, the VM did not display a warning message when the CPU is not fully reserved. For more information, see Knowledge Base article 2087525.

This issue is resolved in this release.
 SNMPD might start automatically after ESXi host upgrade
The SNMPD might start automatically after you upgrade the ESXi host to 5.5 Update 2.

This issue is resolved in this release. 
 Host profiles become non-compliant with simple change to SNMP syscontact or syslocation
Host Profiles become non-compliant with a simple change to SNMP syscontact or syslocation. The issue occurs as the SNMP host profile plugin applies only a single value to all hosts attached to the host profile. An error message similar to the following might be displayed:

SNMP Agent Configuration differs

This issue is resolved in this release by enabling per-host value settings for certain parameters like syslocation, syscontact, v3targets,v3users and engineid.
 Attempts to create a FIFO and write data on it might result in a purple diagnostic screen
When you create a FIFO and attempt to write data to the /tmp/dpafifo, a purple diagnostic screen might displayed under certain conditions.

This issue is resolved in this release.
 Attempts to reboot Windows 8 and Windows 2012 server on ESXi host virtual machines might fail
After you reboot, the Windows 8 and Windows 2012 Server virtual machines might become unresponsive when the Microsoft Windows boot splash screen appears. For more information refer, Knowledge Base article 2092807.

This issue is resolved in this release.
 Attempts to reboot Windows 8 and Windows 2012 server on ESXi host virtual machines might fail
When you set the CPU limit of a uni-processor virtual machine, the overall ESXi utilization might decrease due to a defect in the ESXi scheduler. This happens when the ESXi scheduler considers the cpu-limited VMs as runnable (when they were not running) while making cpu-load estimations. Hence leading to incorrect load balancing decision. 

For details, see Knowledge Base article 2096897.

This issue is resolved in this release.
Supported Hardware Issues

 Power usage and power cap value missing in esxtop command
On Lenovo systems, the value of power usage and power cap is not available in the esxtop command.

This issue is resolved in this release.
Storage Issues

 During an High Availability failover or a host crash, the .vswp files of powered ON VMs on that host might be left behind on the storage
During a High Availability failover or host crash, the .vswp files of powered ON virtual machines on that host might be left behind on the storage. When many such failovers or crashes occur, the storage capacity might become full.

This issue is resolved in this release.
 False PE change message might be displayed in the VMkernel log file when you rescan a VMFS datastore with multiple extents
When you rescan a VMFS datastore with multiple extents, the following log message might be written in the VMkernel log even without any issues from storage connectivity:

Number of PEs for volume changed from 3 to 1. A VMFS volume rescan may be needed to use this volume.

This issue is resolved in this release.
 During transient error conditions, I/O to a device might repeatedly fail and not failover to an alternate working path
During transient error conditions like BUS BUSY, QFULL, HOST ABORTS, HOST RETRY and so on, you might repeatedly attempt commands on current path and do not failover to another path even after a reasonable amount of time.

This issue is resolved in this release. During occurrence of such transient errors, if the path is busy after a couple of retries, the path state is now changed to DEAD. As a result, a failover is triggered and an alternate working path to the device is used to send I/Os.
 During an High Availability failover or a host crash, the .vswp files of powered ON VMs on that host might be left behind on the storage
During a High Availability failover or host crash, the .vswp files of powered ON virtual machines on that host might be left behind on the storage. When many such failovers or crashes occur, the storage capacity might become full.

This issue is resolved in this release.
 Attempts to get block map of an offline storage might cause the hostd service to crash
The hostd service might fail on an ESXi 5.x host when there is an acquireLeaseExt API execution attempt on a snapshot disk which goes offline. This snapshot disk may be on an extent which has gone offline. The API caller may be a third-party backup solution. An error message similar to the following is displayed invmkernel.log:

cpu4:4739)LVM: 11729: Some trailing extents missing (498, 696).

This issue is resolved in this release.
 ESXi 5.5 host might stop responding with a purple diagnostic screen during collection of vm-support log bundle
When any inbox or third-party drivers do not have their SCSI transport-specific interfaces defined, the ESXi host might stop responding and display a purple diagnostic screen. The issue occurs during collection of vm-support log bundles or when you run I/O Device Management (IODM) Command-Line Interfaces (CLI) such as:

esxcli storage san sas list

esxcli storage san sas stats get


This issue is resolved in this release.
 Attempts to expand VMFS volumes beyond 16 TB might not succeed in certain scenarios
An ESXi host might fail when you attempt to expand a VMFS5 datastore beyond 16 TB. Error messages similar to the following is written to the vmkernel.log file:

cpu38:34276)LVM: 2907: [naa.600000e00d280000002800c000010000:1] Device expanded (actual size 61160331231 blocks, stored size 30580164575 blocks)
cpu38:34276)LVM: 2907: [naa.600000e00d280000002800c000010000:1] Device expanded (actual size 61160331231 blocks, stored size 30580164575 blocks)
cpu47:34276)LVM: 11172: LVM device naa.600000e00d280000002800c000010000:1 successfully expanded (new size: 31314089590272)
cpu47:34276)Vol3: 661: Unable to register file system ds02 for APD timeout notifications: Already exists
cpu47:34276)LVM: 7877: Using all available space (15657303277568).
cpu7:34276)LVM: 7785: Error adding space (0) on device naa.600000e00d280000002800c000010000:1 to volume 52f05483-52ea4568-ce0e-901b0e0cd0f0: No space left on device
cpu7:34276)LVM: 5424: PE grafting failed for dev naa.600000e00d280000002800c000010000:1 (opened: t), vol 52f05483-52ea4568-ce0e-901b0e0cd0f0: Limit exceeded
cpu7:34276)LVM: 7133: Device scan failed for <naa.600000e00d280000002800c000010000:1>: Limit exceeded
cpu7:34276)LVM: 7805: LVMProbeDevice failed for device naa.600000e00d280000002800c000010000:1: Limit exceeded
cpu32:38063)<3>ata1.00: bad CDB len=16, scsi_op=0x9e, max=12
cpu30:38063)LVM: 5424: PE grafting failed for dev naa.600000e00d280000002800c000010000:1 (opened: t), vol 52f05483-52ea4568-ce0e-901b0e0cd0f0: Limit exceeded
cpu30:38063)LVM: 7133: Device scan failed for <naa.600000e00d280000002800c000010000:1>: Limit exceeded

This issue is resolved in this release.
 ESXi host might fail with a purple diagnostic screen when multiple vSCSI filters are attached to a VM disk
An ESXi 5.5 host might fail with a purple diagnostic screen similar to the following when multiple vSCSI filters are attached to a VM disk. 

cpu24:103492 opID=nnnnnnnn)@BlueScreen: #PF Exception 14 in world 103492:hostd-worker IP 0xnnnnnnnnnnnn addr 0x30
PTEs:0xnnnnnnnnnn;0xnnnnnnnnnn;0x0;
cpu24:103492 opID=nnnnnnnn)Code start: 0xnnnnnnnnnnnn VMK uptime: 21:06:32:38.296
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSCSIFilter_GetFilterPrivateData@vmkernel#nover+0x1 stack: 0x4136c7d
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSCSIFilter_IssueInternalCommand@vmkernel#nover+0xc3 stack: 0x410961
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_FileSyncRead@<None>#<None>+0xb1 stack: 0x0
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_DigestRecompute@<None>#<None>+0x291 stack: 0x1391
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_FilterDigestRecompute@<None>#<None>+0x36 stack: 0x20
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSI_SetInfo@vmkernel#nover+0x322 stack: 0x411424b18120
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]UWVMKSyscallUnpackVSI_Set@<None>#<None>+0xef stack: 0x41245111df10
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]User_UWVMKSyscallHandler@<None>#<None>+0x243 stack: 0x41245111df20
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]User_UWVMKSyscallHandler@vmkernel#nover+0x1d stack: 0x275c3918
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]gate_entry@vmkernel#nover+0x64 stack: 0x0 

This issue is resolved in this release.
 ESXi host stops responding and loses connection to vCenter Server during storage hiccups on Non-ATS VMFS datastores
An ESXi host might stop responding and the virtual machines become inaccessible. Also, the ESXi host might lose connection to vCenter Server due to a deadlock during storage hicuups on Non-ATS VMFS datastores.

This issue is resolved in this release.
 ESXi host gets registered with an incorrect IQN on a target management software 
Unisphere Storage Management software registers the given initiator IQN when software iSCSI is first enabled. During stateless boot, the registered IQN does not change with the name defined in host profile. You are required to manually remove the initiators from the array and add them again under the new IQN.

This issue is resolved by adding a new parameter to the software iSCSI enable command so that Unisphere registers the initiator under the name defined in the host profile. The command line to set the IQN during software iSCSI enablement is: 

esxcli iscsi software set --enabled=true --name iqn.xyz
 vSphere Replication sync might fail due to change in source datastore name
If you rename a datastore on which replication source virtual machines are running, replication sync operations for these virtual machines fail with an error message similar to the following:

VRM Server runtime error. Please check the documentation for any troubleshooting information.
The detailed exception is: 'Invalid datastore format '<Datastore Name>'

This issue is resolved in this release.
 Attempts to unmount NFS Datastore might fail
Attempts to unmount NFS Datastore might fail as the NFS IOs could be stuck due to connectivity issues during NFS LOCK LOST errors. You will see an error message similar to the following:

cpu23:xxxxx opID=xxxxxabf)WARNING: NFS: 1985: datastore1 has open files, cannot be unmounted

This issue is resolved in this release.
Upgrade and Installation Issues

 Error message observed on the boot screen when ESXi 5.5 host boots from vSphere Auto Deploy Stateless Caching
An error message similar to the following with tracebacks is observed on the boot screen when ESXi 5.5 host boots from Auto Deploy Stateless Caching. The error is due to an unexpected short length message of less than four characters in the syslog network.py script.

IndexError: string index out of range

This issue is resolved in this release.
 Attempts to install or upgrade VMware Tools on a Solaris 10 Update 3 virtual machine might fail
Attempts to install or upgrade VMware Tools on a Solaris 10 Update 3 virtual machine might fail with the following error message:

Detected X version 6.9
Could not read /usr/lib/vmware-tools/configurator/XOrg/7.0/vmwlegacy_drv.so Execution aborted.

This issue occurs if the vmware-config-tools.pl script copies the vmwlegacy_drv.so file, which should not be used in Xorg 6.9.
 Keyboard layout option for DCUI and host profile user interface might be incorrectly displayed as Czechoslovakian
The keyboard layout option for the Direct Console User Interface (DCUI) and host profile user interface might incorrectly appear as Czechoslovakian. This option is displayed during ESXi installation and also in the DCUI after installation.

This issue is resolved in this release by renaming the keyboard layout option to Czech.
 Option to retain tools.conf file available by default
When you upgrade the Vmware Tools in 64-bit Windows guest operating system, the tools.conf file gets removed automatically. The tools.conf file will be retained by default from ESXi 5.5 Update 3 release onwards.
 Guest Operating System might fail on reboot after install, upgrade, or uninstall of VMware Tools
When you power off a virtual machine immediately after an install, upgrade or uninstall of VMware Tools in a Linux environment (RHEL or Cent OS 6), the guest OS might fail during the next reboot due to corrupted RAMDISK image file. The guest OS reports an error similar to the following:

RAMDISK: incomplete write (31522 != 32768)
write error
Kernel panic - not syncing : VFS: Unable to mount root fs on unknown-block(0,0)

This release resolves the complete creation of the initramfs file creation during an install, upgrade or uninstall of VMware Tools.

Guest OS with corrupted RAMDISK image file can be rescued to complete boot state. For more information, see Knowledge Base article 2086520.

This issue is resolved in this release.
 Applying host profile with stateless caching enabled on stateless ESXi host might take a long time to complete
Applying host profile on stateless ESXi host with large number of storage LUNs might take long time to reboot when you enable stateless caching with the esx as first disk argument. This happens when you manually apply host profile or during the reboot of the host.

This issue is resolved in this release.
 VIB stage operation might cause VIB installation or configuration change to be lost after an ESXi host reboot
When some VIBs are installed on the system, esxupdate constructs a new image in /altbootbank and changes the /altbootbank boot.cfg bootstate to be updated. When a live installable VIB is installed, the system saves the configuration change to /altbootbank. The stage operation deletes the contents of/altbootbank unless you perform a remediate operation after the stage operation. The VIB installation might be lost if you reboot the host after a stage operation.

This issue is resolved in this release.
Virtual SAN Issues

 Virtual SAN cluster check might fail due to unexpected network partitioning in the cluster
Virtual SAN cluster check might fail due to an unexpected network partitioning where the IGMP v3 query is not reported if the system is in V2 mode.

This issue is resolved in this release.
 Virtual SAN on high latency disks might cause the Input/Output backlogs and the cluster to become unresponsive
Virtual SAN does not gracefully handle extremely high latency disks that are about to die. Such a dying disk might cause Input/Output backlogs and the Virtual SAN cluster nodes might become unresponsive in the vCenter Server.

This issue is resolved in this release with a new feature, Dying Disk Handling (DDH) which provides latency monitoring framework in the kernel, a daemon to detect high latency periods, and a mechanism to unmount individual disks and diskgroups.
 Improvement in Virtual SAN resynchronization operation
There might be a situation where the Virtual SAN component resynchronization operation might stall or become very slow. This release introduces the component-based congestion to improve the resynchronization operation and the virtual SAN cluster stability.
vCenter Server and vSphere Web Client Issues

 The Summary tab might display incorrect values for provisioned space values of virtual machines and NFS or NAS Datastores on VAAI enabled hosts
When a virtual disk with Thick Provision Lazy zeroed format is created on a VAAI supported NAS in a VAAI enabled ESXi host, the provisioned space for the corresponding virtual machine and datastore might be displayed incorrectly.

This issue is resolved in this release.
Virtual Machine Management Issues

 Attempts to add USB device through vSphere Client or the vSphere Web Client might fail
Attempts to add USB device(s) through vSphere Client and vSphere Web Client might fail if Intel USB 3.0 driver is used.

This issue is resolved in this release.
 Taking quiesce snapshot of a virtual machine might result in the MOB value of currentSnapshot field to be unset
After you create a quiesce snapshot and browse through the Managed Object Browser (MOB) of the virtual machine, the MOB value of currentSnapshot field is observed to be unset. To view the currentSnapshot, you can navigate to Content -> root folder -> datacenter -> vmFolder -> vmname -> snapshot -> currentSnaphot.

This issue is resolved in this release.
 Multiple opID tagging log messages are rapidly logged in the VMkernel log
The helper world opID tagging generates a lot of log messages that are logged rapidly in the VMkernel log filling it up. Logs similar to the following are logged in the VMkernel log:

cpu16:nnnnn)World: nnnnn: VC opID hostd-60f4 maps to vmkernel opID nnnnnnnn cpu16:nnnnn)World: nnnnn: VC opID HB-host-nnn@nnn-nnnnnnn-nn maps to vmkernel opID nnnnnnnn cpu8:nnnnn)World: nnnnn: VC opID SWI-nnnnnnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu22:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu4:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn

This issue is resolved in this release.
 Support for USB 3.0
Support for USB 3.0 has been added in this release, currently only for Apple Mac Pro.

High Availability and Fault Tolerance Issues

 In an environment with Nutanix NFS storage, the secondary FT virtual machine fails to take over when the primary FT virtual machine is down (KB 2096296).
vMotion and Storage vMotion Issues

 Unable to perform Fast Suspend and Resume or Storage vMotion on preallocated virtual machines
When you perform Fast Suspend and Resume (FSR) or Storage vMotion on preallocated virtual machines, the operation might fail as the reservation validation fails during reservation transfer from the source to the destination virtual machine.

This issue is resolved in this release.
 Storage vMotion fails on a virtual machine
Performing a storage vMotion on a virtual machine might fail if you have configured the local host swap and set the value of the checkpoint.cptConfigName in the VMX file. An error message similar to the following might be displayed:

xxxx-xx-xxT00:xx:xx.808Z| vmx| I120: VMXVmdbVmVmxMigrateGetParam: type: 2 srcIp=<127.0.0.1> dstIp=<127.0.0.1> mid=xxxxxxxxxxxxx uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx priority=none checksumMemory=no maxDowntime=0 encrypted=0 resumeDuringPageIn=no latencyAware=no diskOpFile=
<snip>
xxxx-xx-xxT00:xx:xx.812Z| vmx| I120: VMXVmdb_SetMigrationHostLogState: hostlog state transits to failure for migrate 'to' mid xxxxxxxxxxxxxxxx

This issue is resolved in this release.
 Changed Block Tracking (CBT) is reset for virtual RDM disks during cold migration 
Cold migration between different datastores does not support CBT reset for the virtual Raw Device Mapping (RDM) disks. 

This issue is resolved in this release.
VMware Tools Issues

 Attempts to upgrade VMware Tools on a Windows 2000 virtual machine might fail
Attempts to upgrade VMware Tools on a Windows 2000 virtual machine might fail with an error message similar to the following written to the vmmsi.log file:

Invoking remote custom action. DLL: C:\WINNT\Installer\MSI12.tmp, Entrypoint: VMRun
VM_CacheMod. Return value 3.
PROPERTY CHANGE: Deleting RESUME property. Its current value is '1'.
INSTALL. Return value 3.

This issue is resolved in this release.
 Some of the drivers might not work as expected on Solaris 11 virtual machine
On an ESXi 5.5 host, some of the drivers installed on Solaris 11 guest operating system might be from Solaris 10. As a result, the drivers might not work as expected.

This issue is resolved in this release.
 Attempts to configure VMware Tools with new kernel might truncate the driver list in add_drivers entry
When you attempt to configure VMware Tools with new kernel using the /usr/bin/vmware-config-tools.pl -k <kernel version> script after the kernel has been updated with Dracut, the driver list in add_drivers entry of /etc/dracut.conf.d/vmware-tools.conf file gets truncated. This issue occurs when the VMware Tools are upstreamed in the kernel.

This issue is resolved in this release.
 Unable to open telnet on Windows 8 or Windows Server 2012 guest operating system after installing VMware Tools
After installing VMware Tools on Windows 8 or Windows Server 2012 guest operating system, attempts to open telnet using the start telnet://xx.xx.xx.xxcommand fails with the following error message

Make sure the virtual machine's configuration allows the guest to open host applications

This issue is resolved in this release.
 Guest operating system event viewer displays warning messages after you install VMware Tools
After you install VMware Tools, if you attempt to do a RDP to a Windows virtual machine, some of the plugins might display a warning message in the Windows event log. The warning message indicates the failure to send remote procedure calls to the host.

This issue is resolved in this release.
 VMware Tools service might fail on a Linux virtual machine during shutdown
On a Linux virtual machine, the VMware Tools service, vmtoolsd, might fail when you shut down the guest operating system.

This issue is resolved in this release.
 VMware Tools might fail to automatically upgrade during the first power-on operation of the virtual machine
When a virtual machine is deployed or cloned with guest customization and the VMware Tools Upgrade Policy is set to allow the virtual machine to automatically upgrade VMware Tools at next power-on, VMware Tools might fail to automatically upgrade during the first power-on operation of the virtual machine.

This issue is resolved in this release.
 Quiescing operations might result in a Windows virtual machine to panic
Attempts to perform a quiesced snapshot on a virtual machine running Microsoft Windows 2008 or later might fail and the VM might panic with a blue screen and error message similar to the following:

A problem has been detected and Windows has been shut down to prevent damage to your computer. If this is the first time you've seen this Stop error screen restart your computer. If this screen appears again, follow these steps:

Disable or uninstall any anti-virus, disk defragmentation or backup utilities. Check your hard drive configuration, and check for any updated drivers. Run CHKDSK /F to check for hard drive corruption, and then restart your computer.

For more information, see Knowledge Base article 2115997.

This issue is resolved in this release.
 Virtual machine might fail to respond after a snapshot operation on a Linux VM
When you attempt to create a quiesced snapshot of a Linux virtual machine, the VM might fail after the snapshot operation and require a reboot. Error messages similar to the following are written to vmware.log file:

TZ| vmx| I120: SnapshotVMXTakeSnapshotComplete: done with snapshot 'smvi_UUID': 0
TZ| vmx| I120: SnapshotVMXTakeSnapshotComplete: Snapshot 0 failed: Failed to quiesce the virtual machine (40).
TZ| vmx| I120: GuestRpcSendTimedOut: message to toolbox timed out.
TZ| vmx| I120: Vix: [18631 guestCommands.c:1926]: Error VIX_E_TOOLS_NOT_RUNNING in
MAutomationTranslateGuestRpcError(): VMware Tools are not running in the guest

For further details, see Knowledge Base article 2116120

This issue is resolved in this release.
 New Issue Attempts to perform snapshot consolidation might fail with the error: Unexpected signal: 11
Snapshot consolidation or deletion results in the virtual machines running on VMware ESXi 5.5 Update 3 hosts to fail with the error: Unexpected signal: 11. You will see an log message similar to the following in the vmware.log file:

[YYYY-MM-DD] <time>Z| vcpu-0| I120: SNAPSHOT: SnapshotDiskTreeFind: Detected node change from 'scsiX:X' to ''.

For further details, see Knowledge Base article 2133118.

This issue is resolved in this release.
Known Issues

The known issues existing in ESXi 5.5 are grouped as follows:
Installation and Upgrade Issues
Networking Issues
Server Configuration Issues
Storage Issues
Virtual SAN Issues
vCenter Server and vSphere Web Client Issues
Virtual Machine Management Issues
VMware HA and Fault Tolerance Issues
Supported Hardware Issues
VMware Tools Issues
Miscellaneous Issues
New known issues documented in this release are highlighted as New Issue.
Installation and Upgrade Issues
New Issue The VMware Tools service user processes might not run on Linux OS after installing the latest VMware Tools package
On Linux OS, you might encounter VMware Tools upgrade or installation issues or the VMware Tools service (vmtoolsd) user processes might not run after installing the latest VMware Tools package. The issue occurs if the your glibc version is older than version 2.5, like SLES10sp4.
Workaround: Upgrade the Linux glibc to version 2.5 or above.
Attempts to get all image profiles might fail while running the Get-EsxImageProfile command in vSphere PowerCLI
When you run the Get-EsxImageProfile command using vSphere PowerCLI to get all image profiles, an error similar to the following is displayed:

PowerCLI C:\Windows\system32> Get-EsxImageProfile
Get-EsxImageProfile : The parameter 'name' cannot be an empty string.
Parameter name: name
At line:1 char:20
+ Get-EsxImageProfile <<<<
+ CategoryInfo : NotSpecified: (:) [Get-EsxImageProfile], ArgumentException
+ FullyQualifiedErrorId : System.ArgumentException,VMware.ImageBuilder.Commands.GetProfiles

Workaround: Run the Get-EsxImageProfile -name "ESXi-5.x*" command, which includes the -name option and display all image profiles created during the PowerCLI session.

For example, running the command Get-EsxImageProfile -name "ESXi-5.5.*" displays all 5.5 image profiles similar to the following: 

PowerCLI C:\Program Files (x86)\VMware\Infrastructure\vSphere PowerCLI> Get-EsxmageProfile -name "ESXi-5.5.*"

Name Vendor Last Modified Acceptance Level
---- ------ ------------- ----------------
ESXi-5.5.0-20140701001s-no-... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140302001-no-t... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140604001-no-t... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140401020s-sta... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20131201001s-sta... VMware, Inc. 8/23/2014 6:... PartnerSupported
Simple Install fails on Windows Server 2012
Simple Install fails on Windows Server 2012 if the operating system is configured to use a DHCP IP address
Workaround: Configure the Windows 2012 Server to use a static IP address.
If you use preserve VMFS with Auto Deploy Stateless Caching or Auto Deploy Stateful Installs, no core dump partition is created
When you use Auto Deploy for Stateless Caching or Stateful Install on a blank disk, an MSDOS partition table is created. However, no core dump partition is created.
Workaround: When you enable the Stateless Caching or Stateful Install host profile option, select Overwrite VMFS, even when you install on a blank disk. When you do so, a 2.5GB coredump partition is created.
During scripted installation, ESXi is installed on an SSD even though the --ignoressd option is used with the installorupgrade command
In ESXi 5.5, the --ignoressd option is not supported with the installorupgrade command. If you use the --ignoressd option with the installorupgradecommand, the installer displays a warning that this is an invalid combination. The installer continues to install ESXi on the SSD instead of stopping the installation and displaying an error message.
Workaround: To use the --ignoressd option in a scripted installation of ESXi, use the install command instead of the installorupgrade command.
Delay in Auto Deploy cache purging might apply a host profile that has been deleted
After you delete a host profile, it is not immediately purged from the Auto Deploy. As long as the host profile is persisted in the cache, Auto Deploy continues to apply the host profile. Any rules that apply the profile fail only after the profile is purged from the cache.
Workaround: You can determine whether any rules use deleted host profiles by using the Get-DeployRuleSet PowerCLI cmdlet. The cmdlet shows the stringdeleted in the rule's itemlist. You can then run the Remove-DeployRule cmdlet to remove the rule.
Applying host profile that is set up to use Auto Deploy with stateless caching fails if ESX is installed on the selected disk
You use host profiles to set up Auto Deploy with stateless caching enabled. In the host profile, you select a disk on which a version of ESX (not ESXi) is installed. When you apply the host profile, an error that includes the following text appears. 
Expecting 2 bootbanks, found 0
Workaround: Select a different disk to use for stateless caching, or remove the ESX software from the disk. If you remove the ESX software, it becomes unavailable.
Installing or booting ESXi version 5.5.0 fails on servers from Oracle America (Sun) vendors
When you perform a fresh ESXi version 5.5.0 installation or boot an existing ESXi version 5.5.0 installation on servers from Oracle America (Sun) vendors, the server console displays a blank screen during the installation process or when the existing ESXi 5.5.0 build boots. This happens because servers from Oracle America (Sun) vendors have a HEADLESS flag set in the ACPI FADT table, even though they are not headless platforms.
Workaround: When you install or boot ESXi 5.5.0, pass the boot option ignoreHeadless="TRUE".
If you use ESXCLI commands to upgrade an ESXi host with less than 4GB physical RAM, the upgrade succeeds, but some ESXi operations fail upon reboot
ESXi 5.5 requires a minimum of 4GB of physical RAM. The ESXCLI command-line interface does not perform a pre-upgrade check for the required 4GB of memory. You successfully upgrade a host with insufficient memory with ESXCLI, but when you boot the upgraded ESXi 5.5 host with less than 4GB RAM, some operations might fail.
Workaround: None. Verify that the ESXi host has more than 4GB of physical RAM before the upgrade to version 5.5.
After upgrade from vCenter Server Appliance 5.0.x to 5.5, vCenter Server fails to start if an external vCenter Single Sign-On is used 
If the user chooses to use an external vCenter Single Sign-On instance while upgrading the vCenter Server Appliance from 5.0.x to 5.5, the vCenter Server fails to start after the upgrade. In the appliance management interface, the vCenter Single Sign-On is listed as not configured.
Workaround: Perform the following steps:
In a Web browser, open the vCenter Server Appliance management interface (https://appliance-address:5480).
On the vCenter Server/Summary page, click the Stop Server button.
On the vCenter Server/SSO page, complete the form with the appropriate settings, and click Save Settings.
Return to the Summary page and click Start Server.
When you use ESXCLI to upgrade an ESXi 4.x or 5.0.x host to version 5.1 or 5.5, the vMotion and Fault Tolerance Logging (FT Logging) settings of any VMKernel port group are lost after the upgrade
If you use the command esxcli software profile update <options> to upgrade an ESXi 4.x or 5.0.x host to version 5.1 or 5.5, the upgrade succeeds, but the vMotion and FT Logging settings of any VMkernel port group are lost. As a result, vMotion and FT Logging are restored to the default setting (disabled).
Workaround: Perform an interactive or scripted upgrade, or use vSphere Update Manager to upgrade hosts. If you use the esxcli command, apply vMotion and FT Logging settings manually to the affected VMkernel port group after the upgrade.
When you upgrade vSphere 5.0.x or earlier to version 5.5, system resource allocation values that were set manually are reset to the default value
In vSphere 5.0.x and earlier, you modify settings in the system resource allocation user interface as a temporary workaround. You cannot reset the value for these settings to the default without completely reinstalling ESXi. In vSphere 5.1 and later, the system behavior changes, so that preserving custom system resource allocation settings might result in values that are not safe to use. The upgrade resets all such values.
Workaround: None.
IPv6 settings of virtual NIC vmk0 are not retained after upgrade from ESX 4.x to ESXi 5.5
When you upgrade an ESX 4.x host with IPv6 enabled to ESXi 5.5 by using the --forcemigrate option, the IPv6 address of virtual NIC vmk0 is not retained after the upgrade.
Workaround: None.
Networking Issues
Unable to use PCNet32 network adapter with NSX opaque network
When PCNet32 flexible network adapter is configured with NSX opaque network backing, the adapter disconnects while powering on the VM.
Workaround: None
 Upgrading to ESXi 5.5 might change the IGMP configuration of TCP/IP stack for multicast group management
The default IGMP version of the management interfaces is changed from IGMP V2 to IGMP V3 for ESXi 5.5 hosts for multicast group management. As a result, when you upgrade to ESXi 5.5, the management interface might revert back to IGMP V2 from IGMP V3 if it receives an IGMP query of a previous version and you might notice IGMP version mismatch error messages.

Workaround: Edit the default IGMP version by modifying the TCP/IP IGMP rejoin interval in the Advanced Configuration option.
Static routes associated with vmknic interfaces and dynamic IP addresses might fail to appear after reboot
After you reboot the host, static routes that are associated with VMkernel network interface (vmknic) and dynamic IP address might fail to appear.
This issue occurs due to a race condition between DHCP client and restore routes command. The DHCP client might not finish acquiring an IP address for vmknics when the host attempts to restore custom routes during the reboot process. As a result, the gateway might not be set up and the routes are not restored.

Workaround: Run the esxcfg-route –r command to restore the routes manually.
An ESXi host stops responding after being added to vCenter Server by its IPv6 address
When you add an ESXi host to vCenter Server by IPv6 link-local address of the form fe80::/64, within a short time the host name becomes dimmed and the host stops responding to vCenter Server.
Workaround: Use a valid IPv6 address that is not a link-local address.
The vSphere Web Client lets you configure more virtual functions than are supported by the physical NIC and does not display an error message
In the SR-IOV settings of a physical adapter, you can configure more virtual functions than are supported by the adapter. For example, you can configure 100 virtual functions on a NIC that supports only 23, and no error message appears. A message prompts you to reboot the host so that the SR-IOV settings are applied. After the host reboots, the NIC is configured with as many virtual functions as the adapter supports, or 23 in this example. The message that prompts you to reboot the host persists when it should not appear.
Workaround: None
On an SR-IOV enabled ESXi host, virtual machines associated with virtual functions might not start
When SR-IOV is enabled on an ESXi host 5.1 or later with Intel ixgbe NICs, if several virtual functions are enabled in the environment, some virtual machines might fail to start.
The vmware.log file contains messages similar to the following:
2013-02-28T07:06:31.863Z| vcpu-1| I120: Msg_Post: Error
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.log.error.unrecoverable] VMware ESX unrecoverable error: (vcpu-1)
2013-02-28T07:06:31.863Z| vcpu-1| I120+ PCIPassthruChangeIntrSettings: 0a:17.3 failed to register interrupt (error code 195887110)
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.haveLog] A log file is available in "/vmfs/volumes/5122262e-ab950f8e-cd4f-b8ac6f917d68/VMLibRoot/VMLib-RHEL6.2-64-HW7-default-3-2-1361954882/vmwar
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.requestSupport.withoutLog] You can request support.
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.requestSupport.vmSupport.vmx86]
2013-02-28T07:06:31.863Z| vcpu-1| I120+ To collect data to submit to VMware technical support, run "vm-support".
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.response] We will respond on the basis of your support entitlement.
Workaround: Reduce the number of virtual functions associated with the affected virtual machine before starting it.
On an Emulex BladeEngine 3 physical network adapter, a virtual machine network adapter backed by a virtual function cannot reach a VMkernel adapter that uses the physical function as an uplink
Traffic does not flow between a virtual function and its physical function. For example, on a switch backed by the physical function, a virtual machine that uses a virtual function on the same port cannot contact a VMkernel adapter on the same switch. This is a known issue of the Emulex BladeEngine 3 physical adapters. For information, contact Emulex.
Workaround: Disable the native driver for Emulex BladeEngine 3 devices on the host. For more information, see VMware KB 2044993.
The ESXi Dump Collector fails to send the ESXi core file to the remote server
The ESXi Dump Collector fails to send the ESXi core file if the VMkernel adapter that handles the traffic of the dump collector is configured to a distributed port group that has a link aggregation group (LAG) set as the active uplink. An LACP port channel is configured on the physical switch.
Workaround: Perform one of the following workarounds:
Use a vSphere Standard Switch to configure the VMkernel adapter that handles the traffic for the ESXi Dump Collector with the remote server.
Use standalone uplinks to handle the traffic for the distributed port group where the VMkernel adapter is configured.
If you change the number of ports that a vSphere Standard Switch or vSphere Distributed Switch has on a host by using the vSphere Client, the change is not saved, even after a reboot
If you change the number of ports that a vSphere Standard Switch or vSphere Distributed Switch has on an ESXi 5.5 host by using the vSphere Client, the number of ports does not change even after you reboot the host.
When a host that runs ESXi 5.5 is rebooted, it dynamically scales up or down the ports of virtual switches. The number of ports is based on the number of virtual machines that the host can run. You do not have to configure the number of switch ports on such hosts.
Workaround: None in the vSphere Client.
Server Configuration Issues
New Issue NIC hardware might stop responding with an hardware error message
The NIC hardware might occassionally stop responding under certain circumstances with the following error message in the driver logs:

Detected Hardware Unit Hang

The issue is observed with some new e1000e devices like 82579, i217, i218 and i219.
Workaround: The NIC hardware resets itself after the issue occurs.
Menu navigation problem is experienced When Direct Control User Interface is accessed from a serial console
When Direct Control User Interface is accessed from a serial console, the Up and Down arrow keys do not work while navigating to the menu and the user is forcefully logged out of the DCUI configuration screen.

Workaround: Stop the DCUI process. The DCUI process will be restarted automatically.

Host profiles might incorrectly appear as compliant after ESXi hosts are upgrade to 5.5 Update 2 followed by changes in host configuration 
If an ESXi host that is compliant with an host profile is updated to ESXi 5.5 Update 2 followed by some changes in host configuration and you re-check the compliance of the host with the host profile, the profile is incorrectly reported to be compliant. 

Workaround:
In vSPhere Client, navigate to the host profile that has the issue and run Update profile From Reference Host.
In vSPhere Web Client, navigate to host Profile that has the issue, click Copy settings from host, select the host from which you want to copy the configuration settings and click OK.
Host Profile remediation fails with vSphere Distributed Switch
Remediation errors might occur when applying a Host Profile with a vSphere Distributed Switch and a virtual machine with Fault Tolerance is in a powered off state on a host that uses the distributed switch in that Host Profile.
Workaround: Move the powered off virtual machines to another host in order for the Host Profile to succeed.
Host profile receives firewall settings compliance errors when you apply ESX 4.0 or ESX 4.1 profile to ESXi 5.5.x host
If you extract a host profile from an ESX 4.0 or ESX 4.1 host and attempt to apply it to an ESXi 5.5.x host, the profile remediation succeeds. The compliance check receives firewall settings errors that include the following:

Ruleset LDAP not found
Ruleset LDAPS not found
Ruleset TSM not found
Ruleset VCB not found
Ruleset activeDirectorKerberos not found
Workaround: No workaround is required. This is expected because the firewall settings for an ESX 4.0 or ESX 4.1 host are different from those for an ESXi 5.5.x host.
Changing BIOS device settings for an ESXi host might result in invalid device names
Changing a BIOS device setting on an ESXi host might result in invalid device names if the change causes a shift in the <segment:bus:device:function> values assigned to devices. For example, enabling a previously-disabled integrated NIC might shift the <segment:bus:device:function> values assigned to other PCI devices, causing ESXi to change the names assigned to these NICs. Unlike previous versions of ESXi, ESXi 5.5 attempts to preserve devices names through <segment:bus:device:function> changes if the host BIOS provides specific device location information. Due to a bug in this feature, invalid names such as vmhba1 and vmnic32 are sometimes generated.
Workaround: Rebooting the ESXi host once or twice might clear the invalid device names and restore the original names. Do not run an ESXi host with invalid device names in production.
Storage Issues
ESXi hosts with HBA drivers might stop responding when the VMFS heartbeats to the datastores timeout
ESXi hosts with HBA drivers might stop responding when the VMFS heartbeats to the datastores timeout with messages similar to the following:

mem>2014-05-12T13:34:00.639Z cpu8:1416436)VMW_SATP_ALUA: satp_alua_issueCommandOnPath:651: Path "vmhba2:C0:T1:L10" (UP) command 0xa3 failed with status Timeout. H:0x5 D:0x0 P:0x0 Possible sense data: 0x5 0x20 0x0.2014-05-12T13:34:05.637Z cpu0:33038)VMW_SATP_ALUA: satp_alua_issueCommandOnPath:651: Path "vmhba2:C0:T1:L4" (UP) command 0xa3 failed with status Timeout. H:0x5 D:0x0 P:0x0 Possible sense data: 0x0 0x0 0x0.

This issue occurs with the HBA driver when a high disk I/O on the datastore is connected to the ESXi host and multipathing is enabled at the target level instead of the HBA level.

Workaround: Replace the HBA driver with the latest async HBA driver.
Attempts to perform live storage vMotion of virtual machines with RDM disks might fail 
Storage vMotion of virtual machines with RDM disks might fail and virtual machines might be seen in powered off state. Attempts to power on the virtual machine fails with the following error: 

Failed to lock the file

Workaround: None.
Renamed tags appear as missing in the Edit VM Storage Policy wizard
A virtual machine storage policy can include rules based on datastore tags. If you rename a tag, the storage policy that references this tag does not automatically update the tag and shows it as missing.
Workaround: Remove the tag marked as missing from the virtual machine storage policy and then add the renamed tag. Reapply the storage policy to all out-of-date entities.
A virtual machine cannot be powered on when the Flash Read Cache block size is set to 16KB, 256KB, 512KB, or 1024KB
A virtual machine configured with Flash Read Cache and a block size of 16KB, 256KB, 512KB, or 1024KB cannot be powered on. Flash Read Cache supports a minimum cache size of 4MB and maximum of 200GB, and a minimum block size of 4KB and maximum block size of 1MB. When you power on a virtual machine, the operation fails and the following messages appear:
An error was received from the ESX host while powering on VM.
Failed to start the virtual machine.
Module DiskEarly power on failed.
Failed to configure disk scsi0:0.
The virtual machine cannot be powered on with an unconfigured disk. vFlash cache cannot be attached: msg.vflashcache.error.VFC_FAILURE
Workaround: Configure virtual machine Flash Read Cache size and block size.
Right-click the virtual machine and select Edit Settings.
On the Virtual Hardware tab, expand Hard disk to view the disk options.
Click Advanced next to the Virtual Flash Read Cache field.
Increase the cache size reservation or decrease the block size.
Click OK to save your changes.
A custom extension of a saved resource pool tree file cannot be loaded in the vSphere Web Client
A DRS error message appears on host summary page.
When you disable DRS in the vSphere Web Client, you are prompted to save the resource pool structure so that it can be reloaded in the future. The default extension of this file is .snapshot, but you can select a different extension for this file. If the file has a custom extension, it appears as disabled when you try to load it. This behavior is observed only on OS X.
Workaround: Change the extension to .snapshot to load it in the vSphere Web Client on OS X.
DRS error message appears on the host summary page
The following DRS error message appears on the host summary page:
Unable to apply DRS resource settings on host. The operation is not allowed in the current state. This can significantly reduce the effectiveness of DRS.
In some configurations a race condition might result in the creation of an error message in the log that is not meaningful or actionable. This error might occur if a virtual machine is unregistered at the same time that DRS resource settings are applied.
Workaround: Ignore this error message.
Configuring virtual Flash Read Cache for VMDKs larger than 16TB results in an error
Virtual Flash Read Cache does not support virtual machine disks larger than 16TB. Attempts to configure such disks will fail.
Workaround: None
Virtual machines might power off when the cache size is reconfigured 
If you incorrectly reconfigure the virtual Flash Read Cache on a virtual machine, for example by assigning an invalid value, the virtual machine might power off.
Workaround: Follow the recommended cache size guidelines in the vSphere Storage documentation.
Reconfiguring a virtual machine with virtual Flash Read Cache enabled might fail with the Operation timed out error 
Reconfiguration operations require a significant amount of I/O bandwidth. When you run a heavy load, such operations might time out before they finish. You might also see this behavior if the host has LUNs that are in an all paths down (APD) state.
Workaround: Fix all host APD states and retry the operation with a smaller I/O load on the LUN and host.
DRS does not vMotion virtual machines with virtual Flash Read Cache for load balancing purpose
DRS does not vMotion virtual machines with virtual Flash Read Cache for load balancing purposes.
Workaround: DRS does not recommend these virtual machines for vMotion except for the following reasons:
To evacuate a host that the user has requested to enter maintenance or standby mode.
To fix DRS rule violations.
Host resource usage is in red state.
One or most hosts is over utilized and virtual machine demand is not being met. 
Note: You can optionally set DRS to ignore this reason.
Hosts are put in standby when the active memory of virtual machines is low but consumed memory is high
ESXi 5.5 introduces a change in the default behavior of DPM designed to make the feature less aggressive, which can help prevent performance degradation for virtual machines when active memory is low but consumed memory is high. The DPM metric is X%*IdleConsumedMemory + active memory. The X% variable is adjustable and is set to 25% by default.
Workaround: You can revert to the aggressive DPM behavior found in earlier releases of ESXi by setting PercentIdleMBInMemDemand=0 in the advanced options.
vMotion initiated by DRS might fail
When DRS recommends vMotion for virtual machines with a virtual Flash Read Cache reservation, vMotion might fail because the memory (RAM) available on the target host is insufficient to manage the Flash Read Cache reservation of the virtual machines.
Workaround: Follow the Flash Read Cache configuration recommendations documented in vSphere Storage.
If vMotion fails, perform the following steps:
Reconfigure the block sizes of the virtual machines on the target host and the incoming virtual machines to reduce the overall target usage of the VMkernel memory on the target host.
Use vMotion to manually migrate the virtual machine to the target host to ensure the condition is resolved.
You are unable to view problems that occur during virtual flash configuration of individual SSD devices 
The configuration of virtual flash resources is a task that operates on a list of SSD devices. When the task finishes for all objects, the vSphere Web Client reports it as successful, and you might not be notified of problems with the configuration of individual SSD devices.
Workaround: Perform one of the following tasks.
In the Recent Tasks panel, double-click the completed task. 
Any configuration failures appear in the Related events section of the Task Details dialog box.
Alternatively, follow these steps:
Select the host in the inventory.
Click the Monitor tab, and click Events.
Unable to obtain SMART information for Micron PCIe SSDs on the ESXi host 
Your attempts to use the esxcli storage core device smart get -d command to display statistics for the Micron PCIe SSD device fail. You get the following error message:
Error getting Smart Parameters: CANNOT open device
Workaround: None. In this release, the esxcli storage core device smart command does not support Micron PCIe SSDs.
ESXi does not apply the bandwidth limit that is configured for a SCSI virtual disk in the configuration file of a virtual machine
You configure the bandwidth and throughput limits of a SCSI virtual disk by using a set of parameters in the virtual machine configuration file (.vmx). For example, the configuration file might contain the following limits for a scsi0:0 virtual disk:
sched.scsi0:0.throughputCap = "80IOPS"
sched.scsi0:0.bandwidthCap = "10MBps"
sched.scsi0:0.shares = "normal"
ESXi does not apply the sched.scsi0:0.bandwidthCap limit to the scsi0:0 virtual disk.
Workaround: Revert to an earlier version of the disk I/O scheduler by using the vSphere Web Client or the esxcli system settings advanced set command.
In the vSphere Web Client, edit the Disk.SchedulerWithReservation parameter in the Advanced System Settings list for the host.
Navigate to the host.
On the Manage tab, select Settings and select Advanced System Settings.
Locate the Disk.SchedulerWithReservation parameter, for example, by using the Filter or Find text boxes.
Click Edit and set the parameter to 0.
Click OK.
In the ESXi Shell to the host, run the following console command:
esxcli system settings advanced set -o /Disk/SchedulerWithReservation -i=0
A virtual machine configured with Flash Read Cache cannot be migrated off a host if there is an error in the cache
A virtual machine with Flash Read Cache configured might have a migration error if the cache is in an error state and is unusable. This error causes migration of the virtual machine to fail.
Workaround:
Reconfigure the virtual machine and disable the cache.
Perform the migration.
Re-enable the cache after the virtual machine is migrated.
Alternatively, the virtual machine must be powered off and then powered on to correct the error with the cache.
You cannot delete the VFFS volume after a host is upgraded from ESXi 5.5 Beta
You cannot delete the VFFS volume after a host is upgraded from ESXi 5.5 Beta.
Workaround: This occurs only when you upgrade from ESXi 5.5 Beta to ESXi 5.5. To avoid this problem, install ESXi 5.5 instead of upgrading. If a you upgrade from ESXi 5.5 Beta, delete the VFFS volume before you upgrade.
Expected latency runtime improvements are not seen when virtual Flash Read Cache is enabled on virtual machines with older Windows and Linux guest operating systems
Virtual Flash Read Cache provides optimal performance when the cache is sized to match the target working set, and when the guest file systems are aligned to at least a 4KB boundary. The Flash Read Cache filters out misaligned blocks to avoid caching partial blocks within the cache. This behavior is typically seen when virtual Flash Read Cache is configured for VMDKs of virtual machines with Windows XP and Linux distributions earlier than 2.6. In such cases, a low cache hit rate with a low cache occupancy is observed, which implies a waste of cache reservation for such VMDKs. This behavior is not seen with virtual machines running Windows 7, Windows 2008, and Linux 2.6 and later distributions, which align their file systems to a 4KB boundary to ensure optimal performance.
Workaround: To improve the cache hit rate and optimal use of the cache reservation for each VMDK, ensure that the guest operating file system installed on the VMDK is aligned to at least a 4KB boundary.
Virtual SAN
 New Issue Unmounted Virtual SAN disks and diskgroups displayed as mounted in the vSphere Client UI Operational Status field
After the Virtual SAN disks or diskgroups are unmounted using the esxcli vsan storage diskgroup unmount CLI command or automatically by the Virtual SAN Device Monitor service when disks show persistently high latencies, the vSphere Client UI incorrectly displays the Operational Status field as Mounted. 

Workaround: Verify the Health field that shows a non-healthy value instead of the Operational Status field.
 ESXi host with multiple VSAN disk groups might not display the magnetic disk statistics when you run the vsan.disks_stats command
An ESXi host with multiple VSAN disk groups might not display the magnetic disk (MD) statistics when you run the vsan.disks_stats Ruby vSphere Console (RVC)command. The host displays only the solid-state drive (SSD) information.

Workaround: None 
VM directories contain duplicate swap (.vswp) files
This might occur if virtual machines running on Virtual SAN are not cleanly shutdown, and if you perform a fresh installation of ESXi and vCenter Server without erasing data from Virtual SAN disks. As a result, old swap files (.vswp) are found in the directories for virtual machines that are shut down uncleanly.

Workaround: None

Attempts to add more than seven magnetic disks to a Virtual SAN disk group might fail with incorrect error message
Virtual SAN disk group supports maximum of one SSD and seven magnetic disks (HDD). Attempts to add an additional magnetic disk might fail with an incorrect error message similar to the following:

The number of disks is not sufficient.

Workaround: None
Re-scan failure experienced while adding a Virtual SAN disk
When you add a Virtual SAN disk, re-scan fails due to probe failure for a non-Virtual SAN volume, which causes the operation to fail.

Workaround: Ignore the error as all the disks are registered correctly.
A hard disk drive (HDD) that is removed after its associated solid state drive (SSD) is removed might still be listed as a storage disk claimed by Virtual SAN
If an SSD and then its associated HDD is removed from a Virtual SAN datastore and you run the esxcli vsan storage list command, the removed HDD is still listed as a storage disk claimed by Virtual SAN. If the HDD is inserted back in a different host, the disk might appear to be part of two different hosts.

Workaround: For example, if SSD and HDD is removed from ESXi x and inserted into ESXi y, perform the following steps to prevent the HDD from appearing to be a part of both ESXi x and ESXi y:
1. Insert the SSD and HDD removed from the ESXi x, into ESXi y.
2. Decommission the SSD from ESXi x.
3. Run the command esxcfg-rescan -A.
   The HDD and SSD will no longer be listed on ESXi x.
The Working with Virtual SAN section of the vSphere Storage documentation indicates that the maximum number of HDD disks per a disk group is six. However, the maximum allowed number of HDDs is seven.
After a failure in a Virtual SAN cluster, vSphere HA might report multiple events, some misleading, before restarting a virtual machine
The vSphere HA master agent makes multiple attempts to restart a virtual machine running on Virtual SAN after it has appeared to have failed. If the virtual machine cannot be immediately restarted, the master agent monitors the cluster state, and makes another attempt when conditions indicate that a restart might be successful. For virtual machines running on Virtual SAN, the vSphere HA master has special application logic to detect when the accessibility of a virtual machine's objects might have changed, and attempts a restart whenever an accessibility change is likely. The master agent makes an attempt after each possible accessibility change, and if it did not successfully power on the virtual machine before giving up and waiting for the next possible accessibility change.
After each failed attempt, vSphere HA reports an event indicating that the failover was not successful, and after five failed attempts, reports that vSphere HA stopped trying to restart the virtual machine because the maximum number of failover attempts was reached. Even after reporting that the vSphere HA master agent has stopped trying, however, it does try the next time a possible accessibility change occurs.
Workaround: None.
Powering off a Virtual SAN host causes the Storage Providers view in the vSphere Web Client to refresh longer than expected
If you power off a Virtual San host, the Storage Providers view might appear empty. The Refresh button continues to spin even though no information is shown.
Workaround: Wait at least 15 minutes for the Storage Providers view to be populated again. The view also refreshes after you power on the host.
Virtual SAN reports a failed task as completed
Virtual SAN might report certain tasks as completed even though they failed internally.
The following are conditions and corresponding reasons for errors:
Condition: Users attempt to create a new disk group or add a new disk to already existing disk group when the Virtual SAN license has expired.
Error stack: A general system error occurred: Cannot add disk: VSAN is not licensed on this host.
Condition: Users attempt to create a disk group with the number of disk higher than the supported number. Or they try to add new disks to already existing disk group so that the total number exceeds the supported number of disks per disk group.
Error stack: A general system error occurred: Too many disks.
Condition: Users attempt to add a disk to the disk group that has errors.
Error stack: A general system error occurred: Unable to create partition table.
Workaround: After identifying the reason for a failure, correct the reason and perform the task again.
Virtual SAN datastores cannot store host local and system swap files
Typically, you can place the system swap or host local swap file on a datastore. However, the Virtual SAN datastore does not support system swap and host local swap files. As a result, the UI option that allows you to select the Virtual SAN datastore as the file location for system swap or host local swap is not available.
Workaround: In Virtual SAN environment, use other supported options to place the system swap and host local swap files.
A Virtual SAN virtual machine in a vSphere HA cluster is reported as vSphere HA protected although it has been powered off
This might happen when you power off a virtual machine with its home object residing on a Virtual SAN datastore, and the home object is not accessible. This problem is seen if a HA master agent election occurs after the object becomes inaccessible.
Workaround:
Make sure that the home object is accessible again by checking the compliance of the object with the specified storage policy.
Power on the virtual machine then power it off again.
The status should change to unprotected.
Virtual machine object remains in Out of Date status even after Reapply action is triggered and completed successfully
If you edit an existing virtual machine profile due to the new storage requirements, the associated virtual machine objects, home or disk, might go in Out of Date status.This occurs when your current environment cannot support reconfiguration of virtual machine objects. Using Reapply action does not change the status.
Workaround: Add additional resources, hosts or disks, to the Virtual SAN cluster and invoke Reapply action again.
Automatic disk claiming for Virtual SAN does not work as expected if you license Virtual SAN after enabling it
If you enable Virtual SAN in automatic mode and then assign a license, Virtual SAN fails to claim disks.
Workaround: Change the mode to Manual, and then switch back to Automatic. Virtual SAN will properly claim the disks.
vSphere High Availability (HA) fails to restart a virtual machine when Virtual SAN network is partitioned
This occurs when Virtual SAN uses VMkernel adapters for internode communication, which are on the same subnet as other VMkernel adapters in a cluster. Such configuration could cause network failure and disrupt Virtual SAN internode communication, while vSphere HA internode communication remains unaffected.
In this situation, the HA master agent might detect the failure in a virtual machine, but is unable to restart it. For example, this could occur when the host on which the master agent is running does not have access to the virtual machine's objects.
Workaround: Make sure that the VMkernel adapters used by Virtual SAN do not share a subnet with the VMkernel adapters used for other purposes.
VM directories contain duplicate swap (.vswp) files   
This might occur if virtual machines running on Virtual SAN are not cleanly shutdown, and if you perform a fresh installation of ESXi and vCenter Server without erasing data from Virtual SAN disks. As a result, old swap files (.vswp) are found in the directories for virtual machines that are shut down uncleanly.

Workaround: None

VMs might become inaccessible due to high network latency
In a Virtual SAN cluster setup, if the network latency is high, some VMs might become inaccessible on vCenter Server and you will not be able to power on or access the VM.

Workaround: Run the vsan.check_state -e -r RVC command.
VM operations might timeout due to high network latency
When storage controller with low queue depths are used, high network latency might cause VM operations to time out. 

Workaround: Re-attempt the operations when the network load is lower.
VMs might get renamed to a truncated version of their vmx file path
If the vmx file of a virtual machines is temporarily inaccessible, the VM gets renamed to a truncated version of the vmx file path. For example, the virtual machine might get renamed to /vmfs/volumes/vsan:52f1686bdcb477cd-8e97188e35b99d2e/236d5552-ad93. The truncation might delete half the UUID of the VM home directory making it difficult to map the renamed VM with the original VM, from just the VM name.

Workaround: Run the vsan.fix_renamed_vms RVC command.
vCenter Server and vSphere Web Client
Unable to add ESXi host to Active Directory domain
You might observe that Active Directory domain name is not displayed in Domain drop-down list under Select Users and Groups option when you attempt to assign permissions. Also, the Authentication Services Settings option might not display any trusted domain controller even when the active directory has trusted domains.

Workaround:
Restart netlogond, lwiod, and then lsassd daemons.
Login to ESXi host using vSphere Client.
In the Configuration tab and click Authentication Services Settings.
Refresh to view the trusted domains.
Virtual Machine Management Issues
Unable to perform cold migration and storage vMotion of a virtual machine if the VMDK file name begins with "core"
Attempts to perform cold migration and storage vMotion of a virtual machine might fail if the VMDK file name begins with "core" with error message similar to the following:

A general system error occurred: Error naming or renaming a VM file.

Error messages similar to the following might be displayed in the vpxd.log file:

mem> 2014-01-01T11:08:33.150-08:00 [13512 info 'commonvpxLro' opID=8BA11741-0000095D-86-97] [VpxLRO] -- FINISH task-internal-2471 -- -- VmprovWorkflow --
mem> 2014-01-01T11:08:33.150-08:00 [13512 info 'Default' opID=8BA11741-0000095D-86-97] [VpxLRO] -- ERROR task-internal-2471 -- -- VmprovWorkflow: vmodl.fault.SystemError:
mem> --> Result:
mem> --> (vmodl.fault.SystemError){
mem> --> dynamicType = ,
mem> --> faultCause = (vmodl.MethodFault) null, 
mem> --> reason = "Error naming or renaming a VM file.", 
mem> --> msg = "", 
mem> --> }
This issue occurs when the ESXi host incorrectly classifies VMDK files with a name beginning with "core" as a core file instead of the expected disk type.

Workaround: Ensure that the VMDK file name of the virtual machine does not begin with "core". Also, use the vmkfstools utility to rename the VMDK file to ensure that the file name do not begin with the word "core".
Virtual machines with Windows 7 Enterprise 64-bit guest operating systems in the French locale experience problems during clone operations
If you have a cloned Windows 7 Enterprise 64-bit virtual machine that is running in the French locale, the virtual machine disconnects from the network and the customization specification is not applied. This issue appears when the virtual machine is running on an ESXi 5.1 host and you clone it to ESXi 5.5 and upgrade the VMware Tools version to the latest version available with the 5.5 host.
Workaround: Upgrade the virtual machine compatibility to ESXi 5.5 and later before you upgrade to the latest available version of VMware Tools.
Attempts to increase the size of a virtual disk on a running virtual machine fail with an error
If you increase the size of a virtual disk when the virtual machine is running, the operation might fail with the following error:
This operation is not supported for this device type.
The failure might occur if you are extending the disk to the size of 2TB or larger. The hot-extend operation supports increasing the disk size to only 2TB or less. SATA virtual disks do not support the hot-extend operation no matter what their size is.
Workaround: Power off the virtual machine to extend the virtual disk to 2TB or larger.
VMware HA and Fault Tolerance Issues
New Issue Fault Tolerance (FT) is not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform
Fault tolerance is not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform. Attempts to power on a virtual machine will fail after you enable single-processor Fault Tolerance.
Workaround: None
If you select an ESX/ESXi 4.0 or 4.1 host in a vSphere HA cluster to fail over a virtual machine, the virtual machine might not restart as expected 
When vSphere HA restarts a virtual machine on an ESX/ESXi 4.0 or 4.1 host that is different from the original host the virtual machine was running on, a query is issued that is not answered. The virtual machine is not powered on on the new host until you answer the query manually from the vSphere Client.
Workaround: Answer the query from the vSphere Client. Alternatively, you can wait for a timeout (15 minutes by default), and vSphere HA attempts to restart the virtual machine on a different host. If the host is running ESX/ESXi 5.0 or later, the virtual machine is restarted.
If a vMotion operation without shared storage fails in a vSphere HA cluster, the destination virtual machine might be registered to an unexpected host
A vMotion migration involving no shared storage might fail because the destination virtual machine does not receive a handshake message that coordinates the transfer of control between the two virtual machines. The vMotion protocol powers off both the source and destination virtual machines. If the source and destination hosts are in the same cluster and if vSphere HA has been enabled, the destination virtual machine might be registered by vSphere HA on another host than the one chosen as the target for the vMotion migration.
Workaround: If you want to retain the destination virtual machine and you want it to be registered to a specific host, relocate the destination virtual machine to the destination host. This relocation is best done before powering on the virtual machine.
Supported Hardware Issues
Sensor values for Fan, Power Supply, Voltage, and Current sensors appear under the Other group of the vCenter Server Hardware Status Tab
Some sensor values are listed in the Other group instead of the respective categorized group.
Workaround: None.
I/O memory management unit (IOMMU) faults might appear when the debug direct memory access (DMA) mapper is enabled
The debug mapper places devices in IOMMU domains to help catch device memory accesses to addresses that have not been explicitly mapped. On some HP systems with old firmware, IOMMU faults might appear.
Workaround: Download firmware upgrades from the HP Web site and apply them.
Upgrade the firmware of the HP iLO2 controller. 
Version 2.07, released in August 2011, resolves the problem.
Upgrade the firmware of the HP Smart Array.
For the HP Smart Array P410, version 5.14, released in January 2012, resolves the problem.
VMware Tools Issues
Unable to install VMware Tools on Linux guest operating systems by executing the vmware-install.pl -d command if VMware Tools is not installed earlier
If VMware Tools is not installed on your Linux guest operating system, attempts to perform a fresh installation of VMware Tools by executing the vmware-install.pl -d command might fail.
This issue occurs in the following guest operating systems:
RHEL 7 and later
CentOS 7 and later
Oracle Linux 7 and later
Fedora 19 and later
SLES 12 and later
SLED 12 and later
openSUSE 12.2 and later
Ubuntu 14.04 and later
Debian 7 and later

Workaround: There is no workaround that helps the -default (-d) switch work. However, you can install VMware Tools without the - default switch.
Select Yes when you are prompted with the option Do you still want to proceed with this legacy installer? by the installer. 

Note: This release introduces a new --force-install’(-f) switch to install VMware Tools.
File disappears after VMware Tools upgrade
deployPkg.dll file which is present in C:\Program Files\Vmware\Vmware Tools\ is not found after upgrading VMware Tools. This is observed when it is upgraded from version 5.1 Update 2 to 5.5 Update 1 or later and version 5.5 to 5.5 Update 1 or later. 

Workaround: None
User is forcefully logged out while installing or uninstalling VMware Tools by OSP
While installing or uninstalling VMware Tools packages in a RHEL (Red Hat Linux Enterprise) and CentOS virtual machines that were installed using operating system specific packages (OSP), the current user is forcefully logged out. This issue occurs in RHEL 6.5 64-bit, RHEL 6.5 32-bit, CentOS 6.5 64-bit and CentOS 6.5 32-bit virtual machines.

Workaround:
Use secure shell (SSH) to install or uninstall VMware Tools 
or
The user must log in again to install or uninstall the VMware Tools packages
Miscellaneous Issues
SRM test recovery operation might fail with an error
Attempts to perform Site Recovery Manager (SRM) test recovery might fail with error message similar to the following:
'Error - A general system error occurred: VM not found'.
When several test recovery operations are performed simultaneously, the probability of encountering the error messages increases.

Workaround: None. However, this is not a persistent issue and this issue might not occur if you perform the SRM test recovery operation again.
Copyright ? 2015 VMware, Inc. All rights reserved.
Terms of Use
Privacy
Accessibility
Trademarks
>

VMware ESXi 5.5 Update 3a Release Notes

VMware ESXi? 5.5 Update 3a | 6 OCT 2015 | Build 3116895
Last updated: 6 OCT 2015
Check for additions and updates to these release notes.
What's in the Release Notes

The release notes cover the following topics:
What's New
Earlier Releases of ESXi 5.5
Internationalization
Compatibility and Installation
Upgrades for This Release
Open Source Components for VMware vSphere
Product Support Notices
Patches Contained in this Release
Resolved Issues
Known Issues
What's New

This release of VMware ESXi contains the following enhancements:
Log Rotation Enablement – Log rotation for vmx files allows you to reduce the log file sizes by specifying the size of each log and the number of previous logs to keep.


Certification of PVSCSI Adapter – PVSCSI adapter is certified for use with MSCS, core clustering and applications including SQL and Exchange. This creates performance gains when moving from LSI Logic SAS to PVSCSI.


Support for Next Generation Processors – In this release, we will continue our support for next generation processors from Intel and AMD. Please see the VMware Compatibility Guide for more info.


ESXi Authentication for Active Directory – ESXi is modified to only support AES256-CTS/AES128-CTS/RC4-HMAC encryption for Kerberos communication between ESXi and Active Directory.


Resolved Issues – This release delivers a number of bug fixes that have been documented in the Resolved Issues section.
Earlier Releases of ESXi 5.5

Features and known issues of ESXi 5.5 are described in the release notes for each release. Release notes for earlier releases of ESXi 5.5, are:
VMware ESXi 5.5 Update 2 Release Notes
VMware ESXi 5.5 Update 1 Release Notes
VMware vSphere 5.5 Release Notes
Internationalization

VMware vSphere 5.5 Update 3a is available in the following languages:
English
French
German
Japanese
Korean
Simplified Chinese
Traditional Chinese
Compatibility and Installation

ESXi, vCenter Server, and vSphere Web Client Version Compatibility

The VMware Product Interoperability Matrix provides details about the compatibility of current and earlier versions of VMware vSphere components, including ESXi, VMware vCenter Server, the vSphere Web Client, and optional VMware products. Check the VMware Product Interoperability Matrix also for information about supported management and backup agents before you install ESXi or vCenter Server.
The vSphere Client and the vSphere Web Client are packaged on the vCenter Server ISO. You can install one or both clients by using the VMware vCenter? Installer wizard.
ESXi, vCenter Server, and VDDK Compatibility

Virtual Disk Development Kit (VDDK) 5.5.3 adds support for ESXi 5.5 Update 3 and vCenter Server 5.5 Update 3 releases.
For more information about VDDK, see http://www.vmware.com/support/developer/vddk/.
ESXi and Virtual SAN Compatibility

Virtual SAN does not support clusters that are configured with ESXi hosts earlier than 5.5 Update 1. Make sure all hosts in the Virtual SAN cluster are upgraded to ESXi 5.5 Update 1 or later, before enabling Virtual SAN. vCenter Server should also be upgraded to 5.5 Update 1 or later.
Hardware Compatibility for ESXi

To view a list of processors, storage devices, SAN arrays, and I/O devices that are compatible with vSphere 5.5 Update 3, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Device Compatibility for ESXi

To determine which devices are compatible with ESXi 5.5 Update 3a, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Some devices are deprecated and no longer supported on ESXi 5.5 and later. During the upgrade process, the device driver is installed on the ESXi 5.5.x host. It might still function on ESXi 5.5.x, but the device is not supported on ESXi 5.5.x. For a list of devices that have been deprecated and are no longer supported on ESXi 5.5.x, see the VMware Knowledge Base article Deprecated devices and warnings during ESXi 5.5 upgrade process.
Guest Operating System Compatibility for ESXi

To determine which guest operating systems are compatible with vSphere 5.5 Update 3a, use the ESXi 5.5 Update 3 information in the VMware Compatibility Guide.
Virtual Machine Compatibility for ESXi

Virtual machines that are compatible with ESX 3.x and later (hardware version 4) are supported with ESXi 5.5 Update 3. Virtual machines that are compatible with ESX 2.x and later (hardware version 3) are not supported. To use such virtual machines on ESXi 5.5 Update 3, upgrade the virtual machine compatibility. See thevSphere Upgrade documentation.
vSphere Client Connections to Linked Mode Environments with vCenter Server 5.x

vCenter Server 5.5 can exist in Linked Mode only with other instances of vCenter Server 5.5.
Installation Notes for This Release

Read the vSphere Installation and Setup documentation for guidance about installing and configuring ESXi and vCenter Server.
Although the installations are straightforward, several subsequent configuration steps are essential. Read the following documentation:
Licensing in the vCenter Server and Host Management documentation
Networking in the vSphere Networking documentation
Security in the vSphere Security documentation for information on firewall ports
Migrating Third-Party Solutions

You cannot directly migrate third-party solutions installed on an ESX or ESXi host as part of a host upgrade. Architectural changes between ESXi 5.1 and ESXi 5.5 result in the loss of third-party components and possible system instability. To accomplish such migrations, you can create a custom ISO file with Image Builder. For information about upgrading your host with third-party customizations, see the vSphere Upgrade documentation. For information about using Image Builder to make a custom ISO, see the vSphere Installation and Setup documentation.
Upgrades and Installations Disallowed for Unsupported CPUs

vSphere 5.5.x supports only CPUs with LAHF and SAHF CPU instruction sets. During an installation or upgrade, the installer checks the compatibility of the host CPU with vSphere 5.5.x. If your host hardware is not compatible, a purple screen appears with a message about incompatibility. You cannot install or upgrade to vSphere 5.5.x.
Upgrades for This Release

For instructions about upgrading vCenter Server and ESX/ESXi hosts, see the vSphere Upgrade documentation.
Supported Upgrade Paths for Upgrade to ESXi 5.5 Update 3a:
Upgrade Deliverables
Supported Upgrade Tools
Supported Upgrade Paths to ESXi 5.5 Update 3a
ESX/ESXi 4.0:
Includes
ESX/ESXi 4.0 Update 1
ESX/ESXi 4.0 Update 2
ESX/ESXi 4.0 Update 3
ESX/ESXi 4.0 Update 4
ESX/ESXi 4.1:
Includes
ESX/ESXi 4.1 Update 1
ESX/ESXi 4.1 Update 2
ESX/ESXi 4.1 Update 3
ESXi 5.0:
Includes
ESXi 5.0 Update 1 
ESXi 5.0 Update 2 
ESXi 5.0 Update 3 
ESXi 5.1
Includes
ESXi 5.1 Update 1
ESXi 5.1 Update 2 
ESXi 5.5
Includes
ESXi 5.5 Update 1
ESXi 5.5 Update 2

VMware-VMvisor-Installer-5.5.0.update03-3116895.x86_64.iso

VMware vCenter Update Manager
CD Upgrade
Scripted Upgrade
Yes
Yes
Yes
Yes
Yes
ESXi550-201510001.zip    
VMware vCenter Update Manager
ESXCLI
VMware vSphere CLI
No
No
Yes*
Yes*
Yes
Using patch definitions downloaded from VMware portal (online)    VMware vCenter Update Manager with patch baseline    
No
No
No
No
Yes

*Note: Upgrade from ESXi 5.0.x, or ESXi 5.1.x, to ESXi 5.5 Update 3a using -    ESXi550-201510001.zip is supported only with ESXCLI. You need to run the esxcli software profile update --depot=<depot_location> --profile=<profile_name> command to perform the upgrade. For more information, see the ESXi 5.5.x Upgrade Options topic in the vSphere Upgrade guide.
Open Source Components for VMware vSphere 5.5 Update 3

The copyright statements and licenses applicable to the open source software components distributed in vSphere 5.5 Update 3 are available athttp://www.vmware.com/download/vsphere/open_source.html, on the Open Source tab. You can also download the source files for any GPL, LGPL, or other similar licenses that require the source code or modifications to source code to be made available for the most recent available release of vSphere.
Product Support Notices

vSphere Web Client. Because Linux platforms are no longer supported by Adobe Flash, vSphere Web Client is not supported on the Linux OS. Third party browsers that add support for Adobe Flash on the Linux desktop OS might continue to function.
VMware vCenter Server Appliance. In vSphere 5.5, the VMware vCenter Server Appliance meets high-governance compliance standards through the enforcement of the DISA Security Technical Information Guidelines (STIG). Before you deploy VMware vCenter Server Appliance, see the VMware Hardened Virtual Appliance Operations Guide for information about the new security deployment standards and to ensure successful operations.
vCenter Server database. vSphere 5.5 removes support for IBM DB2 as the vCenter Server database.
VMware Tools. Beginning with vSphere 5.5, all information about how to install and configure VMware Tools in vSphere is merged with the other vSphere documentation. For information about using VMware Tools in vSphere, see the vSphere documentation. Installing and Configuring VMware Tools is not relevant to vSphere 5.5 and later.
VMware Tools. Beginning with vSphere 5.5, VMware Tools do not provide ThinPrint features.
vSphere Data Protection. vSphere Data Protection 5.1 is not compatible with vSphere 5.5 because of a change in the way vSphere Web Client operates. vSphere Data Protection 5.1 users who upgrade to vSphere 5.5 must also update vSphere Data Protection to continue using vSphere Data Protection.
Patches Contained in this Release

This release contains all bulletins for ESXi that were released earlier to the release date of this product. See the VMware Download Patches page for more information about the individual bulletins.
Patch Release ESXi550-Update03 contains the following individual bulletins:
ESXi550-201510401-BG: Updates ESXi 5.5 esx-base vib
Patch Release ESXi550-Update03 contains the following image profiles:
ESXi-5.5.0-20151004001-standard
ESXi-5.5.0-20151004001-no-tools
For information on patch and update classification, see KB 2014447.
Resolved Issues

This section describes resolved issues in this release:
Backup
CIM and API Issues
Miscellaneous Issues
Networking Issues
Security Issues
Server Configuration Issues
Supported Hardware Issues
Storage Issues
Upgrade and Installation Issues
Virtual SAN Issues
vCenter Server and vSphere Web Client Issues
Virtual Machine Management Issues
VMware HA and Fault Tolerance
vMotion and Storage vMotion Issues
VMware Tools Issues
Backup Issues

 Attempts to restore a virtual machine might fail with an error
Attempts to restore a virtual machine on an ESXi host using vSphere Data Protection might fail and display an error message similar to the following:

Unexpected exception received during reconfigure

This issue is resolved in this release.
CIM and API Issues

 Spew in syslog when system event log is full and indication subscriptions exist
Spew in syslog when System Event Log (SEL) is full and indication subscriptions exist. The following logs are logged in rapidly: 

sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default
sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default
sfcb-vmware_raw[xxxxxxxxxx]: Can't get Alert Indication Class. Use default

This issue is resolved in this release.
 CIM indications might fail when you use Auto Deploy to reboot the ESXi hosts
If the sfcbd service stops running, the CIM indications in host profile cannot be applied successfully.

This issue is resolved in this release by ensuring that the CIM indications do not rely on the status of the sfcbd service while applying the host profile.
 Status of some disks might be displayed as UNCONFIGURED GOOD instead of ONLINE
Status of some disks on an ESXi 5.5 host might be displayed as UNCONFIGURED GOOD instead of ONLINE. This issue occurs for LSI controller using the LSI CIM provider.

This issue is resolved in this release.
 Load kernel module might fail through CIM interface
The LoadModule command might fail when using the CIM interface client to load the kernel module. An error message similar to the following is displayed:

Access denied by VMkernel access control policy.

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to an openwsmand error. An error message similar to the following might be reported in thesyslog.log file:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release.
 Querying hardware status on the vSphere Client might fail with an error
Attempts to query the hardware status on the vSphere Client might fail. An error message similar to the following is displayed in the /var/log/syslog.log file in the ESXi host:

TIMEOUT DOING SHARED SOCKET RECV RESULT (1138472) Timeout (or other socket error) waiting for response from provider Header Id (16040) Request to provider 111 in process 4 failed. Error:Timeout (or other socket error) waiting for response from provider Dropped response operation details -- nameSpace: root/cimv2, className: OMC_RawIpmiSensor, Type: 0

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to an openwsmand error. An error message similar to the following might be reported in thesyslog.log file:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release.
 The sfcbd service might stop responding with an error message
The sfcbd service might stop responding and you might find the following error message in the syslog file:

spSendReq/spSendMsg failed to send on 7 (-1)
Error getting provider context from provider manager: 11

This issue occurs when there is a contention for semaphore between the CIM server and the providers.

This issue is resolved in this release.
 False alarms appear in the Hardware Status tab of the vSphere Client
After you upgrade Integrated Lights Out (iLO) firmware on HP DL980 G7, false alarms appear in the Hardware Status tab of the vSphere Client. Error messages similar to the following might be logged in the /var/log/syslog.log file:

sfcb-vmware_raw[nnnnn]: IpmiIfruInfoAreaLength: Reading FRU for 0x0 at 0x8 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfcFruChassis: Reading FRU Chassis Info Area length for 0x0 FAILED
sfcb-vmware_raw[nnnnn]: IpmiIfcFruBoard: Reading FRU Board Info details for 0x0 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfruInfoAreaLength: Reading FRU for 0x0 at 0x70 FAILED cc=0xffffffff
sfcb-vmware_raw[nnnnn]: IpmiIfcFruProduct: Reading FRU product Info Area length for 0x0 FAILED
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: data length mismatch req=19,resp=3
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0001,resp=0002
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0002,resp=0003
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0003,resp=0004
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0004,resp=0005
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0005,resp=0006
sfcb-vmware_raw[nnnnn]: IpmiIfcSelReadEntry: EntryId mismatch req=0006,resp=0007

This issue is resolved in this release.
 ESXi might send duplicate events to management software
ESXi might send duplicate events to the management software when an Intelligent Platform Management Interface (IPMI) sensor event is triggered on the ESXi Host.

This issue is resolved in this release.
 Unable to monitor Hardware Status after removing CIM indication subscription
If the CIM client sends two requests of Delete Instance to the same CIM indication subscription, the sfcb-vmware_int might stop responding due to memory contention. You might not be able to monitor the Hardware Status with the vCenter Server and ESXi.

This issue is resolved in this release.
 Monitoring an ESXi 5.5 host with Dell OpenManage might fail due to openwsmand error
Monitoring an ESXi 5.5 host with Dell OpenManage might fail respond due to openwsmand error. An error message similar to the following might be reported:

Failed to map segment from shared object: No space left on device

This issue is resolved in this release. 
 CIM client might display an error due to multiple enumeration
When you execute multiple enumerate queries on VMware Ethernet port class using the CBEnumInstances method, servers running on an ESXi 5.5 might notice an error message similar to the following:

CIM error: enumInstances Class not found

This issue occurs when the management software fails to retrieve information provided by VMware_EthernetPort()class. When the issue occurs, query on memstats might display the following error message:

MemStatsTraverseGroups: VSI_GetInstanceListAlloc failure: Not found.

This issue is resolve in this release.
 Unable to monitor Hardware Status on an ESXi host
An ESXi host might report an error in the Hardware Status tab due to the unresponsive hardware monitoring service (sfcbd). An error similar to the following is written to syslog.log:

sfcb-hhrc[5149608]: spGetMsg receiving from 65 5149608-11 Resource temporarily unavailable
sfcb-hhrc[5149608]: rcvMsg receiving from 65 5149608-11 Resource temporarily unavailable
sfcb-hhrc[5149608]: Timeout or other socket error
sfcb-LSIESG_SMIS13_HHR[6064161]: spGetMsg receiving from 51 6064161-11 Resource temporarily unavailable
sfcb-LSIESG_SMIS13_HHR[6064161]: rcvMsg receiving from 51 6064161-11 Resource temporarily unavailable
sfcb-LSIESG_SMIS13_HHR[6064161]: Timeout or other socket error
sfcb-kmoduleprovider[6064189]: spGetMsg receiving from 57 6064189-11 Resource temporarily unavailable
sfcb-kmoduleprovider[6064189]: rcvMsg receiving from 57 6064189-11 Resource temporarily unavailable
sfcb-kmoduleprovider[6064189]: Timeout or other socket error

The syslog below in debug level indicates that an invalid data of 0x3c is sent by IPMI when the expected data is 0x01.

sfcb-vmware_raw[35704]: IpmiIfcRhFruInv: fru.header.version: 0x3c

This issue occurs when sfcb-vmware_raw provider receives invalid data from the Intelligent Platform Management Interface (IPMI) tool while reading the Field Replaceable Unit (FRU) inventory data.

This issue is resolved in this release.
Miscellaneous Issues

 Cloning CBT-enabled virtual machine templates from ESXi hosts might fail 
Attempt to clone CBT-enabled virtual machines templates simultaneously from two different ESXi 5.5 hosts might fail. An error message similar to the following is displayed:

Failed to open VM_template.vmdk': Could not open/create change tracking file (2108).

This issue is resolved in this release.
 Unable to log into ESXi host with Active Directory credentials 
Attempts to login to an ESXi host might fail after the host successfully joins an Active Directory. This occurs when a user from one domain attempts to join another trusted domain, which is not present in the ESXi client site. An error similar to the following is written to sys.log/netlogon.log file:

netlogond[17229]: [LWNetDnsQueryWithBuffer() /build/mts/release/bora-1028347/likewise/esxi-esxi/src/linux/netlogon/utils/lwnet-dns.c:1185] DNS lookup for '_ldap._tcp.<domain details>' failed with errno 0, h_errno = 1

This issue is resolved in this release.
 Update to the cURL library
cURL fails to resolve localhost when IPv6 is disabled. An error message similar to the following is displayed:

error: enumInstances Failed initialization

This issue is resolved in this release.
Networking Issues

 ESXi hosts with the virtual machines having e1000 or e1000e vNIC driver might fail with a purple screen 
ESXi hosts with the virtual machines having e1000 or e1000e vNIC driver might fail with a purple screen when you enable TCP segmentation Offload (TSO). Error messages similar to the following might be written to the log files:

cpu7:nnnnnn)Code start: 0xnnnnnnnnnnnn VMK uptime: 9:21:12:17.991
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000TxTSOSend@vmkernel#nover+0x65b stack: 0xnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000PollTxRing@vmkernel#nover+0x18ab stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]E1000DevAsyncTx@vmkernel#nover+0xa2 stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetWorldletPerVMCB@vmkernel#nover+0xae stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]WorldletProcessQueue@vmkernel#nover+0x488 stack: 0xnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]WorldletBHHandler@vmkernel#nover+0x60 stack: 0xnnnnnnnnnnnnnnn
cpu7:nnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]BH_Check@vmkernel#nover+0x185 stack: 0xnnnnnnnnnnnn

This issue is resolved in this release.
 ESXi responds to unnecessary Internet Control Message Protocol request types
The ESXi host might respond to unnecessary Internet Control Message Protocol (ICMP) request types.

This issue is resolved in this release.
 ESXi hostd might fail when performing storage device rescan operations
When you perform storage device rescan operations, the hostd might fail as multiple threads attempt to modify the same object. You might see error messages similar to the following in the vmkwarning.log file: 

cpu43:nnnnnnn)ALERT: hostd detected to be non-responsive
cpu20:nnnnnnn)ALERT: hostd detected to be non-responsive

This issue is resolved in this release.
 Log spew is observed when ESXi host is added in vCenter server
When you add an ESXi host in vCenter server and create a VMkernel interface for vMotion, you will see the following message displayed in quick succession (log spew) in the hostd.log file:

Failed to find vds Id for portset vSwitch0

This issue is resolved in this release.
 Microsoft Windows Deployment Services (WDS) might fail to PXE boot virtual machines that use the VMXNET3 network adapter
Attempts to PXE boot virtual machines that use the VMXNET3 network adapter by using the Microsoft Windows Deployment Services (WDS) might fail with messages similar to the following:

Windows failed to start. A recent hardware or software change might be the cause. To fix the problem:
1. Insert your Windows installation disc and restart your computer.
2. Choose your language setting, and then click Next.
3. Click Repair your computer.
If you do not have the disc, contact your system administrator or computer manufacturer for assistance.

Status: 0xc0000001

Info: The boot selection failed because a required device is inaccessible.

This issue is resolved in this release.
 Enable the configuration of Rx Ring#2 to solve the Rx Ring#2 out of memory and packet drops on the receiver side issues
A Linux virtual machine enabled with Large Receive Offload (LRO) functionality on VMXNET3 device might experience packet drops on the receiver side when the Rx Ring #2 runs out of memory, since the size of Rx Ring#2 is unable to be configured originally.

This issue is resolved in this release.
 Purple diagnostic screen might be displayed when using DvFilter with a NetQueue supported uplink
An ESXi server might experience a purple diagnostic screen when using DvFilter with a NetQueue supported uplink connected to a vSwitch or a vSphere Distributed Switch (VDS). The ESXi host might report a backtrace similar to the following:

pcpu:22 world:4118 name:"idle22" (IS)
pcpu:23 world:2592367 name:"vmm1:S10274-AAG" (V)
@BlueScreen: Spin count exceeded (^P) - possible deadlock
Code start: 0xnnnnnnnnnnnn VMK uptime: 57:09:18:15.770
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]Panic@vmkernel#nover+0xnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]SP_WaitLock@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetSchedFIFOInput@vmkernel#nover+0xnnn stack: 0x0
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]NetSchedInput@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]IOChain_Resume@vmkernel#nover+0xnnn stack: 0xnnnnnnnnnnnn
0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]PortOutput@vmkernel#nover+0xnn stack: 0xnn

This issue is resolved in this release.
 ESXi host fails with purple diagnostic screen when the Netflow feature is deactivated
An ESXi host might fail with a PF exception 14 purple diagnostic screen when the Netflow feature of vSphere Distributed Switch gets deactivated. The issue occurs due to a timer synchronization problem.

This issue is resolved in this release.
 Changing the network scheduler to SFQ during heavy I/O might result in an unrecoverable transmission
When heavy I/O load is in progress, the SFQ Network scheduler might reset the physical NIC when switching the network schedulers. This might cause an unrecoverable transmission where no packets are transmitted to the driver.

This issue is resolved in this release.
 vmkping command with Jumbo Frames might fail
The vmkping command with Jumbo Frames might fail after one vmknic MTU is changed amongst many in the same switch. An error message similar to the following is displayed:

sendto() failed (Message too long)

This issue is resolved in this release.
 ESXi firewall might reject the services that use port 0-65535 as service port
The Virtual Serial Port Concentrator (vSPC) or NFS client service might not function on the ESXi platform. This happens when there is a different ruleset order, which allows port 0-65535, as a result of enabling sequence. This results in the vSPC or NFS Client related packets to be dropped unexpectedly even if the allowed IP on corresponding ruleset is specified.

This issue is resolved in this release.
 IPv6 RA does not function as expected when tagging 802.1q with VMXNET3 adapters
IPv6 Router Advertisements (RA) does not function as expected when tagging 802.1q with VMXNET3 adapters in an Linux virtual machine as the IPv6 RA address intended for the VLAN interface is delivered to the base interface.

This issue is resolved in this release.
 ESXi host might lose network connectivity
An ESXi host might lose network connectivity and experience stability issues when multiple error messages similar to the following are logged in:

WARNING: Heartbeat: 785: PCPU 63 didn't have a heartbeat for 7 seconds; *may* be locked up.

This issue is resolved in this release.
 Network connectivity lost when applying host profile during Auto Deploy
When applying host profile during Auto Deploy, you might lose network connectivity because the VXLAN Tunnel Endpoint (VTEP) NIC gets tagged as management vmknic.

This issue is resolved in this release.

Security Issues

 Update to the libxml2 library
The ESXi userworld libxml2 library is updated to verion 2.9.2.
 Update to the ESXi userworld OpenSSL library
The ESXi userworld OpenSSL library is updated to version 1.0.1m.
 Update to the libPNG library
The libPNG library is updated to libpng-1.6.16.
Server Configuration Issues

 Serial over LAN Console Redirection might not function properly
A PCIe serial port redirection card might not function properly when connected to an Industry Standard Architecture (ISA) Interrupt Request (IRQ) (0-15 decimals) on an Advanced Programmable Interrupt Controller (APIC) as it is unable have its interrupts received by the CPU. To allow these and other PCI devices connected to ISA IRQs to function, VMkernel will now allow level-triggered interrupts on ISA IRQs.

This issue is resolved in this release. 
 Esxtop might incorrectly display the CPU utilization at 100%
The PCPU UTIL/CORE UTIL in esxtop utility incorrectly displays CPU utilization at 100% if you have the PcpuMigrateIdlePcpus set at 0.

This issue is resolved in this release. 
 Unknown(1) status reported when querying Fiber Channel Host Bus Adapters
After you upgrade the ESXi host from ESXi 5.1 to 5.5 and import the latest MIB module, the third-party monitoring software returns "unknown(1)" status when querying Fiber Channel (FC) Host Bus Adapters (HBA).

This issue is resolved in this release. 
 Host gateway deleted and compliance failures might occur when existing ESXi host profile re-applied to stateful ESXi host
When an existing ESXi host profile is applied to a newly installed ESXi 5.5 host, the profile compliance status might show as noncompliant. This happens when the host profile is created from hosts with VXLAN interface configured, the test for compliance on hosts with the previously created host profile might fail. An error message similar to the following is displayed:

IP route configuration doesn't match the specification

This issue is resolved in this release. 
 Purple diagnostic screen with Page Fault exception displayed in a nested ESXi environment 
In a nested ESXi environment, implementation of CpuSchedAfterSwitch() results in a race condition in the scheduler code and a purple diagnostic screen with Page Fault exception is displayed.

This issue is resolved in this release. 
 iSCSI initiator name allowed when enabling software iSCSI using esxcli 
You can now specify an iSCSI initiator name to the esxcli iscsi software set command.
 Virtual machine might not display a warning message when the CPU is not fully reserved
When you create a virtual machine with sched.cpu.latencySensitivity set to high and power it on, the exclusive affinity for the vCPUs might not get enabled if the VM does not have a full CPU reservation.

In earlier releases, the VM did not display a warning message when the CPU is not fully reserved. For more information, see Knowledge Base article 2087525.

This issue is resolved in this release.
 SNMPD might start automatically after ESXi host upgrade
The SNMPD might start automatically after you upgrade the ESXi host to 5.5 Update 2.

This issue is resolved in this release. 
 Host profiles become non-compliant with simple change to SNMP syscontact or syslocation
Host Profiles become non-compliant with a simple change to SNMP syscontact or syslocation. The issue occurs as the SNMP host profile plugin applies only a single value to all hosts attached to the host profile. An error message similar to the following might be displayed:

SNMP Agent Configuration differs

This issue is resolved in this release by enabling per-host value settings for certain parameters like syslocation, syscontact, v3targets,v3users and engineid.
 Attempts to create a FIFO and write data on it might result in a purple diagnostic screen
When you create a FIFO and attempt to write data to the /tmp/dpafifo, a purple diagnostic screen might displayed under certain conditions.

This issue is resolved in this release.
 Attempts to reboot Windows 8 and Windows 2012 server on ESXi host virtual machines might fail
After you reboot, the Windows 8 and Windows 2012 Server virtual machines might become unresponsive when the Microsoft Windows boot splash screen appears. For more information refer, Knowledge Base article 2092807.

This issue is resolved in this release.
 Attempts to reboot Windows 8 and Windows 2012 server on ESXi host virtual machines might fail
When you set the CPU limit of a uni-processor virtual machine, the overall ESXi utilization might decrease due to a defect in the ESXi scheduler. This happens when the ESXi scheduler considers the cpu-limited VMs as runnable (when they were not running) while making cpu-load estimations. Hence leading to incorrect load balancing decision. 

For details, see Knowledge Base article 2096897.

This issue is resolved in this release.
Supported Hardware Issues

 Power usage and power cap value missing in esxtop command
On Lenovo systems, the value of power usage and power cap is not available in the esxtop command.

This issue is resolved in this release.
Storage Issues

 During an High Availability failover or a host crash, the .vswp files of powered ON VMs on that host might be left behind on the storage
During a High Availability failover or host crash, the .vswp files of powered ON virtual machines on that host might be left behind on the storage. When many such failovers or crashes occur, the storage capacity might become full.

This issue is resolved in this release.
 False PE change message might be displayed in the VMkernel log file when you rescan a VMFS datastore with multiple extents
When you rescan a VMFS datastore with multiple extents, the following log message might be written in the VMkernel log even without any issues from storage connectivity:

Number of PEs for volume changed from 3 to 1. A VMFS volume rescan may be needed to use this volume.

This issue is resolved in this release.
 During transient error conditions, I/O to a device might repeatedly fail and not failover to an alternate working path
During transient error conditions like BUS BUSY, QFULL, HOST ABORTS, HOST RETRY and so on, you might repeatedly attempt commands on current path and do not failover to another path even after a reasonable amount of time.

This issue is resolved in this release. During occurrence of such transient errors, if the path is busy after a couple of retries, the path state is now changed to DEAD. As a result, a failover is triggered and an alternate working path to the device is used to send I/Os.
 During an High Availability failover or a host crash, the .vswp files of powered ON VMs on that host might be left behind on the storage
During a High Availability failover or host crash, the .vswp files of powered ON virtual machines on that host might be left behind on the storage. When many such failovers or crashes occur, the storage capacity might become full.

This issue is resolved in this release.
 Attempts to get block map of an offline storage might cause the hostd service to crash
The hostd service might fail on an ESXi 5.x host when there is an acquireLeaseExt API execution attempt on a snapshot disk which goes offline. This snapshot disk may be on an extent which has gone offline. The API caller may be a third-party backup solution. An error message similar to the following is displayed invmkernel.log:

cpu4:4739)LVM: 11729: Some trailing extents missing (498, 696).

This issue is resolved in this release.
 ESXi 5.5 host might stop responding with a purple diagnostic screen during collection of vm-support log bundle
When any inbox or third-party drivers do not have their SCSI transport-specific interfaces defined, the ESXi host might stop responding and display a purple diagnostic screen. The issue occurs during collection of vm-support log bundles or when you run I/O Device Management (IODM) Command-Line Interfaces (CLI) such as:

esxcli storage san sas list

esxcli storage san sas stats get


This issue is resolved in this release.
 Attempts to expand VMFS volumes beyond 16 TB might not succeed in certain scenarios
An ESXi host might fail when you attempt to expand a VMFS5 datastore beyond 16 TB. Error messages similar to the following is written to the vmkernel.log file:

cpu38:34276)LVM: 2907: [naa.600000e00d280000002800c000010000:1] Device expanded (actual size 61160331231 blocks, stored size 30580164575 blocks)
cpu38:34276)LVM: 2907: [naa.600000e00d280000002800c000010000:1] Device expanded (actual size 61160331231 blocks, stored size 30580164575 blocks)
cpu47:34276)LVM: 11172: LVM device naa.600000e00d280000002800c000010000:1 successfully expanded (new size: 31314089590272)
cpu47:34276)Vol3: 661: Unable to register file system ds02 for APD timeout notifications: Already exists
cpu47:34276)LVM: 7877: Using all available space (15657303277568).
cpu7:34276)LVM: 7785: Error adding space (0) on device naa.600000e00d280000002800c000010000:1 to volume 52f05483-52ea4568-ce0e-901b0e0cd0f0: No space left on device
cpu7:34276)LVM: 5424: PE grafting failed for dev naa.600000e00d280000002800c000010000:1 (opened: t), vol 52f05483-52ea4568-ce0e-901b0e0cd0f0: Limit exceeded
cpu7:34276)LVM: 7133: Device scan failed for <naa.600000e00d280000002800c000010000:1>: Limit exceeded
cpu7:34276)LVM: 7805: LVMProbeDevice failed for device naa.600000e00d280000002800c000010000:1: Limit exceeded
cpu32:38063)<3>ata1.00: bad CDB len=16, scsi_op=0x9e, max=12
cpu30:38063)LVM: 5424: PE grafting failed for dev naa.600000e00d280000002800c000010000:1 (opened: t), vol 52f05483-52ea4568-ce0e-901b0e0cd0f0: Limit exceeded
cpu30:38063)LVM: 7133: Device scan failed for <naa.600000e00d280000002800c000010000:1>: Limit exceeded

This issue is resolved in this release.
 ESXi host might fail with a purple diagnostic screen when multiple vSCSI filters are attached to a VM disk
An ESXi 5.5 host might fail with a purple diagnostic screen similar to the following when multiple vSCSI filters are attached to a VM disk. 

cpu24:103492 opID=nnnnnnnn)@BlueScreen: #PF Exception 14 in world 103492:hostd-worker IP 0xnnnnnnnnnnnn addr 0x30
PTEs:0xnnnnnnnnnn;0xnnnnnnnnnn;0x0;
cpu24:103492 opID=nnnnnnnn)Code start: 0xnnnnnnnnnnnn VMK uptime: 21:06:32:38.296
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSCSIFilter_GetFilterPrivateData@vmkernel#nover+0x1 stack: 0x4136c7d
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSCSIFilter_IssueInternalCommand@vmkernel#nover+0xc3 stack: 0x410961
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_FileSyncRead@<None>#<None>+0xb1 stack: 0x0
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_DigestRecompute@<None>#<None>+0x291 stack: 0x1391
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]CBRC_FilterDigestRecompute@<None>#<None>+0x36 stack: 0x20
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]VSI_SetInfo@vmkernel#nover+0x322 stack: 0x411424b18120
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]UWVMKSyscallUnpackVSI_Set@<None>#<None>+0xef stack: 0x41245111df10
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]User_UWVMKSyscallHandler@<None>#<None>+0x243 stack: 0x41245111df20
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]User_UWVMKSyscallHandler@vmkernel#nover+0x1d stack: 0x275c3918
cpu24:103492 opID=nnnnnnnn)0xnnnnnnnnnnnn:[0xnnnnnnnnnnnn]gate_entry@vmkernel#nover+0x64 stack: 0x0 

This issue is resolved in this release.
 ESXi host stops responding and loses connection to vCenter Server during storage hiccups on Non-ATS VMFS datastores
An ESXi host might stop responding and the virtual machines become inaccessible. Also, the ESXi host might lose connection to vCenter Server due to a deadlock during storage hicuups on Non-ATS VMFS datastores.

This issue is resolved in this release.
 ESXi host gets registered with an incorrect IQN on a target management software 
Unisphere Storage Management software registers the given initiator IQN when software iSCSI is first enabled. During stateless boot, the registered IQN does not change with the name defined in host profile. You are required to manually remove the initiators from the array and add them again under the new IQN.

This issue is resolved by adding a new parameter to the software iSCSI enable command so that Unisphere registers the initiator under the name defined in the host profile. The command line to set the IQN during software iSCSI enablement is: 

esxcli iscsi software set --enabled=true --name iqn.xyz
 vSphere Replication sync might fail due to change in source datastore name
If you rename a datastore on which replication source virtual machines are running, replication sync operations for these virtual machines fail with an error message similar to the following:

VRM Server runtime error. Please check the documentation for any troubleshooting information.
The detailed exception is: 'Invalid datastore format '<Datastore Name>'

This issue is resolved in this release.
 Attempts to unmount NFS Datastore might fail
Attempts to unmount NFS Datastore might fail as the NFS IOs could be stuck due to connectivity issues during NFS LOCK LOST errors. You will see an error message similar to the following:

cpu23:xxxxx opID=xxxxxabf)WARNING: NFS: 1985: datastore1 has open files, cannot be unmounted

This issue is resolved in this release.
Upgrade and Installation Issues

 Error message observed on the boot screen when ESXi 5.5 host boots from vSphere Auto Deploy Stateless Caching
An error message similar to the following with tracebacks is observed on the boot screen when ESXi 5.5 host boots from Auto Deploy Stateless Caching. The error is due to an unexpected short length message of less than four characters in the syslog network.py script.

IndexError: string index out of range

This issue is resolved in this release.
 Attempts to install or upgrade VMware Tools on a Solaris 10 Update 3 virtual machine might fail
Attempts to install or upgrade VMware Tools on a Solaris 10 Update 3 virtual machine might fail with the following error message:

Detected X version 6.9
Could not read /usr/lib/vmware-tools/configurator/XOrg/7.0/vmwlegacy_drv.so Execution aborted.

This issue occurs if the vmware-config-tools.pl script copies the vmwlegacy_drv.so file, which should not be used in Xorg 6.9.
 Keyboard layout option for DCUI and host profile user interface might be incorrectly displayed as Czechoslovakian
The keyboard layout option for the Direct Console User Interface (DCUI) and host profile user interface might incorrectly appear as Czechoslovakian. This option is displayed during ESXi installation and also in the DCUI after installation.

This issue is resolved in this release by renaming the keyboard layout option to Czech.
 Option to retain tools.conf file available by default
When you upgrade the Vmware Tools in 64-bit Windows guest operating system, the tools.conf file gets removed automatically. The tools.conf file will be retained by default from ESXi 5.5 Update 3 release onwards.
 Guest Operating System might fail on reboot after install, upgrade, or uninstall of VMware Tools
When you power off a virtual machine immediately after an install, upgrade or uninstall of VMware Tools in a Linux environment (RHEL or Cent OS 6), the guest OS might fail during the next reboot due to corrupted RAMDISK image file. The guest OS reports an error similar to the following:

RAMDISK: incomplete write (31522 != 32768)
write error
Kernel panic - not syncing : VFS: Unable to mount root fs on unknown-block(0,0)

This release resolves the complete creation of the initramfs file creation during an install, upgrade or uninstall of VMware Tools.

Guest OS with corrupted RAMDISK image file can be rescued to complete boot state. For more information, see Knowledge Base article 2086520.

This issue is resolved in this release.
 Applying host profile with stateless caching enabled on stateless ESXi host might take a long time to complete
Applying host profile on stateless ESXi host with large number of storage LUNs might take long time to reboot when you enable stateless caching with the esx as first disk argument. This happens when you manually apply host profile or during the reboot of the host.

This issue is resolved in this release.
 VIB stage operation might cause VIB installation or configuration change to be lost after an ESXi host reboot
When some VIBs are installed on the system, esxupdate constructs a new image in /altbootbank and changes the /altbootbank boot.cfg bootstate to be updated. When a live installable VIB is installed, the system saves the configuration change to /altbootbank. The stage operation deletes the contents of/altbootbank unless you perform a remediate operation after the stage operation. The VIB installation might be lost if you reboot the host after a stage operation.

This issue is resolved in this release.
Virtual SAN Issues

 Virtual SAN cluster check might fail due to unexpected network partitioning in the cluster
Virtual SAN cluster check might fail due to an unexpected network partitioning where the IGMP v3 query is not reported if the system is in V2 mode.

This issue is resolved in this release.
 Virtual SAN on high latency disks might cause the Input/Output backlogs and the cluster to become unresponsive
Virtual SAN does not gracefully handle extremely high latency disks that are about to die. Such a dying disk might cause Input/Output backlogs and the Virtual SAN cluster nodes might become unresponsive in the vCenter Server.

This issue is resolved in this release with a new feature, Dying Disk Handling (DDH) which provides latency monitoring framework in the kernel, a daemon to detect high latency periods, and a mechanism to unmount individual disks and diskgroups.
 Improvement in Virtual SAN resynchronization operation
There might be a situation where the Virtual SAN component resynchronization operation might stall or become very slow. This release introduces the component-based congestion to improve the resynchronization operation and the virtual SAN cluster stability.
vCenter Server and vSphere Web Client Issues

 The Summary tab might display incorrect values for provisioned space values of virtual machines and NFS or NAS Datastores on VAAI enabled hosts
When a virtual disk with Thick Provision Lazy zeroed format is created on a VAAI supported NAS in a VAAI enabled ESXi host, the provisioned space for the corresponding virtual machine and datastore might be displayed incorrectly.

This issue is resolved in this release.
Virtual Machine Management Issues

 Attempts to add USB device through vSphere Client or the vSphere Web Client might fail
Attempts to add USB device(s) through vSphere Client and vSphere Web Client might fail if Intel USB 3.0 driver is used.

This issue is resolved in this release.
 Taking quiesce snapshot of a virtual machine might result in the MOB value of currentSnapshot field to be unset
After you create a quiesce snapshot and browse through the Managed Object Browser (MOB) of the virtual machine, the MOB value of currentSnapshot field is observed to be unset. To view the currentSnapshot, you can navigate to Content -> root folder -> datacenter -> vmFolder -> vmname -> snapshot -> currentSnaphot.

This issue is resolved in this release.
 Multiple opID tagging log messages are rapidly logged in the VMkernel log
The helper world opID tagging generates a lot of log messages that are logged rapidly in the VMkernel log filling it up. Logs similar to the following are logged in the VMkernel log:

cpu16:nnnnn)World: nnnnn: VC opID hostd-60f4 maps to vmkernel opID nnnnnnnn cpu16:nnnnn)World: nnnnn: VC opID HB-host-nnn@nnn-nnnnnnn-nn maps to vmkernel opID nnnnnnnn cpu8:nnnnn)World: nnnnn: VC opID SWI-nnnnnnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu22:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu14:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn cpu4:nnnnn)World: nnnnn: VC opID hostd-nnnn maps to vmkernel opID nnnnnnnn

This issue is resolved in this release.
 Support for USB 3.0
Support for USB 3.0 has been added in this release, currently only for Apple Mac Pro.

High Availability and Fault Tolerance Issues

 In an environment with Nutanix NFS storage, the secondary FT virtual machine fails to take over when the primary FT virtual machine is down (KB 2096296).
vMotion and Storage vMotion Issues

 Unable to perform Fast Suspend and Resume or Storage vMotion on preallocated virtual machines
When you perform Fast Suspend and Resume (FSR) or Storage vMotion on preallocated virtual machines, the operation might fail as the reservation validation fails during reservation transfer from the source to the destination virtual machine.

This issue is resolved in this release.
 Storage vMotion fails on a virtual machine
Performing a storage vMotion on a virtual machine might fail if you have configured the local host swap and set the value of the checkpoint.cptConfigName in the VMX file. An error message similar to the following might be displayed:

xxxx-xx-xxT00:xx:xx.808Z| vmx| I120: VMXVmdbVmVmxMigrateGetParam: type: 2 srcIp=<127.0.0.1> dstIp=<127.0.0.1> mid=xxxxxxxxxxxxx uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx priority=none checksumMemory=no maxDowntime=0 encrypted=0 resumeDuringPageIn=no latencyAware=no diskOpFile=
<snip>
xxxx-xx-xxT00:xx:xx.812Z| vmx| I120: VMXVmdb_SetMigrationHostLogState: hostlog state transits to failure for migrate 'to' mid xxxxxxxxxxxxxxxx

This issue is resolved in this release.
 Changed Block Tracking (CBT) is reset for virtual RDM disks during cold migration 
Cold migration between different datastores does not support CBT reset for the virtual Raw Device Mapping (RDM) disks. 

This issue is resolved in this release.
VMware Tools Issues

 Attempts to upgrade VMware Tools on a Windows 2000 virtual machine might fail
Attempts to upgrade VMware Tools on a Windows 2000 virtual machine might fail with an error message similar to the following written to the vmmsi.log file:

Invoking remote custom action. DLL: C:\WINNT\Installer\MSI12.tmp, Entrypoint: VMRun
VM_CacheMod. Return value 3.
PROPERTY CHANGE: Deleting RESUME property. Its current value is '1'.
INSTALL. Return value 3.

This issue is resolved in this release.
 Some of the drivers might not work as expected on Solaris 11 virtual machine
On an ESXi 5.5 host, some of the drivers installed on Solaris 11 guest operating system might be from Solaris 10. As a result, the drivers might not work as expected.

This issue is resolved in this release.
 Attempts to configure VMware Tools with new kernel might truncate the driver list in add_drivers entry
When you attempt to configure VMware Tools with new kernel using the /usr/bin/vmware-config-tools.pl -k <kernel version> script after the kernel has been updated with Dracut, the driver list in add_drivers entry of /etc/dracut.conf.d/vmware-tools.conf file gets truncated. This issue occurs when the VMware Tools are upstreamed in the kernel.

This issue is resolved in this release.
 Unable to open telnet on Windows 8 or Windows Server 2012 guest operating system after installing VMware Tools
After installing VMware Tools on Windows 8 or Windows Server 2012 guest operating system, attempts to open telnet using the start telnet://xx.xx.xx.xxcommand fails with the following error message

Make sure the virtual machine's configuration allows the guest to open host applications

This issue is resolved in this release.
 Guest operating system event viewer displays warning messages after you install VMware Tools
After you install VMware Tools, if you attempt to do a RDP to a Windows virtual machine, some of the plugins might display a warning message in the Windows event log. The warning message indicates the failure to send remote procedure calls to the host.

This issue is resolved in this release.
 VMware Tools service might fail on a Linux virtual machine during shutdown
On a Linux virtual machine, the VMware Tools service, vmtoolsd, might fail when you shut down the guest operating system.

This issue is resolved in this release.
 VMware Tools might fail to automatically upgrade during the first power-on operation of the virtual machine
When a virtual machine is deployed or cloned with guest customization and the VMware Tools Upgrade Policy is set to allow the virtual machine to automatically upgrade VMware Tools at next power-on, VMware Tools might fail to automatically upgrade during the first power-on operation of the virtual machine.

This issue is resolved in this release.
 Quiescing operations might result in a Windows virtual machine to panic
Attempts to perform a quiesced snapshot on a virtual machine running Microsoft Windows 2008 or later might fail and the VM might panic with a blue screen and error message similar to the following:

A problem has been detected and Windows has been shut down to prevent damage to your computer. If this is the first time you've seen this Stop error screen restart your computer. If this screen appears again, follow these steps:

Disable or uninstall any anti-virus, disk defragmentation or backup utilities. Check your hard drive configuration, and check for any updated drivers. Run CHKDSK /F to check for hard drive corruption, and then restart your computer.

For more information, see Knowledge Base article 2115997.

This issue is resolved in this release.
 Virtual machine might fail to respond after a snapshot operation on a Linux VM
When you attempt to create a quiesced snapshot of a Linux virtual machine, the VM might fail after the snapshot operation and require a reboot. Error messages similar to the following are written to vmware.log file:

TZ| vmx| I120: SnapshotVMXTakeSnapshotComplete: done with snapshot 'smvi_UUID': 0
TZ| vmx| I120: SnapshotVMXTakeSnapshotComplete: Snapshot 0 failed: Failed to quiesce the virtual machine (40).
TZ| vmx| I120: GuestRpcSendTimedOut: message to toolbox timed out.
TZ| vmx| I120: Vix: [18631 guestCommands.c:1926]: Error VIX_E_TOOLS_NOT_RUNNING in
MAutomationTranslateGuestRpcError(): VMware Tools are not running in the guest

For further details, see Knowledge Base article 2116120

This issue is resolved in this release.
 New Issue Attempts to perform snapshot consolidation might fail with the error: Unexpected signal: 11
Snapshot consolidation or deletion results in the virtual machines running on VMware ESXi 5.5 Update 3 hosts to fail with the error: Unexpected signal: 11. You will see an log message similar to the following in the vmware.log file:

[YYYY-MM-DD] <time>Z| vcpu-0| I120: SNAPSHOT: SnapshotDiskTreeFind: Detected node change from 'scsiX:X' to ''.

For further details, see Knowledge Base article 2133118.

This issue is resolved in this release.
Known Issues

The known issues existing in ESXi 5.5 are grouped as follows:
Installation and Upgrade Issues
Networking Issues
Server Configuration Issues
Storage Issues
Virtual SAN Issues
vCenter Server and vSphere Web Client Issues
Virtual Machine Management Issues
VMware HA and Fault Tolerance Issues
Supported Hardware Issues
VMware Tools Issues
Miscellaneous Issues
New known issues documented in this release are highlighted as New Issue.
Installation and Upgrade Issues
New Issue The VMware Tools service user processes might not run on Linux OS after installing the latest VMware Tools package
On Linux OS, you might encounter VMware Tools upgrade or installation issues or the VMware Tools service (vmtoolsd) user processes might not run after installing the latest VMware Tools package. The issue occurs if the your glibc version is older than version 2.5, like SLES10sp4.
Workaround: Upgrade the Linux glibc to version 2.5 or above.
Attempts to get all image profiles might fail while running the Get-EsxImageProfile command in vSphere PowerCLI
When you run the Get-EsxImageProfile command using vSphere PowerCLI to get all image profiles, an error similar to the following is displayed:

PowerCLI C:\Windows\system32> Get-EsxImageProfile
Get-EsxImageProfile : The parameter 'name' cannot be an empty string.
Parameter name: name
At line:1 char:20
+ Get-EsxImageProfile <<<<
+ CategoryInfo : NotSpecified: (:) [Get-EsxImageProfile], ArgumentException
+ FullyQualifiedErrorId : System.ArgumentException,VMware.ImageBuilder.Commands.GetProfiles

Workaround: Run the Get-EsxImageProfile -name "ESXi-5.x*" command, which includes the -name option and display all image profiles created during the PowerCLI session.

For example, running the command Get-EsxImageProfile -name "ESXi-5.5.*" displays all 5.5 image profiles similar to the following: 

PowerCLI C:\Program Files (x86)\VMware\Infrastructure\vSphere PowerCLI> Get-EsxmageProfile -name "ESXi-5.5.*"

Name Vendor Last Modified Acceptance Level
---- ------ ------------- ----------------
ESXi-5.5.0-20140701001s-no-... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140302001-no-t... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140604001-no-t... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20140401020s-sta... VMware, Inc. 8/23/2014 6:... PartnerSupported
ESXi-5.5.0-20131201001s-sta... VMware, Inc. 8/23/2014 6:... PartnerSupported
Simple Install fails on Windows Server 2012
Simple Install fails on Windows Server 2012 if the operating system is configured to use a DHCP IP address
Workaround: Configure the Windows 2012 Server to use a static IP address.
If you use preserve VMFS with Auto Deploy Stateless Caching or Auto Deploy Stateful Installs, no core dump partition is created
When you use Auto Deploy for Stateless Caching or Stateful Install on a blank disk, an MSDOS partition table is created. However, no core dump partition is created.
Workaround: When you enable the Stateless Caching or Stateful Install host profile option, select Overwrite VMFS, even when you install on a blank disk. When you do so, a 2.5GB coredump partition is created.
During scripted installation, ESXi is installed on an SSD even though the --ignoressd option is used with the installorupgrade command
In ESXi 5.5, the --ignoressd option is not supported with the installorupgrade command. If you use the --ignoressd option with the installorupgradecommand, the installer displays a warning that this is an invalid combination. The installer continues to install ESXi on the SSD instead of stopping the installation and displaying an error message.
Workaround: To use the --ignoressd option in a scripted installation of ESXi, use the install command instead of the installorupgrade command.
Delay in Auto Deploy cache purging might apply a host profile that has been deleted
After you delete a host profile, it is not immediately purged from the Auto Deploy. As long as the host profile is persisted in the cache, Auto Deploy continues to apply the host profile. Any rules that apply the profile fail only after the profile is purged from the cache.
Workaround: You can determine whether any rules use deleted host profiles by using the Get-DeployRuleSet PowerCLI cmdlet. The cmdlet shows the stringdeleted in the rule's itemlist. You can then run the Remove-DeployRule cmdlet to remove the rule.
Applying host profile that is set up to use Auto Deploy with stateless caching fails if ESX is installed on the selected disk
You use host profiles to set up Auto Deploy with stateless caching enabled. In the host profile, you select a disk on which a version of ESX (not ESXi) is installed. When you apply the host profile, an error that includes the following text appears. 
Expecting 2 bootbanks, found 0
Workaround: Select a different disk to use for stateless caching, or remove the ESX software from the disk. If you remove the ESX software, it becomes unavailable.
Installing or booting ESXi version 5.5.0 fails on servers from Oracle America (Sun) vendors
When you perform a fresh ESXi version 5.5.0 installation or boot an existing ESXi version 5.5.0 installation on servers from Oracle America (Sun) vendors, the server console displays a blank screen during the installation process or when the existing ESXi 5.5.0 build boots. This happens because servers from Oracle America (Sun) vendors have a HEADLESS flag set in the ACPI FADT table, even though they are not headless platforms.
Workaround: When you install or boot ESXi 5.5.0, pass the boot option ignoreHeadless="TRUE".
If you use ESXCLI commands to upgrade an ESXi host with less than 4GB physical RAM, the upgrade succeeds, but some ESXi operations fail upon reboot
ESXi 5.5 requires a minimum of 4GB of physical RAM. The ESXCLI command-line interface does not perform a pre-upgrade check for the required 4GB of memory. You successfully upgrade a host with insufficient memory with ESXCLI, but when you boot the upgraded ESXi 5.5 host with less than 4GB RAM, some operations might fail.
Workaround: None. Verify that the ESXi host has more than 4GB of physical RAM before the upgrade to version 5.5.
After upgrade from vCenter Server Appliance 5.0.x to 5.5, vCenter Server fails to start if an external vCenter Single Sign-On is used 
If the user chooses to use an external vCenter Single Sign-On instance while upgrading the vCenter Server Appliance from 5.0.x to 5.5, the vCenter Server fails to start after the upgrade. In the appliance management interface, the vCenter Single Sign-On is listed as not configured.
Workaround: Perform the following steps:
In a Web browser, open the vCenter Server Appliance management interface (https://appliance-address:5480).
On the vCenter Server/Summary page, click the Stop Server button.
On the vCenter Server/SSO page, complete the form with the appropriate settings, and click Save Settings.
Return to the Summary page and click Start Server.
When you use ESXCLI to upgrade an ESXi 4.x or 5.0.x host to version 5.1 or 5.5, the vMotion and Fault Tolerance Logging (FT Logging) settings of any VMKernel port group are lost after the upgrade
If you use the command esxcli software profile update <options> to upgrade an ESXi 4.x or 5.0.x host to version 5.1 or 5.5, the upgrade succeeds, but the vMotion and FT Logging settings of any VMkernel port group are lost. As a result, vMotion and FT Logging are restored to the default setting (disabled).
Workaround: Perform an interactive or scripted upgrade, or use vSphere Update Manager to upgrade hosts. If you use the esxcli command, apply vMotion and FT Logging settings manually to the affected VMkernel port group after the upgrade.
When you upgrade vSphere 5.0.x or earlier to version 5.5, system resource allocation values that were set manually are reset to the default value
In vSphere 5.0.x and earlier, you modify settings in the system resource allocation user interface as a temporary workaround. You cannot reset the value for these settings to the default without completely reinstalling ESXi. In vSphere 5.1 and later, the system behavior changes, so that preserving custom system resource allocation settings might result in values that are not safe to use. The upgrade resets all such values.
Workaround: None.
IPv6 settings of virtual NIC vmk0 are not retained after upgrade from ESX 4.x to ESXi 5.5
When you upgrade an ESX 4.x host with IPv6 enabled to ESXi 5.5 by using the --forcemigrate option, the IPv6 address of virtual NIC vmk0 is not retained after the upgrade.
Workaround: None.
Networking Issues
Unable to use PCNet32 network adapter with NSX opaque network
When PCNet32 flexible network adapter is configured with NSX opaque network backing, the adapter disconnects while powering on the VM.
Workaround: None
 Upgrading to ESXi 5.5 might change the IGMP configuration of TCP/IP stack for multicast group management
The default IGMP version of the management interfaces is changed from IGMP V2 to IGMP V3 for ESXi 5.5 hosts for multicast group management. As a result, when you upgrade to ESXi 5.5, the management interface might revert back to IGMP V2 from IGMP V3 if it receives an IGMP query of a previous version and you might notice IGMP version mismatch error messages.

Workaround: Edit the default IGMP version by modifying the TCP/IP IGMP rejoin interval in the Advanced Configuration option.
Static routes associated with vmknic interfaces and dynamic IP addresses might fail to appear after reboot
After you reboot the host, static routes that are associated with VMkernel network interface (vmknic) and dynamic IP address might fail to appear.
This issue occurs due to a race condition between DHCP client and restore routes command. The DHCP client might not finish acquiring an IP address for vmknics when the host attempts to restore custom routes during the reboot process. As a result, the gateway might not be set up and the routes are not restored.

Workaround: Run the esxcfg-route –r command to restore the routes manually.
An ESXi host stops responding after being added to vCenter Server by its IPv6 address
When you add an ESXi host to vCenter Server by IPv6 link-local address of the form fe80::/64, within a short time the host name becomes dimmed and the host stops responding to vCenter Server.
Workaround: Use a valid IPv6 address that is not a link-local address.
The vSphere Web Client lets you configure more virtual functions than are supported by the physical NIC and does not display an error message
In the SR-IOV settings of a physical adapter, you can configure more virtual functions than are supported by the adapter. For example, you can configure 100 virtual functions on a NIC that supports only 23, and no error message appears. A message prompts you to reboot the host so that the SR-IOV settings are applied. After the host reboots, the NIC is configured with as many virtual functions as the adapter supports, or 23 in this example. The message that prompts you to reboot the host persists when it should not appear.
Workaround: None
On an SR-IOV enabled ESXi host, virtual machines associated with virtual functions might not start
When SR-IOV is enabled on an ESXi host 5.1 or later with Intel ixgbe NICs, if several virtual functions are enabled in the environment, some virtual machines might fail to start.
The vmware.log file contains messages similar to the following:
2013-02-28T07:06:31.863Z| vcpu-1| I120: Msg_Post: Error
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.log.error.unrecoverable] VMware ESX unrecoverable error: (vcpu-1)
2013-02-28T07:06:31.863Z| vcpu-1| I120+ PCIPassthruChangeIntrSettings: 0a:17.3 failed to register interrupt (error code 195887110)
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.haveLog] A log file is available in "/vmfs/volumes/5122262e-ab950f8e-cd4f-b8ac6f917d68/VMLibRoot/VMLib-RHEL6.2-64-HW7-default-3-2-1361954882/vmwar
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.requestSupport.withoutLog] You can request support.
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.requestSupport.vmSupport.vmx86]
2013-02-28T07:06:31.863Z| vcpu-1| I120+ To collect data to submit to VMware technical support, run "vm-support".
2013-02-28T07:06:31.863Z| vcpu-1| I120: [msg.panic.response] We will respond on the basis of your support entitlement.
Workaround: Reduce the number of virtual functions associated with the affected virtual machine before starting it.
On an Emulex BladeEngine 3 physical network adapter, a virtual machine network adapter backed by a virtual function cannot reach a VMkernel adapter that uses the physical function as an uplink
Traffic does not flow between a virtual function and its physical function. For example, on a switch backed by the physical function, a virtual machine that uses a virtual function on the same port cannot contact a VMkernel adapter on the same switch. This is a known issue of the Emulex BladeEngine 3 physical adapters. For information, contact Emulex.
Workaround: Disable the native driver for Emulex BladeEngine 3 devices on the host. For more information, see VMware KB 2044993.
The ESXi Dump Collector fails to send the ESXi core file to the remote server
The ESXi Dump Collector fails to send the ESXi core file if the VMkernel adapter that handles the traffic of the dump collector is configured to a distributed port group that has a link aggregation group (LAG) set as the active uplink. An LACP port channel is configured on the physical switch.
Workaround: Perform one of the following workarounds:
Use a vSphere Standard Switch to configure the VMkernel adapter that handles the traffic for the ESXi Dump Collector with the remote server.
Use standalone uplinks to handle the traffic for the distributed port group where the VMkernel adapter is configured.
If you change the number of ports that a vSphere Standard Switch or vSphere Distributed Switch has on a host by using the vSphere Client, the change is not saved, even after a reboot
If you change the number of ports that a vSphere Standard Switch or vSphere Distributed Switch has on an ESXi 5.5 host by using the vSphere Client, the number of ports does not change even after you reboot the host.
When a host that runs ESXi 5.5 is rebooted, it dynamically scales up or down the ports of virtual switches. The number of ports is based on the number of virtual machines that the host can run. You do not have to configure the number of switch ports on such hosts.
Workaround: None in the vSphere Client.
Server Configuration Issues
New Issue NIC hardware might stop responding with an hardware error message
The NIC hardware might occassionally stop responding under certain circumstances with the following error message in the driver logs:

Detected Hardware Unit Hang

The issue is observed with some new e1000e devices like 82579, i217, i218 and i219.
Workaround: The NIC hardware resets itself after the issue occurs.
Menu navigation problem is experienced When Direct Control User Interface is accessed from a serial console
When Direct Control User Interface is accessed from a serial console, the Up and Down arrow keys do not work while navigating to the menu and the user is forcefully logged out of the DCUI configuration screen.

Workaround: Stop the DCUI process. The DCUI process will be restarted automatically.

Host profiles might incorrectly appear as compliant after ESXi hosts are upgrade to 5.5 Update 2 followed by changes in host configuration 
If an ESXi host that is compliant with an host profile is updated to ESXi 5.5 Update 2 followed by some changes in host configuration and you re-check the compliance of the host with the host profile, the profile is incorrectly reported to be compliant. 

Workaround:
In vSPhere Client, navigate to the host profile that has the issue and run Update profile From Reference Host.
In vSPhere Web Client, navigate to host Profile that has the issue, click Copy settings from host, select the host from which you want to copy the configuration settings and click OK.
Host Profile remediation fails with vSphere Distributed Switch
Remediation errors might occur when applying a Host Profile with a vSphere Distributed Switch and a virtual machine with Fault Tolerance is in a powered off state on a host that uses the distributed switch in that Host Profile.
Workaround: Move the powered off virtual machines to another host in order for the Host Profile to succeed.
Host profile receives firewall settings compliance errors when you apply ESX 4.0 or ESX 4.1 profile to ESXi 5.5.x host
If you extract a host profile from an ESX 4.0 or ESX 4.1 host and attempt to apply it to an ESXi 5.5.x host, the profile remediation succeeds. The compliance check receives firewall settings errors that include the following:

Ruleset LDAP not found
Ruleset LDAPS not found
Ruleset TSM not found
Ruleset VCB not found
Ruleset activeDirectorKerberos not found
Workaround: No workaround is required. This is expected because the firewall settings for an ESX 4.0 or ESX 4.1 host are different from those for an ESXi 5.5.x host.
Changing BIOS device settings for an ESXi host might result in invalid device names
Changing a BIOS device setting on an ESXi host might result in invalid device names if the change causes a shift in the <segment:bus:device:function> values assigned to devices. For example, enabling a previously-disabled integrated NIC might shift the <segment:bus:device:function> values assigned to other PCI devices, causing ESXi to change the names assigned to these NICs. Unlike previous versions of ESXi, ESXi 5.5 attempts to preserve devices names through <segment:bus:device:function> changes if the host BIOS provides specific device location information. Due to a bug in this feature, invalid names such as vmhba1 and vmnic32 are sometimes generated.
Workaround: Rebooting the ESXi host once or twice might clear the invalid device names and restore the original names. Do not run an ESXi host with invalid device names in production.
Storage Issues
ESXi hosts with HBA drivers might stop responding when the VMFS heartbeats to the datastores timeout
ESXi hosts with HBA drivers might stop responding when the VMFS heartbeats to the datastores timeout with messages similar to the following:

mem>2014-05-12T13:34:00.639Z cpu8:1416436)VMW_SATP_ALUA: satp_alua_issueCommandOnPath:651: Path "vmhba2:C0:T1:L10" (UP) command 0xa3 failed with status Timeout. H:0x5 D:0x0 P:0x0 Possible sense data: 0x5 0x20 0x0.2014-05-12T13:34:05.637Z cpu0:33038)VMW_SATP_ALUA: satp_alua_issueCommandOnPath:651: Path "vmhba2:C0:T1:L4" (UP) command 0xa3 failed with status Timeout. H:0x5 D:0x0 P:0x0 Possible sense data: 0x0 0x0 0x0.

This issue occurs with the HBA driver when a high disk I/O on the datastore is connected to the ESXi host and multipathing is enabled at the target level instead of the HBA level.

Workaround: Replace the HBA driver with the latest async HBA driver.
Attempts to perform live storage vMotion of virtual machines with RDM disks might fail 
Storage vMotion of virtual machines with RDM disks might fail and virtual machines might be seen in powered off state. Attempts to power on the virtual machine fails with the following error: 

Failed to lock the file

Workaround: None.
Renamed tags appear as missing in the Edit VM Storage Policy wizard
A virtual machine storage policy can include rules based on datastore tags. If you rename a tag, the storage policy that references this tag does not automatically update the tag and shows it as missing.
Workaround: Remove the tag marked as missing from the virtual machine storage policy and then add the renamed tag. Reapply the storage policy to all out-of-date entities.
A virtual machine cannot be powered on when the Flash Read Cache block size is set to 16KB, 256KB, 512KB, or 1024KB
A virtual machine configured with Flash Read Cache and a block size of 16KB, 256KB, 512KB, or 1024KB cannot be powered on. Flash Read Cache supports a minimum cache size of 4MB and maximum of 200GB, and a minimum block size of 4KB and maximum block size of 1MB. When you power on a virtual machine, the operation fails and the following messages appear:
An error was received from the ESX host while powering on VM.
Failed to start the virtual machine.
Module DiskEarly power on failed.
Failed to configure disk scsi0:0.
The virtual machine cannot be powered on with an unconfigured disk. vFlash cache cannot be attached: msg.vflashcache.error.VFC_FAILURE
Workaround: Configure virtual machine Flash Read Cache size and block size.
Right-click the virtual machine and select Edit Settings.
On the Virtual Hardware tab, expand Hard disk to view the disk options.
Click Advanced next to the Virtual Flash Read Cache field.
Increase the cache size reservation or decrease the block size.
Click OK to save your changes.
A custom extension of a saved resource pool tree file cannot be loaded in the vSphere Web Client
A DRS error message appears on host summary page.
When you disable DRS in the vSphere Web Client, you are prompted to save the resource pool structure so that it can be reloaded in the future. The default extension of this file is .snapshot, but you can select a different extension for this file. If the file has a custom extension, it appears as disabled when you try to load it. This behavior is observed only on OS X.
Workaround: Change the extension to .snapshot to load it in the vSphere Web Client on OS X.
DRS error message appears on the host summary page
The following DRS error message appears on the host summary page:
Unable to apply DRS resource settings on host. The operation is not allowed in the current state. This can significantly reduce the effectiveness of DRS.
In some configurations a race condition might result in the creation of an error message in the log that is not meaningful or actionable. This error might occur if a virtual machine is unregistered at the same time that DRS resource settings are applied.
Workaround: Ignore this error message.
Configuring virtual Flash Read Cache for VMDKs larger than 16TB results in an error
Virtual Flash Read Cache does not support virtual machine disks larger than 16TB. Attempts to configure such disks will fail.
Workaround: None
Virtual machines might power off when the cache size is reconfigured 
If you incorrectly reconfigure the virtual Flash Read Cache on a virtual machine, for example by assigning an invalid value, the virtual machine might power off.
Workaround: Follow the recommended cache size guidelines in the vSphere Storage documentation.
Reconfiguring a virtual machine with virtual Flash Read Cache enabled might fail with the Operation timed out error 
Reconfiguration operations require a significant amount of I/O bandwidth. When you run a heavy load, such operations might time out before they finish. You might also see this behavior if the host has LUNs that are in an all paths down (APD) state.
Workaround: Fix all host APD states and retry the operation with a smaller I/O load on the LUN and host.
DRS does not vMotion virtual machines with virtual Flash Read Cache for load balancing purpose
DRS does not vMotion virtual machines with virtual Flash Read Cache for load balancing purposes.
Workaround: DRS does not recommend these virtual machines for vMotion except for the following reasons:
To evacuate a host that the user has requested to enter maintenance or standby mode.
To fix DRS rule violations.
Host resource usage is in red state.
One or most hosts is over utilized and virtual machine demand is not being met. 
Note: You can optionally set DRS to ignore this reason.
Hosts are put in standby when the active memory of virtual machines is low but consumed memory is high
ESXi 5.5 introduces a change in the default behavior of DPM designed to make the feature less aggressive, which can help prevent performance degradation for virtual machines when active memory is low but consumed memory is high. The DPM metric is X%*IdleConsumedMemory + active memory. The X% variable is adjustable and is set to 25% by default.
Workaround: You can revert to the aggressive DPM behavior found in earlier releases of ESXi by setting PercentIdleMBInMemDemand=0 in the advanced options.
vMotion initiated by DRS might fail
When DRS recommends vMotion for virtual machines with a virtual Flash Read Cache reservation, vMotion might fail because the memory (RAM) available on the target host is insufficient to manage the Flash Read Cache reservation of the virtual machines.
Workaround: Follow the Flash Read Cache configuration recommendations documented in vSphere Storage.
If vMotion fails, perform the following steps:
Reconfigure the block sizes of the virtual machines on the target host and the incoming virtual machines to reduce the overall target usage of the VMkernel memory on the target host.
Use vMotion to manually migrate the virtual machine to the target host to ensure the condition is resolved.
You are unable to view problems that occur during virtual flash configuration of individual SSD devices 
The configuration of virtual flash resources is a task that operates on a list of SSD devices. When the task finishes for all objects, the vSphere Web Client reports it as successful, and you might not be notified of problems with the configuration of individual SSD devices.
Workaround: Perform one of the following tasks.
In the Recent Tasks panel, double-click the completed task. 
Any configuration failures appear in the Related events section of the Task Details dialog box.
Alternatively, follow these steps:
Select the host in the inventory.
Click the Monitor tab, and click Events.
Unable to obtain SMART information for Micron PCIe SSDs on the ESXi host 
Your attempts to use the esxcli storage core device smart get -d command to display statistics for the Micron PCIe SSD device fail. You get the following error message:
Error getting Smart Parameters: CANNOT open device
Workaround: None. In this release, the esxcli storage core device smart command does not support Micron PCIe SSDs.
ESXi does not apply the bandwidth limit that is configured for a SCSI virtual disk in the configuration file of a virtual machine
You configure the bandwidth and throughput limits of a SCSI virtual disk by using a set of parameters in the virtual machine configuration file (.vmx). For example, the configuration file might contain the following limits for a scsi0:0 virtual disk:
sched.scsi0:0.throughputCap = "80IOPS"
sched.scsi0:0.bandwidthCap = "10MBps"
sched.scsi0:0.shares = "normal"
ESXi does not apply the sched.scsi0:0.bandwidthCap limit to the scsi0:0 virtual disk.
Workaround: Revert to an earlier version of the disk I/O scheduler by using the vSphere Web Client or the esxcli system settings advanced set command.
In the vSphere Web Client, edit the Disk.SchedulerWithReservation parameter in the Advanced System Settings list for the host.
Navigate to the host.
On the Manage tab, select Settings and select Advanced System Settings.
Locate the Disk.SchedulerWithReservation parameter, for example, by using the Filter or Find text boxes.
Click Edit and set the parameter to 0.
Click OK.
In the ESXi Shell to the host, run the following console command:
esxcli system settings advanced set -o /Disk/SchedulerWithReservation -i=0
A virtual machine configured with Flash Read Cache cannot be migrated off a host if there is an error in the cache
A virtual machine with Flash Read Cache configured might have a migration error if the cache is in an error state and is unusable. This error causes migration of the virtual machine to fail.
Workaround:
Reconfigure the virtual machine and disable the cache.
Perform the migration.
Re-enable the cache after the virtual machine is migrated.
Alternatively, the virtual machine must be powered off and then powered on to correct the error with the cache.
You cannot delete the VFFS volume after a host is upgraded from ESXi 5.5 Beta
You cannot delete the VFFS volume after a host is upgraded from ESXi 5.5 Beta.
Workaround: This occurs only when you upgrade from ESXi 5.5 Beta to ESXi 5.5. To avoid this problem, install ESXi 5.5 instead of upgrading. If a you upgrade from ESXi 5.5 Beta, delete the VFFS volume before you upgrade.
Expected latency runtime improvements are not seen when virtual Flash Read Cache is enabled on virtual machines with older Windows and Linux guest operating systems
Virtual Flash Read Cache provides optimal performance when the cache is sized to match the target working set, and when the guest file systems are aligned to at least a 4KB boundary. The Flash Read Cache filters out misaligned blocks to avoid caching partial blocks within the cache. This behavior is typically seen when virtual Flash Read Cache is configured for VMDKs of virtual machines with Windows XP and Linux distributions earlier than 2.6. In such cases, a low cache hit rate with a low cache occupancy is observed, which implies a waste of cache reservation for such VMDKs. This behavior is not seen with virtual machines running Windows 7, Windows 2008, and Linux 2.6 and later distributions, which align their file systems to a 4KB boundary to ensure optimal performance.
Workaround: To improve the cache hit rate and optimal use of the cache reservation for each VMDK, ensure that the guest operating file system installed on the VMDK is aligned to at least a 4KB boundary.
Virtual SAN
 New Issue Unmounted Virtual SAN disks and diskgroups displayed as mounted in the vSphere Client UI Operational Status field
After the Virtual SAN disks or diskgroups are unmounted using the esxcli vsan storage diskgroup unmount CLI command or automatically by the Virtual SAN Device Monitor service when disks show persistently high latencies, the vSphere Client UI incorrectly displays the Operational Status field as Mounted. 

Workaround: Verify the Health field that shows a non-healthy value instead of the Operational Status field.
 ESXi host with multiple VSAN disk groups might not display the magnetic disk statistics when you run the vsan.disks_stats command
An ESXi host with multiple VSAN disk groups might not display the magnetic disk (MD) statistics when you run the vsan.disks_stats Ruby vSphere Console (RVC)command. The host displays only the solid-state drive (SSD) information.

Workaround: None 
VM directories contain duplicate swap (.vswp) files
This might occur if virtual machines running on Virtual SAN are not cleanly shutdown, and if you perform a fresh installation of ESXi and vCenter Server without erasing data from Virtual SAN disks. As a result, old swap files (.vswp) are found in the directories for virtual machines that are shut down uncleanly.

Workaround: None

Attempts to add more than seven magnetic disks to a Virtual SAN disk group might fail with incorrect error message
Virtual SAN disk group supports maximum of one SSD and seven magnetic disks (HDD). Attempts to add an additional magnetic disk might fail with an incorrect error message similar to the following:

The number of disks is not sufficient.

Workaround: None
Re-scan failure experienced while adding a Virtual SAN disk
When you add a Virtual SAN disk, re-scan fails due to probe failure for a non-Virtual SAN volume, which causes the operation to fail.

Workaround: Ignore the error as all the disks are registered correctly.
A hard disk drive (HDD) that is removed after its associated solid state drive (SSD) is removed might still be listed as a storage disk claimed by Virtual SAN
If an SSD and then its associated HDD is removed from a Virtual SAN datastore and you run the esxcli vsan storage list command, the removed HDD is still listed as a storage disk claimed by Virtual SAN. If the HDD is inserted back in a different host, the disk might appear to be part of two different hosts.

Workaround: For example, if SSD and HDD is removed from ESXi x and inserted into ESXi y, perform the following steps to prevent the HDD from appearing to be a part of both ESXi x and ESXi y:
1. Insert the SSD and HDD removed from the ESXi x, into ESXi y.
2. Decommission the SSD from ESXi x.
3. Run the command esxcfg-rescan -A.
   The HDD and SSD will no longer be listed on ESXi x.
The Working with Virtual SAN section of the vSphere Storage documentation indicates that the maximum number of HDD disks per a disk group is six. However, the maximum allowed number of HDDs is seven.
After a failure in a Virtual SAN cluster, vSphere HA might report multiple events, some misleading, before restarting a virtual machine
The vSphere HA master agent makes multiple attempts to restart a virtual machine running on Virtual SAN after it has appeared to have failed. If the virtual machine cannot be immediately restarted, the master agent monitors the cluster state, and makes another attempt when conditions indicate that a restart might be successful. For virtual machines running on Virtual SAN, the vSphere HA master has special application logic to detect when the accessibility of a virtual machine's objects might have changed, and attempts a restart whenever an accessibility change is likely. The master agent makes an attempt after each possible accessibility change, and if it did not successfully power on the virtual machine before giving up and waiting for the next possible accessibility change.
After each failed attempt, vSphere HA reports an event indicating that the failover was not successful, and after five failed attempts, reports that vSphere HA stopped trying to restart the virtual machine because the maximum number of failover attempts was reached. Even after reporting that the vSphere HA master agent has stopped trying, however, it does try the next time a possible accessibility change occurs.
Workaround: None.
Powering off a Virtual SAN host causes the Storage Providers view in the vSphere Web Client to refresh longer than expected
If you power off a Virtual San host, the Storage Providers view might appear empty. The Refresh button continues to spin even though no information is shown.
Workaround: Wait at least 15 minutes for the Storage Providers view to be populated again. The view also refreshes after you power on the host.
Virtual SAN reports a failed task as completed
Virtual SAN might report certain tasks as completed even though they failed internally.
The following are conditions and corresponding reasons for errors:
Condition: Users attempt to create a new disk group or add a new disk to already existing disk group when the Virtual SAN license has expired.
Error stack: A general system error occurred: Cannot add disk: VSAN is not licensed on this host.
Condition: Users attempt to create a disk group with the number of disk higher than the supported number. Or they try to add new disks to already existing disk group so that the total number exceeds the supported number of disks per disk group.
Error stack: A general system error occurred: Too many disks.
Condition: Users attempt to add a disk to the disk group that has errors.
Error stack: A general system error occurred: Unable to create partition table.
Workaround: After identifying the reason for a failure, correct the reason and perform the task again.
Virtual SAN datastores cannot store host local and system swap files
Typically, you can place the system swap or host local swap file on a datastore. However, the Virtual SAN datastore does not support system swap and host local swap files. As a result, the UI option that allows you to select the Virtual SAN datastore as the file location for system swap or host local swap is not available.
Workaround: In Virtual SAN environment, use other supported options to place the system swap and host local swap files.
A Virtual SAN virtual machine in a vSphere HA cluster is reported as vSphere HA protected although it has been powered off
This might happen when you power off a virtual machine with its home object residing on a Virtual SAN datastore, and the home object is not accessible. This problem is seen if a HA master agent election occurs after the object becomes inaccessible.
Workaround:
Make sure that the home object is accessible again by checking the compliance of the object with the specified storage policy.
Power on the virtual machine then power it off again.
The status should change to unprotected.
Virtual machine object remains in Out of Date status even after Reapply action is triggered and completed successfully
If you edit an existing virtual machine profile due to the new storage requirements, the associated virtual machine objects, home or disk, might go in Out of Date status.This occurs when your current environment cannot support reconfiguration of virtual machine objects. Using Reapply action does not change the status.
Workaround: Add additional resources, hosts or disks, to the Virtual SAN cluster and invoke Reapply action again.
Automatic disk claiming for Virtual SAN does not work as expected if you license Virtual SAN after enabling it
If you enable Virtual SAN in automatic mode and then assign a license, Virtual SAN fails to claim disks.
Workaround: Change the mode to Manual, and then switch back to Automatic. Virtual SAN will properly claim the disks.
vSphere High Availability (HA) fails to restart a virtual machine when Virtual SAN network is partitioned
This occurs when Virtual SAN uses VMkernel adapters for internode communication, which are on the same subnet as other VMkernel adapters in a cluster. Such configuration could cause network failure and disrupt Virtual SAN internode communication, while vSphere HA internode communication remains unaffected.
In this situation, the HA master agent might detect the failure in a virtual machine, but is unable to restart it. For example, this could occur when the host on which the master agent is running does not have access to the virtual machine's objects.
Workaround: Make sure that the VMkernel adapters used by Virtual SAN do not share a subnet with the VMkernel adapters used for other purposes.
VM directories contain duplicate swap (.vswp) files   
This might occur if virtual machines running on Virtual SAN are not cleanly shutdown, and if you perform a fresh installation of ESXi and vCenter Server without erasing data from Virtual SAN disks. As a result, old swap files (.vswp) are found in the directories for virtual machines that are shut down uncleanly.

Workaround: None

VMs might become inaccessible due to high network latency
In a Virtual SAN cluster setup, if the network latency is high, some VMs might become inaccessible on vCenter Server and you will not be able to power on or access the VM.

Workaround: Run the vsan.check_state -e -r RVC command.
VM operations might timeout due to high network latency
When storage controller with low queue depths are used, high network latency might cause VM operations to time out. 

Workaround: Re-attempt the operations when the network load is lower.
VMs might get renamed to a truncated version of their vmx file path
If the vmx file of a virtual machines is temporarily inaccessible, the VM gets renamed to a truncated version of the vmx file path. For example, the virtual machine might get renamed to /vmfs/volumes/vsan:52f1686bdcb477cd-8e97188e35b99d2e/236d5552-ad93. The truncation might delete half the UUID of the VM home directory making it difficult to map the renamed VM with the original VM, from just the VM name.

Workaround: Run the vsan.fix_renamed_vms RVC command.
vCenter Server and vSphere Web Client
Unable to add ESXi host to Active Directory domain
You might observe that Active Directory domain name is not displayed in Domain drop-down list under Select Users and Groups option when you attempt to assign permissions. Also, the Authentication Services Settings option might not display any trusted domain controller even when the active directory has trusted domains.

Workaround:
Restart netlogond, lwiod, and then lsassd daemons.
Login to ESXi host using vSphere Client.
In the Configuration tab and click Authentication Services Settings.
Refresh to view the trusted domains.
Virtual Machine Management Issues
Unable to perform cold migration and storage vMotion of a virtual machine if the VMDK file name begins with "core"
Attempts to perform cold migration and storage vMotion of a virtual machine might fail if the VMDK file name begins with "core" with error message similar to the following:

A general system error occurred: Error naming or renaming a VM file.

Error messages similar to the following might be displayed in the vpxd.log file:

mem> 2014-01-01T11:08:33.150-08:00 [13512 info 'commonvpxLro' opID=8BA11741-0000095D-86-97] [VpxLRO] -- FINISH task-internal-2471 -- -- VmprovWorkflow --
mem> 2014-01-01T11:08:33.150-08:00 [13512 info 'Default' opID=8BA11741-0000095D-86-97] [VpxLRO] -- ERROR task-internal-2471 -- -- VmprovWorkflow: vmodl.fault.SystemError:
mem> --> Result:
mem> --> (vmodl.fault.SystemError){
mem> --> dynamicType = ,
mem> --> faultCause = (vmodl.MethodFault) null, 
mem> --> reason = "Error naming or renaming a VM file.", 
mem> --> msg = "", 
mem> --> }
This issue occurs when the ESXi host incorrectly classifies VMDK files with a name beginning with "core" as a core file instead of the expected disk type.

Workaround: Ensure that the VMDK file name of the virtual machine does not begin with "core". Also, use the vmkfstools utility to rename the VMDK file to ensure that the file name do not begin with the word "core".
Virtual machines with Windows 7 Enterprise 64-bit guest operating systems in the French locale experience problems during clone operations
If you have a cloned Windows 7 Enterprise 64-bit virtual machine that is running in the French locale, the virtual machine disconnects from the network and the customization specification is not applied. This issue appears when the virtual machine is running on an ESXi 5.1 host and you clone it to ESXi 5.5 and upgrade the VMware Tools version to the latest version available with the 5.5 host.
Workaround: Upgrade the virtual machine compatibility to ESXi 5.5 and later before you upgrade to the latest available version of VMware Tools.
Attempts to increase the size of a virtual disk on a running virtual machine fail with an error
If you increase the size of a virtual disk when the virtual machine is running, the operation might fail with the following error:
This operation is not supported for this device type.
The failure might occur if you are extending the disk to the size of 2TB or larger. The hot-extend operation supports increasing the disk size to only 2TB or less. SATA virtual disks do not support the hot-extend operation no matter what their size is.
Workaround: Power off the virtual machine to extend the virtual disk to 2TB or larger.
VMware HA and Fault Tolerance Issues
New Issue Fault Tolerance (FT) is not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform
Fault tolerance is not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform. Attempts to power on a virtual machine will fail after you enable single-processor Fault Tolerance.
Workaround: None
If you select an ESX/ESXi 4.0 or 4.1 host in a vSphere HA cluster to fail over a virtual machine, the virtual machine might not restart as expected 
When vSphere HA restarts a virtual machine on an ESX/ESXi 4.0 or 4.1 host that is different from the original host the virtual machine was running on, a query is issued that is not answered. The virtual machine is not powered on on the new host until you answer the query manually from the vSphere Client.
Workaround: Answer the query from the vSphere Client. Alternatively, you can wait for a timeout (15 minutes by default), and vSphere HA attempts to restart the virtual machine on a different host. If the host is running ESX/ESXi 5.0 or later, the virtual machine is restarted.
If a vMotion operation without shared storage fails in a vSphere HA cluster, the destination virtual machine might be registered to an unexpected host
A vMotion migration involving no shared storage might fail because the destination virtual machine does not receive a handshake message that coordinates the transfer of control between the two virtual machines. The vMotion protocol powers off both the source and destination virtual machines. If the source and destination hosts are in the same cluster and if vSphere HA has been enabled, the destination virtual machine might be registered by vSphere HA on another host than the one chosen as the target for the vMotion migration.
Workaround: If you want to retain the destination virtual machine and you want it to be registered to a specific host, relocate the destination virtual machine to the destination host. This relocation is best done before powering on the virtual machine.
Supported Hardware Issues
Sensor values for Fan, Power Supply, Voltage, and Current sensors appear under the Other group of the vCenter Server Hardware Status Tab
Some sensor values are listed in the Other group instead of the respective categorized group.
Workaround: None.
I/O memory management unit (IOMMU) faults might appear when the debug direct memory access (DMA) mapper is enabled
The debug mapper places devices in IOMMU domains to help catch device memory accesses to addresses that have not been explicitly mapped. On some HP systems with old firmware, IOMMU faults might appear.
Workaround: Download firmware upgrades from the HP Web site and apply them.
Upgrade the firmware of the HP iLO2 controller. 
Version 2.07, released in August 2011, resolves the problem.
Upgrade the firmware of the HP Smart Array.
For the HP Smart Array P410, version 5.14, released in January 2012, resolves the problem.
VMware Tools Issues
Unable to install VMware Tools on Linux guest operating systems by executing the vmware-install.pl -d command if VMware Tools is not installed earlier
If VMware Tools is not installed on your Linux guest operating system, attempts to perform a fresh installation of VMware Tools by executing the vmware-install.pl -d command might fail.
This issue occurs in the following guest operating systems:
RHEL 7 and later
CentOS 7 and later
Oracle Linux 7 and later
Fedora 19 and later
SLES 12 and later
SLED 12 and later
openSUSE 12.2 and later
Ubuntu 14.04 and later
Debian 7 and later

Workaround: There is no workaround that helps the -default (-d) switch work. However, you can install VMware Tools without the - default switch.
Select Yes when you are prompted with the option Do you still want to proceed with this legacy installer? by the installer. 

Note: This release introduces a new --force-install’(-f) switch to install VMware Tools.
File disappears after VMware Tools upgrade
deployPkg.dll file which is present in C:\Program Files\Vmware\Vmware Tools\ is not found after upgrading VMware Tools. This is observed when it is upgraded from version 5.1 Update 2 to 5.5 Update 1 or later and version 5.5 to 5.5 Update 1 or later. 

Workaround: None
User is forcefully logged out while installing or uninstalling VMware Tools by OSP
While installing or uninstalling VMware Tools packages in a RHEL (Red Hat Linux Enterprise) and CentOS virtual machines that were installed using operating system specific packages (OSP), the current user is forcefully logged out. This issue occurs in RHEL 6.5 64-bit, RHEL 6.5 32-bit, CentOS 6.5 64-bit and CentOS 6.5 32-bit virtual machines.

Workaround:
Use secure shell (SSH) to install or uninstall VMware Tools 
or
The user must log in again to install or uninstall the VMware Tools packages
Miscellaneous Issues
SRM test recovery operation might fail with an error
Attempts to perform Site Recovery Manager (SRM) test recovery might fail with error message similar to the following:
'Error - A general system error occurred: VM not found'.
When several test recovery operations are performed simultaneously, the probability of encountering the error messages increases.

Workaround: None. However, this is not a persistent issue and this issue might not occur if you perform the SRM test recovery operation again.
Copyright ? 2015 VMware, Inc. All rights reserved.
Terms of Use
Privacy
Accessibility
Trademarks
>
Features and known issues of ESXi 5.5 are described in the release notes for each release. Release notes for earlier releases of ESXi 5.5, are:
Messaging Model
deliver
acknowledgments
code
resize
bings rule
direct, fanout,topic 
Exchanges are where producers publish their messages.
Queuesare where the messages end up and are received by consumers
Bindings are how the messages get routed from the exchange to particular queues.

guard
Producer
Consumer
Advanced 
Message
 Queue
Error: database disk image is malformed 
Usage:
rabbitmqctl [-n <node>] [-q] <command> [<command options>] 

Options:
    -n node
    -q

Default node is "rabbit@server", where server is the local host. On a host 
named "server.example.com", the node name of the RabbitMQ Erlang node will 
usually be rabbit@server (unless RABBITMQ_NODENAME has been set to some 
non-default value at broker startup time). The output of hostname -s is usually 
the correct suffix to use after the "@" sign. See rabbitmq-server(1) for 
details of configuring the RabbitMQ broker.

Quiet output mode is selected with the "-q" flag. Informational messages are 
suppressed when quiet mode is in effect.

Commands:
    stop [<pid_file>]
    stop_app
    start_app
    wait <pid_file>
    reset
    force_reset
    rotate_logs <suffix>

    join_cluster <clusternode> [--ram]
    cluster_status
    change_cluster_node_type disc | ram
    forget_cluster_node [--offline]
    update_cluster_nodes clusternode
    sync_queue queue
    cancel_sync_queue queue
    set_cluster_name name

    add_user <username> <password>
    delete_user <username>
    change_password <username> <newpassword>
    clear_password <username>
    set_user_tags <username> <tag> ...
    list_users

    add_vhost <vhostpath>
    delete_vhost <vhostpath>
    list_vhosts [<vhostinfoitem> ...]
    set_permissions [-p <vhostpath>] <user> <conf> <write> <read>
    clear_permissions [-p <vhostpath>] <username>
    list_permissions [-p <vhostpath>]
    list_user_permissions <username>

    set_parameter [-p <vhostpath>] <component_name> <name> <value>
    clear_parameter [-p <vhostpath>] <component_name> <key>
    list_parameters [-p <vhostpath>]

    set_policy [-p <vhostpath>] [--priority <priority>] [--apply-to <apply-to>] 
<name> <pattern>  <definition>
    clear_policy [-p <vhostpath>] <name>
    list_policies [-p <vhostpath>]

    list_queues [-p <vhostpath>] [<queueinfoitem> ...]
    list_exchanges [-p <vhostpath>] [<exchangeinfoitem> ...]
    list_bindings [-p <vhostpath>] [<bindinginfoitem> ...]
    list_connections [<connectioninfoitem> ...]
    list_channels [<channelinfoitem> ...]
    list_consumers [-p <vhostpath>]
    status
    environment
    report
    eval <expr>

    close_connection <connectionpid> <explanation>
    trace_on [-p <vhost>]
    trace_off [-p <vhost>]
    set_vm_memory_high_watermark <fraction>

<vhostinfoitem> must be a member of the list [name, tracing].

The list_queues, list_exchanges and list_bindings commands accept an optional 
virtual host parameter for which to display results. The default value is "/".

<queueinfoitem> must be a member of the list [name, durable, auto_delete, 
arguments, policy, pid, owner_pid, exclusive_consumer_pid, 
exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, 
consumers, consumer_utilisation, memory, slave_pids, synchronised_slave_pids, 
status].

<exchangeinfoitem> must be a member of the list [name, type, durable, 
auto_delete, internal, arguments, policy].

<bindinginfoitem> must be a member of the list [source_name, source_kind, 
destination_name, destination_kind, routing_key, arguments].

<connectioninfoitem> must be a member of the list [pid, name, port, host, 
peer_port, peer_host, ssl, ssl_protocol, ssl_key_exchange, ssl_cipher, 
ssl_hash, peer_cert_subject, peer_cert_issuer, peer_cert_validity, state, 
channels, protocol, auth_mechanism, user, vhost, timeout, frame_max, 
channel_max, client_properties, recv_oct, recv_cnt, send_oct, send_cnt, 
send_pend].

<channelinfoitem> must be a member of the list [pid, connection, name, number, 
user, vhost, transactional, confirm, consumer_count, messages_unacknowledged, 
messages_uncommitted, acks_uncommitted, messages_unconfirmed, prefetch_count, 
global_prefetch_count].

 At least 577MB more space needed on the / filesystem.
the guest operating system has locked the CD-ROM door and is probably using the CD-ROM，which can prevent the guest from recognizing media changes. If possible,eject the CD-ROM from inside the guest before disconnecting.
Disconnect anyway and override the lock?
{"could not start kernel pid",application_controller,"error in config file \"./../etc/rabbitmq/rabbitmq.config\" (12): syntax error before: ']'"}

Crash dump was written to: erl_crash.dump
could not start kernel pid (application_controller) (error in config file "./../etc/rabbitmq/rabbitmq.config" (12): syntax error before: ']')
Error: invalid command 'join_cluster root@xiuba225 root@xiuba227'
Usage:
rabbitmqctl [-n <node>] [-q] <command> [<command options>] 

Options:
    -n node
    -q

Default node is "rabbit@server", where server is the local host. On a host 
named "server.example.com", the node name of the RabbitMQ Erlang node will 
usually be rabbit@server (unless RABBITMQ_NODENAME has been set to some 
non-default value at broker startup time). The output of hostname -s is usually 
the correct suffix to use after the "@" sign. See rabbitmq-server(1) for 
details of configuring the RabbitMQ broker.

Quiet output mode is selected with the "-q" flag. Informational messages are 
suppressed when quiet mode is in effect.

Commands:
    stop [<pid_file>]
    stop_app
    start_app
    wait <pid_file>
    reset
    force_reset
    rotate_logs <suffix>

    join_cluster <clusternode> [--ram]
    cluster_status
    change_cluster_node_type disc | ram
    forget_cluster_node [--offline]
    update_cluster_nodes clusternode
    sync_queue queue
    cancel_sync_queue queue

    add_user <username> <password>
    delete_user <username>
    change_password <username> <newpassword>
    clear_password <username>
    set_user_tags <username> <tag> ...
    list_users

    add_vhost <vhostpath>
    delete_vhost <vhostpath>
    list_vhosts [<vhostinfoitem> ...]
    set_permissions [-p <vhostpath>] <user> <conf> <write> <read>
    clear_permissions [-p <vhostpath>] <username>
    list_permissions [-p <vhostpath>]
    list_user_permissions <username>

    set_parameter [-p <vhostpath>] <component_name> <name> <value>
    clear_parameter [-p <vhostpath>] <component_name> <key>
    list_parameters [-p <vhostpath>]

    set_policy [-p <vhostpath>] <name> <pattern>  <definition> [<priority>] 
    clear_policy [-p <vhostpath>] <name>
    list_policies [-p <vhostpath>]

    list_queues [-p <vhostpath>] [<queueinfoitem> ...]
    list_exchanges [-p <vhostpath>] [<exchangeinfoitem> ...]
    list_bindings [-p <vhostpath>] [<bindinginfoitem> ...]
    list_connections [<connectioninfoitem> ...]
    list_channels [<channelinfoitem> ...]
    list_consumers [-p <vhostpath>]
    status
    environment
    report
    eval <expr>

    close_connection <connectionpid> <explanation>
    trace_on [-p <vhost>]
    trace_off [-p <vhost>]
    set_vm_memory_high_watermark <fraction>

<vhostinfoitem> must be a member of the list [name, tracing].

The list_queues, list_exchanges and list_bindings commands accept an optional 
virtual host parameter for which to display results. The default value is "/".

<queueinfoitem> must be a member of the list [name, durable, auto_delete, 
arguments, policy, pid, owner_pid, exclusive_consumer_pid, 
exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, 
consumers, active_consumers, memory, slave_pids, synchronised_slave_pids, 
status].

<exchangeinfoitem> must be a member of the list [name, type, durable, 
auto_delete, internal, arguments, policy].

<bindinginfoitem> must be a member of the list [source_name, source_kind, 
destination_name, destination_kind, routing_key, arguments].

<connectioninfoitem> must be a member of the list [pid, name, port, host, 
peer_port, peer_host, ssl, ssl_protocol, ssl_key_exchange, ssl_cipher, 
ssl_hash, peer_cert_subject, peer_cert_issuer, peer_cert_validity, 
last_blocked_by, last_blocked_age, state, channels, protocol, auth_mechanism, 
user, vhost, timeout, frame_max, client_properties, recv_oct, recv_cnt, 
send_oct, send_cnt, send_pend].

<channelinfoitem> must be a member of the list [pid, connection, name, number, 
user, vhost, transactional, confirm, consumer_count, messages_unacknowledged, 
messages_uncommitted, acks_uncommitted, messages_unconfirmed, prefetch_count, 
client_flow_blocked].
Committer: root <root@localhost.localdomain>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly:

    git config --global user.name "Your Name"
    git config --global user.email you@example.com

If the identity used for this commit is wrong, you can fix it with:

    git commit --amend --author='Your Name <you@example.com>'

 1 files changed, 2 insertions(+), 0 deletions(-)
 create mode 100755 hello.py
--remove-source-files   sender removes synchronized files (non-dirs)
Unable to get project for the run
tuple
Transparent proxy support
=========================

This feature adds Linux 2.2-like transparent proxy support to current kernels.
To use it, enable the socket match and the TPROXY target in your kernel config.
You will need policy routing too, so be sure to enable that as well.


1. Making non-local sockets work
================================

The idea is that you identify packets with destination address matching a local
socket on your box, set the packet mark to a certain value, and then match on that
value using policy routing to have those packets delivered locally:

# iptables -t mangle -N DIVERT
# iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
# iptables -t mangle -A DIVERT -j MARK --set-mark 1
# iptables -t mangle -A DIVERT -j ACCEPT

# ip rule add fwmark 1 lookup 100
# ip route add local 0.0.0.0/0 dev lo table 100

Because of certain restrictions in the IPv4 routing output code you'll have to
modify your application to allow it to send datagrams _from_ non-local IP
addresses. All you have to do is enable the (SOL_IP, IP_TRANSPARENT) socket
option before calling bind:

fd = socket(AF_INET, SOCK_STREAM, 0);
/* - 8< -*/
int value = 1;
setsockopt(fd, SOL_IP, IP_TRANSPARENT, &value, sizeof(value));
/* - 8< -*/
name.sin_family = AF_INET;
name.sin_port = htons(0xCAFE);
name.sin_addr.s_addr = htonl(0xDEADBEEF);
bind(fd, &name, sizeof(name));

A trivial patch for netcat is available here:
http://people.netfilter.org/hidden/tproxy/netcat-ip_transparent-support.patch


2. Redirecting traffic
======================

Transparent proxying often involves "intercepting" traffic on a router. This is
usually done with the iptables REDIRECT target; however, there are serious
limitations of that method. One of the major issues is that it actually
modifies the packets to change the destination address -- which might not be
acceptable in certain situations. (Think of proxying UDP for example: you won't
be able to find out the original destination address. Even in case of TCP
getting the original destination address is racy.)

The 'TPROXY' target provides similar functionality without relying on NAT. Simply
add rules like this to the iptables ruleset above:

# iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY \
  --tproxy-mark 0x1/0x1 --on-port 50080

Note that for this to work you'll have to modify the proxy to enable (SOL_IP,
IP_TRANSPARENT) for the listening socket.


3. Iptables extensions
======================

To use tproxy you'll need to have the 'socket' and 'TPROXY' modules
compiled for iptables. A patched version of iptables is available
here: http://git.balabit.hu/?p=bazsi/iptables-tproxy.git


4. Application support
======================

4.1. Squid
----------

Squid 3.HEAD has support built-in. To use it, pass
'--enable-linux-netfilter' to configure and set the 'tproxy' option on
the HTTP listener you redirect traffic to with the TPROXY iptables
target.

For more information please consult the following page on the Squid
wiki: http://wiki.squid-cache.org/Features/Tproxy4

E265 block comment should start with '# '
There are no TAP-Windows adapters on this system.  You should be able to create a TAP-Windows adapter by going to Start -> All Programs -> TAP-Windows -> Utilities -> Add a new TAP-Windows virtual ethernet adapter.
 Project interpreter not specified
Transparent proxy support
The IP address to which this entry's addressing information pertains.
=========================

This feature adds Linux 2.2-like transparent proxy support to current kernels.
To use it, enable the socket match and the TPROXY target in your kernel config.
You will need policy routing too, so be sure to enable that as well.


1. Making non-local sockets work
================================

The idea is that you identify packets with destination address matching a local
socket on your box, set the packet mark to a certain value, and then match on that
value using policy routing to have those packets delivered locally:

# iptables -t mangle -N DIVERT
# iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
# iptables -t mangle -A DIVERT -j MARK --set-mark 1
# iptables -t mangle -A DIVERT -j ACCEPT

# ip rule add fwmark 1 lookup 100
# ip route add local 0.0.0.0/0 dev lo table 100

Because of certain restrictions in the IPv4 routing output code you'll have to
modify your application to allow it to send datagrams _from_ non-local IP
addresses. All you have to do is enable the (SOL_IP, IP_TRANSPARENT) socket
option before calling bind:

fd = socket(AF_INET, SOCK_STREAM, 0);
/* - 8< -*/
int value = 1;
setsockopt(fd, SOL_IP, IP_TRANSPARENT, &value, sizeof(value));
/* - 8< -*/
name.sin_family = AF_INET;
name.sin_port = htons(0xCAFE);
name.sin_addr.s_addr = htonl(0xDEADBEEF);
bind(fd, &name, sizeof(name));

A trivial patch for netcat is available here:
http://people.netfilter.org/hidden/tproxy/netcat-ip_transparent-support.patch


2. Redirecting traffic
======================

Transparent proxying often involves "intercepting" traffic on a router. This is
usually done with the iptables REDIRECT target; however, there are serious
limitations of that method. One of the major issues is that it actually
modifies the packets to change the destination address -- which might not be
acceptable in certain situations. (Think of proxying UDP for example: you won't
be able to find out the original destination address. Even in case of TCP
getting the original destination address is racy.)

The 'TPROXY' target provides similar functionality without relying on NAT. Simply
add rules like this to the iptables ruleset above:

# iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY \
  --tproxy-mark 0x1/0x1 --on-port 50080

Note that for this to work you'll have to modify the proxy to enable (SOL_IP,
IP_TRANSPARENT) for the listening socket.


3. Iptables extensions
======================

To use tproxy you'll need to have the 'socket' and 'TPROXY' modules
compiled for iptables. A patched version of iptables is available
here: http://git.balabit.hu/?p=bazsi/iptables-tproxy.git


4. Application support
======================

4.1. Squid
----------

Squid 3.HEAD has support built-in. To use it, pass
'--enable-linux-netfilter' to configure and set the 'tproxy' option on
the HTTP listener you redirect traffic to with the TPROXY iptables
target.

For more information please consult the following page on the Squid
wiki: http://wiki.squid-cache.org/Features/Tproxy4
Hi,
I am running Cluster setup of rabbitmq. 
the single node at both the machines are running fine in the same network

and I have enabled ssh entry for both master and slave
slave can access master using ssh master

after that I stopped rabbitmq slave machine
and run join cluster command like join_cluster rabbitmq@master    ---{here rabbitmq is master setup user name and I{p)

but getting
Error: {cannot_discover_cluster,"The nodes provided are either offline or not running"
error
please help me to resolve the issue

Thanks in advance

shankarlaingegowda
Hi, 

On 28 Feb 2013, at 15:40, shankaralingegowda Singonahalli Chikkarevanna wrote: 
> and I have enabled ssh entry for both master and slave 
> slave can access master using ssh master 
> 

Enabling ssh has nothing to do with clustering, so I'm not sure why that's relevant. Are you trying to provide connectivity using ssh tunnels or something? 

> after that I stopped rabbitmq slave machine 
> and run join cluster command like join_cluster rabbitmq@master    ---{here rabbitmq is master setup user name and I{p) 
> 
> but getting 
> Error: {cannot_discover_cluster,"The nodes provided are either offline or not running" 
> error 

This error means that Erlang cannot connect to the rabbitmq@master node. The individual nodes need to be able to contact one another over TCP/IP using port 4369 and both hosts need to be able to resolve the other's hostname via dns. You should be able to `ping master` from slave and vice versa. If you can ping from one machine to another, you should also be able to telnet to the epmd port (4369) successfully. 

> please help me to resolve the issue 
> 

Please confirm that TCP/IP connectivity between the two machines works over port 4369. If this *does* work then please confirm that both machines have been configured with the same erlang cookie. If you've followed the transcript at http://www.rabbitmq.com/clustering.html and are still unable to get join_cluster working then let me know and I'll try to assist you further. 

Cheers, 
Tim 
rabbitmq-discuss mailing list 
reset
The nodes provided are either offline or not running
Plugin configuration has changed. Restart RabbitMQ for changes to take effect
138\Operation not permitted

cp: cannot create regular file `nginx': Text file busy

unable to publish tianxing stream either due to Network Problem at server，or the stream name being used is busy。Please check the network connection with server,or try publishing with other stream name.
NO audio capture devices detected 
Flash Media Live Encoder requires an audio capture device to be connected and properly installed.

mount: mount point /cache does not exist
TypeError: name_zhu() argument after ** must be a mapping, not int

kill(...)
    kill(pid, sig)

    Kill a process with a signal

large file

[root@localhost ~]# man parted 
PARTED(8) GNU Parted Manual PARTED(8) 

NAME 
GNU Parted - a partition manipulation program 

SYNOPSIS 
parted [options] [device [command [options...]...]] 

DESCRIPTION 
parted is a disk partitioning and partition resizing program. It allows you to create, destroy, resize, move and copy ext2, linux-swap, FAT, 
FAT32, and reiserfs partitions. It can create, resize, and move Macintosh HFS partitions, as well as detect jfs, ntfs, ufs, and xfs parti- 
tions. It is useful for creating space for new operating systems, reorganising disk usage, and copying data to new hard disks. 

This manual page documents parted briefly. Complete documentation is distributed with the package in GNU Info format; see near the bottom.

The block which holds all RTMP settings


context: rtmp
Declares RTMP server instance

Archive:  vcastr22.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of vcastr22.zip or
        vcastr22.zip.zip, and cannot find vcastr22.zip.ZIP, period.

Missing home directory

modified
problem with primary server

problem with primary server
Faiure to connect to primary server please verify that your server URL and application name are valid and that your INternet connection is working and retry
Trans
Normal



wait_video

Syntax: wait_video on|off
Context: rtmp, server, application
Disable audio until first video frame is sent. Defaults to off. Can be combined with wait_key to make client receive video key frame with all other data following it. However this usually increases connection delay. You can tune keyframe interval in your encoder to reduce the delay.
Recent versions of IE need this option to be enabled for normal playback.
wait_video on;

wait_key

Syntax: wait_key on|off
Context: rtmp, server, application
Makes video stream start with a key frame. Defaults to off.
wait_key on;
ping

syntax: ping value
context: rtmp, server
RTMP ping interval. Zero turns ping off. RTMP ping is a protocol feature for active connection check. A special packet is sent to remote peer and a reply is expected within a timeout specified with ping_timeout directive. If ping reply is not received within this time then connection is closed. Default value for ping is 1 minute. Default ping timeout is 30 seconds.
ping 3m;
ping_timeout 30s;
ping_timeout

syntax: ping_timeout value
context: rtmp, server
See ping description above.
RTMPDump

rtmpdump is a toolkit for RTMP streams. All forms of RTMP are supported, including rtmp://, rtmpt://, rtmpe://, rtmpte://, and rtmps://.
License: GPLv2 
Copyright (C) 2009 Andrej Stepanchuk 
Copyright (C) 2010-2011 Howard Chu 
Download the source: 
git clone git://git.ffmpeg.org/rtmpdump
The latest release is 2.4 which you can check out from git. Aside from various minor bugfixes since 2.3, RTMPE type 9 handshakes are now supported.
Releases will also be provided as compressed tar archives in the download directory. There are also zip archives in this directory; these contain binaries of builds for Windows or other platforms.

Read the ChangeLog for a summary of the changes in each release.
To build with all features enabled, you will also need a Cryptography library and the zlib compression library. It uses OpenSSL by default, but you can also useGnuTLS or PolarSSL instead. (Note that the Windows binaries here are now built statically with PolarSSL so no other DLLs are needed.)
As of 2.2 the main protocol code is now available in its own library, librtmp. This library is licensed under LGPL so it may be used freely in other applications. As of 2.3 the library may also be built as a shared library, not just a static library

Some manpages have been added for 2.2. You can read the rtmpdump manpage and the rtmpgw manpage online as well.
An overview of the library is also available in the librtmp manpage.

As of 2.1 there are two additional server programs, rtmpsrv and rtmpsuck.
rtmpsrv is a stub for a server; it logs the connect and play parameters from a regular client that connects to it. It then invokes rtmpdump with those parameters to retrieve the stream.
rtmpsuck is a transparent proxy; it intercepts connections from a client and then makes an outbound connection to the real server. After all handshaking is complete and encryption keys with both sides are negotiated, it records the cleartext stream data into files while relaying the data from the server to the client.
2010-06-30: Download source tarball rtmpdump-2.3.tgz 
2013-01-09: Download Windows build rtmpdump-2.4-git-010913-windows.zip 
2010-06-30: Download Android build rtmpdump-2.3-android.zip 
Subscribe to the mailing list: 
https://lists.mplayerhq.hu/mailman/listinfo/rtmpdump 
or read the list archives: http://lists.mplayerhq.hu/pipermail/rtmpdump/
Discussion forums: http://ffmpeg.zeranoe.com/forum/viewforum.php?f=29 note, this forum is not affiliated with the RTMPDump project. 
http://stream-recorder.com/forum/rtmpdump-f54.html - note, that site is not affiliated with the RTMPDump project, but they're pretty active with helping answer questions.
Projects using rtmpdump: 
get-flash-videos
get_iplayer(discontinued)
get_iplayer
iFetch
yle-dl
youtube-dl
python-iview
mlbviewer
iViewNapper
TV3 al Cabàs
streamCapture
PlayDownloader
MTV.it download
Ed's iPlayer Javascript BBC ripper
rtmpdumphelper (windows network hook for rtmpsuck/rtmpsrv)
Projects using librtmp: 
FFmpeg
MPlayer (thru ffmpeg)
HTS Home Theater System
cURL

Patches for librtmp on other projects: 
XBMC

Forks: 
flvstreamer 
xVideoServiceThief 
See Also: 
rtspdump
rtspdump is a program which downloads rtsp:// multimedia stream from a Microsoft WMServer.
If you know of more related projects and want to see them linked, please email projects@mplayerhq.hu.
Meet Django

Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design.

Developed by a fast-moving online-news operation, Django was designed to handle two challenges: the intensive deadlines of a newsroom and the stringent requirements of the experienced Web developers who wrote it. It lets you build high-performing, elegant Web applications quickly.
Django focuses on automating as much as possible and adhering to the DRY principle.
Dive in by reading the overview →
When you're ready to code, read the installation guide and tutorial.
The Django framework

Object-relational mapper

Define your data models entirely in Python. You get a rich, dynamic database-access APIfor free — but you can still write SQL if needed.
Automatic admin interface

Save yourself the tedious work of creating interfaces for people to add and update content. Django does that automatically, and it's production-ready.
Elegant URL design

Design pretty, cruft-free URLs with no framework-specific limitations. Be as flexible as you like.
Template system

Use Django's powerful, extensible and designer-friendly template language to separate design, content and Python code.
Cache system

Hook into memcached or other cache frameworks for super performance — caching is as granular as you need.
Internationalization

Django has full support for multi-language applications, letting you specify translation strings and providing hooks for language-specific functionality.
Download

Latest release: 1.6.5
Open source, BSD license
Documentation

Installation guide
Tutorial
Full index...
Sites that use Django

Disqus
Beautiful, real-time, engaging comments threads for your website.
Instagram
A fast, beautiful and fun photo-sharing app for iOS and Android.
Mozilla
Makers of Firefox, and defenders of the openness, innovation, and opportunity on the web.
OpenStack
Open source software for building private and public clouds.
Pinterest
Pinterest is a virtual pinboard to share beautiful things you find on the web.
PolitiFact.com
The Pulitzer Prize-winning fact-checking site.
Rdio
Your music library in the cloud.
See more sites...
Weblog

Django 1.7 release candidate 1

by James Bennett on Jun. 26, 2014
Today the Django team has issued Django 1.7 release candidate 1, a preview/testing package for the upcoming Django 1.7 release.
Read more
Jannis Leidel joins the DSF Board

by Russell Keith-Magee on Jun. 26, 2014
Following the resignation of Jeremy Dunck from the DSF board, the DSF is pleased to welcome its newest director: Jannis Leidel.
Read more
Seeking Python users in Namibia

by Daniele Procida on Jun. 18, 2014
A project to establish a PyCon in an African city requires the participation of local Python/Django/free software users.
Read more
Announcing DjangoCon US 2014

by Russell Keith-Magee on Jun. 11, 2014
DjangoCon US is returning to Portland, Oregon for 2014; the Call For Papers is now open.
Read more
IndentationError
initial buffer ented block
wowza  Default extend timestamp

initial buffer 
MySQL on Windows
MySQL Yum Repository
MySQL APT Repository
MySQL Community Server
MySQL Cluster
MySQL Fabric
MySQL Utilities
MySQL Workbench
MySQL Proxy
MySQL Connectors
Other Downloads
paramiko is a module for python 2.2 (or higher) that implements the SSH2 protocol for secure (encrypted and authenticated) connections to remote machines. unlike SSL (aka TLS), SSH2 protocol does not require heirarchical certificates signed by a powerful central authority. you may know SSH2 as the protocol that replaced telnet and rsh for secure access to remote shells, but the protocol also includes the ability to open arbitrary channels to remote services across the encrypted tunnel -- this is how sftp works, for example.
paramiko is currently maintained by jeff forcier and the paramiko team on github:
https://github.com/paramiko/paramiko
if you're looking for older versions, archives, and history, you can still visit the legacy page.

/usr/local/python27/lib/python2.7/site-packages/Crypto/Util/number.py:57: PowmInsecureWarning: Not using mpz_powm_sec.  You should rebuild using libgmp >= 5 to avoid timing attack vulnerability.
  _warn("Not using mpz_powm_sec.  You should rebuild using libgmp >= 5 to avoid timing attack vulnerability.", PowmInsecureWarning)
/usr/local/python27/lib/python2.7/site-packages/Crypto/Util/number.py:57: PowmInsecureWarning: Not using mpz_powm_sec.  You should rebuild using libgmp >= 5 to avoid timing attack vulnerability.
  _warn("Not using mpz_powm_sec.  You should rebuild using libgmp >= 5 to avoid timing attack vulnerability.", PowmInsecureWarning)


Options:
  -v VERBOSITY, --verbosity=VERBOSITY
                        Verbosity level; 0=minimal output, 1=normal output,
                        2=verbose output, 3=very verbose output
  --settings=SETTINGS   The Python path to a settings module, e.g.
                        "myproject.settings.main". If this isn't provided, the
                        DJANGO_SETTINGS_MODULE environment variable will be
                        used.
  --pythonpath=PYTHONPATH
                        A directory to add to the Python path, e.g.
                        "/home/djangoprojects/myproject".
  --traceback           Raise on exception
  --version             show program's version number and exit
  -h, --help            show this help message and exit
"""
Django settings for mysite project.
For more information on this file, see
https://docs.djangoproject.com/en/1.6/topics/settings/
For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.6/ref/settings/
"""
# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
It worked!
Congratulations on your first Django-powered page.
Of course, you haven't actually done any work yet. Next, start your first app by running python manage.py startapp [appname].
You're seeing this message because you have DEBUG = True in your Django settings file and you haven't configured any URLs. Get to work!
join([timeout])
Block the calling thread until the process whose join() method is called terminates or until the optional timeout occurs.
If timeout is None then there is no timeout.
A process can be joined many times.
A process cannot join itself because this would cause a deadlock. It is an error to attempt to join a process before it has been started.

source

duplicate location 


This page contains the following errors:

error on line 1 at column 1: Document is empty
Below is a rendering of the page up to the first error.

template

Parameters may be appended to the url to set an initial state eg. head/index.html?base_uri=http://node-01.example.com:9200

base_uri force elasticsearch-head to connect to a particular node.
dashboard experimental feature to open elasticsearch-head in a mode suitable for dashboard / radiator. Accepts one parameter dashboard=cluster
auth_user adds basic auth credentials to http requests ( requires elasticsearch-http-basic plugin or a reverse proxy )
auth_password basic auth password as above (note: without additional security layers, passwords are sent over the network in the clear )
RabbitMQPivotal

Search RabbitMQ

FeaturesInstallationDocsTutorialsSupportCommunityPivotal is HiringBlog
Federation Plugin

Introduction

The high-level goal of the federation plugin is to transmit messages between brokers without requiring clustering. This is useful for various reasons:

Loose coupling
The federation plugin can transmit messages between brokers (or clusters) in different administrative domains:
they may have different users and virtual hosts;
they may run on different versions of RabbitMQ and Erlang.
WAN-friendly
The federation plugin uses AMQP 0-9-1 to communicate between brokers, and is designed to tolerate intermittent connectivity.
Specificity
A broker can contain federated and local-only components - you don't need to federate everything if you don't want to.
Scalability
Federation does not require O(n2) connections between n brokers (although this is the easiest way to set things up), which should mean it scales better.
What Does It Do?

The federation plugin allows you to make exchanges and queues federated. A federated exchange or queue can receive messages from one or more upstreams (remote exchanges and queues on other brokers). A federated exchange can route messages published upstream to a local queue. A federated queue lets a local consumer receive messages from an upstream queue.

Federation links connect to upstreams using RabbitMQ Erlang client. Therefore they can connect to a specific vhost, use TLS, use multiple authentication mechanisms.

For more details, see the documentation on federated exchanges and federated queues.

How is Federation Set Up?

To use federation, one needs to configure two things

One or more upstreams that define federation connections to other nodes. This can be done via runtime parameters or the federation management plugin which adds a federation management tab to the management UI.
One or more policies that match exchanges/queues and makes them federated.
Getting Started

The federation plugin is included in the RabbitMQ distribution. To enable it, use rabbitmq-plugins:

rabbitmq-plugins enable rabbitmq_federation
When using the management plugin, you will also want to enable rabbitmq_federation_management:

rabbitmq-plugins enable rabbitmq_federation_management
When using a federation in a cluster, all the nodes of the cluster should have the federation plugin enabled.

Information about federation upstreams is stored in the RabbitMQ database, along with users, permissions, queues, etc. There are three levels of configuration involved in federation:

Upstreams: each upstream defines how to connect to another broker.
Upstream sets: each upstream set groups together a set of upstreams to use for federation.
Policies: each policy selects a set of exchanges, queues or both, and applies a single upstream or an upstream set to those objects.
In practice, for simple use cases you can almost ignore the existence of upstream sets, since there is an implicitly-defined upstream set called all to which all upstreams are added.

Upstreams and upstream sets are both instances of runtime parameters. Like exchanges and queues, each virtual host has its own distinct set of parameters and policies. For more generic information on parameters and policies, see the documentation on parameters and policies. For full details on the parameters used by federation, see the federation reference.

Parameters and policies can be set in three ways - either with an invocation of rabbitmqctl, a call to the management HTTP API, or (usually) through the web UI presented by rabbitmq_federation_management. (The web UI does not present all possibilities - in particular, it does not allow you to manage upstream sets.)

A simple example
Here we will federate all the built-in exchanges except for the default exchange, with a single upstream. The upstream will be defined to buffer messages when disconnected for up to one hour (3600000ms).

Define an upstream:

rabbitmqctl	rabbitmqctl set_parameter federation-upstream my-upstream \
'{"uri":"amqp://server-name","expires":3600000}'
rabbitmqctl (Windows)	rabbitmqctl set_parameter federation-upstream my-upstream ^
"{""uri"":""amqp://server-name"",""expires"":3600000}"
HTTP API	PUT /api/parameters/federation-upstream/%2f/my-upstream
{"value":{"uri":"amqp://server-name","expires":3600000}}
Web UI	Navigate to Admin > Federation Upstreams > Add a new upstream. Enter "my-upstream" next to Name, "amqp://server-name" next to URI, and 36000000 next to Expiry. Click Add upstream.
Then define a policy that will match built-in exchanges and use this upstream:

rabbitmqctl	rabbitmqctl set_policy --apply-to exchanges federate-me "^amq\." \
'{"federation-upstream-set":"all"}'
rabbitmqctl (Windows)	rabbitmqctl set_policy --apply-to exchanges federate-me "^amq\." ^
"{""federation-upstream-set"":""all""}"
HTTP API	PUT /api/policies/%2f/federate-me
{"pattern":"^amq\.", "definition":{"federation-upstream-set":"all"}, \
"apply-to":"exchanges"}
Web UI	Navigate to Admin > Policies > Add / update a policy. Enter "federate-me" next to "Name", "^amq\." next to "Pattern", choose "Exchanges" from the "Apply to" drop down list and enter "federation-upstream-set" = "all" in the first line next to "Policy". Click "Add" policy.
We tell the policy to federate all exchanges whose names begin with "amq." (i.e. all the built in exchanges except for the default exchange) with (implicit) low priority, and to federate them using the implicitly created upstream set "all", which includes our newly-created upstream. Any other matching policy with a priority greater than 0 will take precedence over this policy. Keep in mind that federate-me is just a name we used for this example, you can use any string you want there.

The built in exchanges should now be federated because they are matched by the policy. You can check that the policy has applied to the exchanges by checking the exchanges list in management or with:

rabbitmqctl list_exchanges name policy | grep federate-me
And you can check that federation links for each exchange have come up with Admin > Federation Status > Running Links or with:

rabbitmqctl eval 'rabbit_federation_status:status().'
In general there will be one federation link for each upstream that is applied to an exchange. So for example with three exchanges and two upstreams for each there will be six links.

For simple use this should be all you need - you will probably want to look at the AMQP URI reference. The federation reference contains more details on setting up upstreams and upstream sets.

Federation Connection (Link) Failures

Inter-node connections used by Federation are based on AMQP 0-9-1 connections. Federation links can be treated as special kind of clients by operators.

Should a link fail, e.g. due to a network interruption, it will attempt to re-connect. Reconnection period is a configurable value that's defined in upstream definition. See federation reference for more details on setting up upstreams and upstream sets.

Links generally try to recover ad infinitum but there are scenarios when they give up:

Failure rate is too high (max tolerated rate depends on upstream's reconnect-delay but is generally a failure every few seconds by default).
Link no longer can locate its source (upstream) queue or exchange.
Policy changes in such a way that a link considers itself no longer necessary.
By increasing reconnect-delay for upstreams it is possible to tolerate higher link failure rates. This is primarily relevant for RabbitMQ installations where a moderate or large number of active links.
Federating Clusters

Clusters can be linked together with federation just as single brokers can. To summarise how clustering and federation interact:

You can define policies and parameters on any node in the downstream cluster; once defined on one node they will apply on all nodes.
Exchange federation links will start on any node in the downstream cluster. They will fail over to other nodes if the node they are running on crashes or stops.
Queue federation links will start on the same node as the downstream queue. If the downstream queue is mirrored, they will start on the same node as the master, and will be recreated on the same node as the new master if the node the existing master is running on crashes or stops.
To connect to an upstream cluster, you can specify multiple URIs in a single upstream. The federation link process will choose one of these URIs at random each time it attempts to connect.
Securing Federation Connections with TLS

Federation connections (links) can be secured with TLS. Because Federation uses a RabbitMQ client under the hood, it is necessary to both configure source broker to listen for TLS connections and Federation/Erlang client to use TLS.

To configure Federation to use TLS, one needs to

Use the amqps URI scheme instead of amqp
Specify CA certificate and client certificate/key pair via URI query parameters when configuring upstream(s)
Configure Erlang client to use TLS
Just like with "regular" client connections, server's CA should be trusted on the node where federation link(s) runs, and vice versa.

Getting Help

If you have any questions or comments regarding RabbitMQ, feel free to ask them on RabbitMQ mailing list.

In This Section
Server Documentation
Client Documentation
Plugins
Management plugin
Federation plugin
Exchanges
Queues
Reference
Shovel plugin
STOMP plugin
MQTT plugin
LDAP plugin
Configuring web plugins
Installing plugins
Plugin development
News
Protocol
Our Extensions
Building
Previous Releases
License
In This Page
Introduction
What Does It Do?
How is Federation Set Up?
Getting Started
Federation Connection (Link) Failures
Federating Clusters
Securing Federation Connections with TLS
Getting Help
Sitemap | Contact | This Site is Open Source | Pivotal is Hiring

Copyright ? 2007-Present Pivotal Software, Inc. All rights reserved. Terms of Use, Privacy and Trademark Guidelines

